---
title: "Sample and Power Calculations"

#------------------------------------------------------------------
# Authors
#------------------------------------------------------------------
# Authors are the main creators of the site's content, credited
# for their work and responsible for its core development,
# including writing and editing.
#------------------------------------------------------------------
authors-ipa:
    - "[Cristhian Pulido](https://poverty-action.org/people/cristhian-pulido)"
    - "[Ishmail Azindoo Baako](https://poverty-action.org/people/ishmail-azindoo-baako)"

#------------------------------------------------------------------
# Contributors
#------------------------------------------------------------------
# Contributors provide support, such as feedback or supplementary
# materials for the site. They can also be responsible for
# updating/maintaining the site.
#------------------------------------------------------------------
---

:::{.custom-summary-block}
This resource covers essential concepts of statistical power and sample size calculations for randomized evaluations, including hands-on applications using Stata.
:::

![An IPA survey in Myanmar in 2020 (¬© IPA)](/assets/images/power-intro.jpg){width=75%}
      
:::{.callout-tip appearance="simple"}
## Key Takeaways
- **Statistical power** determines our ability to detect true effects in research studies.
- **Sample size** directly affects the precision and reliability of our estimates.
- **Statistical concepts** like significance levels, confidence intervals, and effect sizes are crucial for interpreting results.
- **Practical tools** in Stata help balance statistical power with resource constraints.
:::


## **Statistical Power: Why It Matters**

**Statistical power** is the probability of correctly rejecting the null hypothesis (H‚ÇÄ) when the alternative hypothesis (H‚ÇÅ) is true.

- **Formula:** Power = 1 ‚àí Œ∫ (Type II error rate)  
- **Rule of thumb:** Aim for **‚â•80% power** to reduce the risk of missing true effects.

#### Why Statistical Power Matters

1. **Research Quality and Reliability**
   - Ensures studies can detect meaningful effects  
   - Reduces risk of false conclusions  
   - Strengthens confidence in research findings  

2. **Resource Optimization**
   - Prevents wastage of research resources  
   - Avoids underpowered studies that may be inconclusive  
   - Guides efficient allocation of study participants  

3. **Policy Impact**
   - Enables evidence-based policy decisions  
   - Reduces risk of dismissing effective interventions  
   - Helps identify truly impactful programs  

4. **Ethical Considerations**
   - Respects participants' time and effort  
   - Justifies the use of research resources  
   - Promotes responsible funding and implementation  

---

### Real-World Example: Improving Academic Achievement in India[^1]

<div style="background-color: #f7f7f7; padding: 1em; border-radius: 8px; margin-bottom: 1em;">

**üìç Context:**  
In India, many children age 7 to 12 were struggling with basic academic skills:  
- 44% could not read a simple paragraph  
- 50% could not solve a basic subtraction problem  

**üí° Intervention:**  
The Balsakhi tutoring program provided:  
- 2 hours of daily remedial instruction  
- Small group tutoring (15 to 20 students per tutor)  

**üß™ Study Design:**  
- Large-scale randomized controlled trial  
- **Sample size:** 23,000 students  
- **Groups:**  
  - **Treatment:** Received Balsakhi tutoring  
  - **Control:** Followed the standard curriculum  

**üìä Results:**  
- **Effect size:** 0.28 standard deviation increase in test scores  
- **Power:** The large sample size ensured high statistical power, enabling detection of a meaningful effect

</div>

![Estimated Treatment Effect of 0.28 SD](/assets/images/power_3.png){width=90%}

This example shows how proper study design and statistical power can produce actionable evidence, guiding impactful education policy in low-resource settings. [^2]

---

::: {.callout-note}
## Key Concept 1  
The larger the study sample, the more likely we are to estimate the true treatment effect of the program.

::: {layout-ncol=2}
![Sample Size N=200: Wide estimate range](/assets/images/power_1.png){width=75%}

![Sample Size N=2000: Narrow estimate range](/assets/images/power_2.png){width=75%}
:::
:::

---

## **Sample Size: Precision and Reliability**

### Sample Size and Accuracy  
Sample size directly influences our ability to estimate the **true treatment effect** with precision and reliability:

- **Smaller N:** High variability, less precise and reliable estimates  
- **Larger N:** Lower variability, more precise and reliable estimates

### Trade-offs  
While larger samples improve precision, they require more resources:

- **Financial:** Survey costs, transport, devices  
- **Time:** Recruitment, training, fieldwork  
- **Labor:** Enumerators, field staff

::: {.callout-note}
## Key Concept 2
Sample size/power calculations help balance statistical precision with resource constraints:

- **Too small:** Risk missing true treatment effects
- **Too large:** Waste resources unnecessarily
- **Goal:** Find optimal sample to detect meaningful effects reliably
:::

---

## **Understanding Statistical Concepts**

### Probability Density Functions (PDFs)

PDFs help us visualize the uncertainty around the estimated treatment effect (Œ≤ÃÇ) in a study. They show how likely different values of Œ≤ÃÇ are, assuming a true treatment effect Œ≤.

![Two overlapping normal distributions representing possible treatment effects](/assets/images/power_4.png){width=75%}

---

### Treatment Effects Distribution

If we repeated the same RCT many times, we wouldn't get the exact same estimate every time. Instead, the estimated treatment effects (Œ≤ÃÇ) would follow a normal distribution:

$$
\hat{\beta} \sim \text{Normal}(\beta, \sigma^2 / [Np(1-p)])
$$

- **Œ≤**: True treatment effect  
- **œÉ¬≤**: Variance of the outcome  
- **N**: Sample size  
- **p**: Proportion assigned to treatment (e.g., 0.5)

:::{.callout-note}
## Key Concept 3  
A narrower distribution of estimates (Œ≤ÃÇ) increases our chances of detecting true effects. You can achieve this by:  
- Increasing the sample size (**N**)  
- Assigning roughly equal numbers to treatment and control (**p ‚âà 0.5**)  
- Reducing outcome variance (**œÉ¬≤**)
:::

---

### Hypothesis Testing Framework

To determine whether an effect is statistically significant, we use a hypothesis test:

::: {.callout-tip collapse="true"}
## **1. Null Hypothesis (H‚ÇÄ)**
Assumes no effect exists (Œ≤ = 0). This is the starting point for statistical testing and represents the default position that the intervention has no impact.

![Statistical hypothesis testing showing null and alternative distributions](/assets/images/power_ht1.png){width=75%}
:::

::: {.callout-tip collapse="true"}
## **2. Alternative Hypothesis (H‚ÇÅ)**
Proposes existence of an effect (Œ≤ ‚â† 0). Can be one-sided (Œ≤ > 0 or Œ≤ < 0) or two-sided (Œ≤ ‚â† 0).

![Estimate the treatment effect (ùõΩ¬†ÃÇ)](/assets/images/power_ht2.png){width=75%}
:::

::: {.callout-tip collapse="true"}
## **3. Critical Values Computation**
Based on chosen significance level (Œ±), typically uses Œ± = 0.05 (5%). Forms rejection regions in the distribution and uses t-distribution or z-distribution.

![Critial Values](/assets/images/power_ht3.png){width=75%}

![Critial Values - Fail To reject](/assets/images/power_ht3_2.png){width=75%}

![Critial Values - Reject](/assets/images/power_ht3_3.png){width=75%}
:::

::: {.callout-tip collapse="true"}
## **4. Effect Estimation and Comparison**
Calculate estimated effect size (Œ≤ÃÇ), compare to critical values, consider confidence intervals, and assess statistical significance.
:::

::: {.callout-tip collapse="true"}
## **5. Statistical Decision**
Reject H‚ÇÄ if Œ≤ÃÇ exceeds critical values, fail to reject H‚ÇÄ if Œ≤ÃÇ within critical bounds. Consider practical significance and report p-values and confidence intervals.

![No rejection of Ho](/assets/images/power_ht4_1.png){width=75%}

![Rejection of Ho](/assets/images/power_ht4_2.png){width=75%}
:::

:::{.callout-note}
## Key Concept 4  
**Type I Error (Œ±):** Risk of a false positive  
- Usually set at **5% (Œ± = 0.05)**  
- This means there's a 5% chance we incorrectly detect an effect when there is none  
- A lower Œ± reduces false positives‚Äîbut increases the risk of missing real effects
- Its complement (1‚àíŒ±) is known as the confidence level (how likely we are to avoid a false positive)
- While 5% is standard, it can range from 1%-10% in research literature
:::

:::{.callout-note}
## Key Concept 5  
**Statistical Power (1 ‚àí Œ∫):** Probability of detecting a true effect

- **Œ∫** is the Type II Error rate (false negative)  
- Standard goal: **Power ‚â• 80%**  
- Higher power = more confidence in detecting real effects, but requires a larger sample size

Important trade-off:

- The less willing we are to make a false positive (smaller Œ±), the more likely we are to make a false negative (Type II error)
- Type II error rate (Œ∫) is usually set at 20% (much higher than the false positive rate)
- This means NOT finding an effect when the effect does exist!
- Power (1‚àíŒ∫) is the probability of avoiding a false negative
:::

---

## **Statistical Power in Practice**

### How Power Looks in Different Scenarios

### Power in Action: Visual Guide

The graph below shows three key elements of statistical power:

- Null hypothesis distribution (H‚ÇÄ: no effect)
- Alternative hypothesis distribution (H‚ÇÅ: true effect exists)
- Critical value threshold for significance

![Three scenarios showing different power levels based on effect size and sample size](/assets/images/power_7.png){width=90%}

::: {.callout-note}
## Key Concept 6
When power is high (‚â•80%), we can confidently detect true effects. When power is low, we risk missing important program impacts.
:::

### Effect Size and Power

The ability to detect effects depends on:
1. **Sample size (N)**
2. **Effect size (Œ≤)**
3. **Outcome variance (œÉ¬≤)**


![Low Power: Hard to distinguish effect](/assets/images/power_5.png){width=90%}

![High Power: Clear effect distinction](/assets/images/power_6.png){width=90%}


### Common Power Scenarios

1. **Underpowered Study (Power < 80%)**
    - High risk of false negatives
    - Inconclusive results
    - Wasted resources

2. **Well-Powered Study (Power ‚â• 80%)**
    - Reliable effect detection
    - Clear conclusions
    - Efficient resource use

::: {.callout-warning}
## Warning
An underpowered study might fail to detect meaningful program effects, leading to incorrect policy decisions.
:::

---

## **Practical Tools and Applications in Stata**

### Balancing Power and Resources

Use **power calculations** to balance the need for statistical precision with constraints on:

- Sample size  
- Budget  
- Time  
- Staff capacity

### Visualizing Power

**When Power is Low:**  
- Overlapping distributions of H‚ÇÄ and H‚ÇÅ  
- High chance of Type II Error (false negatives)

**When Power is High:**  
- Clear separation between H‚ÇÄ and H‚ÇÅ  
- Low chance of missing a real effect


### Using the `power` Command in Stata

We will use Stata's `power` command to perform sample size and power calculations, and to create sensitivity tables and graphs.[^3]

üìå **Type this in Stata for help:**  
```stata
help power
```
We will revisit the India education intervention example to illustrate three types of power analysis:

::: {.callout-tip collapse="true"}
## üßÆ Case 1: Estimate Required Sample Size
**Goal:** Determine how many participants are needed to detect an expected effect.

**Assumptions:**

- Outcome: Standardized test score (S.D. units)
- Expected effect size: 0.2 S.D.
- Variance: 1.0 (already standardized)
- Desired power: 80%
- Significance level (Œ±): 5%

**Stata code:**
```stata
power twomeans 0 0.2, sd(1) power(0.8) alpha(0.05)
```

This tells Stata to calculate the required sample size for each group (treatment and control) to detect a 0.2 S.D. effect.

You can download a Stata do-file with a clear example of how to perform Case 1 (estimating required sample size) here:

[Download sample-size-power.do](/assets/files/sample-size-power.do)

This do-file contains commented Stata code illustrating the steps for Case 1.

:::

::: {.callout-tip collapse="true"}
## üßÆ Case 2: Estimate Minimum Detectable Effect (MDE)
**Goal:**  Find the smallest effect size you can detect given your sample size.

**Assumptions:**

- Outcome: Standardized test score
- Sample size: 1,000 students (500 per group)
- Power: 80%
- Significance level: 5%

**Stata code:**
```stata
power twomeans 0 (. ), n(1000) sd(1) power(0.8) alpha(0.05)
```

This calculates the minimum effect size that can be detected with the given sample size.

You can download a Stata do-file with a clear example of how to perform Case 2 (estimating minimum detectable effect) here:

[Download mde-power.do](/assets/files/sample-size-power.do)

This do-file contains commented Stata code illustrating the steps for Case 2.
:::

::: {.callout-tip collapse="true"}
## üßÆ Case 3: Estimate Power for a Given Sample Size and Effect
**Goal:** Determine the statistical power based on your sample size and expected effect.

**Assumptions:**

- Outcome: Standardized test score
- Sample size: 1,000 students
- Effect size: 0.2 S.D.
- Significance level: 5%

**Stata code:**
```stata
power twomeans 0 0.2, n(1000) sd(1) alpha(0.05)
```

This estimates the statistical power you'll have in this setup.

You can download a Stata do-file with a clear example of how to perform Case 3 (estimating power for a given sample size and effect) here:

[Download power-simulation.do](/assets/files/power-cals-simulations.do)

This do-file contains commented Stata code that uses simulations to estimate statistical power under the specified assumptions.

:::


::: {.callout-tip collapse="true"}
## üìä Sensitivity Analysis and Graphs
To explore how power changes with different assumptions, you can use Stata's graphing options:

```stata
power twomeans 0 0.2, sd(1) alpha(0.05) graph
```

This will generate a power curve that helps visualize the relationship between sample size and statistical power.
:::

---

## **Determinants of Power Estimation**

Several key factors determine the statistical power of a study. Understanding and adjusting these determinants is crucial for effective research design:

::: {.callout-tip collapse="true"}
## 1. Minimum Detectable Effect (MDE)
The smallest treatment effect you aim to detect. Smaller MDEs require larger sample sizes to achieve the same power.

![Illustration of MDE: Smaller effects require larger samples to detect](/assets/images/power_8.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 2. Sample Size (N)
The total number of study participants. Larger samples increase statistical power and precision.

![Sample size comparison: Small vs. large sample distributions](/assets/images/power_9.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 3. Outcome Variance
The variability in the outcome measure. Lower variance makes it easier to detect effects.

![Outcome variance: Narrow vs. wide outcome distributions](/assets/images/power_10.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 4. Sample Allocation
The proportion assigned to treatment and control groups. Power is maximized when groups are of equal size.

![Sample allocation: Balanced vs. unbalanced groups](/assets/images/power_11.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 5. Non-compliance

- When participants do not adhere to their assigned group.
- Non-compliance reduces the effective sample size and power.
- Non-compliance reduces power via a smaller effect size, or MDE (under no compliance, the estimated treatment effect collapses to zero).
- As we saw before, a smaller effect (MDE) is harder to detect (so less power).
- Increasing compliance is one of your strongest levers to increase power!

![Non-compliance: Some participants do not follow assignment](/assets/images/power_12.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 6. Attrition
- Loss of participants during the study. Attrition reduces sample size and can bias results if not random.
- Attrition reduces power via a smaller study sample size.
- If attrition is correlated with the treatment (e.g., more likely in the control group), the estimated effect is no longer unbiased!
- You must try hard to keep your study sample! (rapport, incentives, tracking respondents, etc.)

![Attrition: Participants lost over time](/assets/images/power_13.png){width=70%}
:::


[^1]: Banerjee, A., Cole, S., Duflo, E., & Linden, L. (2007). Remedying Education: Evidence from Two Randomized Experiments in India. The Quarterly Journal of Economics, 122(3), 1235-1264.
[^2]: Banerjee, A., Cole, S., Duflo, E., & Linden, L. (2017). Balsakhi [Dataset]. Harvard Dataverse.
[^3]: StataCorp. (2023). Stata power and sample size reference manual: Release 18. https://www.stata.com/manuals/power.pdf

