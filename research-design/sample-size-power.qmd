---
title: "Sample and Power Calculations"
abstract: "Essential concepts of statistical power and sample size calculations for randomized evaluations."
date: last-modified

#------------------------------------------------------------------
# Authors
#------------------------------------------------------------------
# Authors are the main creators of the site's content, credited
# for their work and responsible for its core development,
# including writing and editing.
#------------------------------------------------------------------
authors-ipa:
    - "[Cristhian Pulido](https://poverty-action.org/people/cristhian-pulido)"

#------------------------------------------------------------------
# Contributors
#------------------------------------------------------------------
# Contributors provide support, such as feedback or supplementary
# materials for the site. They can also be responsible for
# updating/maintaining the site.
#------------------------------------------------------------------
contributors:
  - "[David Torres](https://poverty-action.org/people/david-francisco-torres-leon)"

keywords:
  - "statistical power"
  - "sample size calculation"
  - "randomized controlled trials"
  - "minimum detectable effect"
  - "research design"
  - "explanation"

license: "CC BY"
---


![An IPA survey in Myanmar in 2020 (Â© IPA)](/assets/images/power-intro.jpg){width=75%}


:::{.callout-tip appearance="simple"}
## Key Takeaways
- **Statistical power** determines the ability to detect true effects in research studies.
- **Sample size** directly affects the precision and reliability of estimates.
- **Statistical concepts** like significance levels, confidence intervals, and effect sizes are crucial for interpreting results.
:::

## What is Statistical Power?

**Statistical power** is the probability that a study will detect a true effect when one actually exists. Think of it as your study's ability to avoid missing something important - like having a metal detector sensitive enough to find buried treasure.

### Why Statistical Power Matters?

::: {.callout-tip collapse="true"}
## 1. Research Quality and Reliability
- Ensures studies can detect meaningful effects
- Reduces risk of false conclusions
- Strengthens confidence in research findings
:::

::: {.callout-tip collapse="true"}
## 2. Resource Optimization
- Prevents wastage of research resources
- Avoids underpowered studies that may be inconclusive
- Guides efficient allocation of study participants
:::

::: {.callout-tip collapse="true"}
## 3. Policy Impact
- Enables evidence-based policy decisions
- Reduces risk of dismissing effective interventions
- Helps identify truly impactful programs
:::

::: {.callout-tip collapse="true"}
## 4. Ethical Considerations
- Respects participants' time and effort
- Justifies the use of research resources
- Promotes responsible funding and implementation
:::

### Real-World Example: Improving Academic Achievement in India[^1]

<div style="background-color: #f7f7f7; padding: 1em; border-radius: 8px; margin-bottom: 1em;">

In India, a significant proportion of children age **7 to 12** were struggling with basic academic skills; **44%** could not read a simple paragraph and **50%** could not solve a basic subtraction problemâ€”highlighting an urgent need for targeted support.

The **Balsakhi tutoring program** addressed this gap by providing **two hours of daily remedial instruction** in **small groups** of 15 to 20 students. To rigorously evaluate its impact, researchers conducted a **large-scale randomized controlled trial** involving **23,000 students**. They assigned students to either a **treatment group** -- receiving Balsakhi tutoring -- or a **control group** -- continuing with the regular curriculum.

### ðŸ“Š The Results

- **Test scores improved** by **0.28 standard deviations** for students who received tutoring
- The **large sample size** gave the study **high statistical power**, which allowed researchers to **confidently detect** this meaningful effect

![Estimated Treatment Effect of 0.28 SD](/assets/images/power_3.png){width=90%}

### ðŸ” Why This Matters: Understanding Statistical Power

This example shows **why statistical power is critical** in research.

Even if a program has a real, positive effect, as Balsakhi did, **a study without enough power might miss it**. Small or underpowered samples are more likely to produce **inconclusive or misleading results**.

Because this study was **well-powered**, it gave strong evidence that the tutoring program worked. If the sample had been smaller, the study might have wrongly concluded that there was no impact.

> ðŸ’¡ **Bottom line:**
> **Statistical power** helps researchers avoid false negatives and ensures that policy decisions are based on **real, reliable evidence** -- not just chance.

This case shows how careful study design and sufficient power can lead to **actionable insights**, helping shape smarter education policies in low-resource settings.[^2]
</div>

::: {.callout-note}
## Key Concept 1
The larger the study sample, the more likely researchers are to estimate the true treatment effect of the program.

::: {layout-ncol=2}
![Sample Size N=200: Wide estimate range](/assets/images/power_1.png){width=75%}

![Sample Size N=2000: Narrow estimate range](/assets/images/power_2.png){width=75%}
:::
:::

---

## **Sample Size: Precision and Reliability**

### Sample Size
Sample size directly influences the ability to estimate the **true treatment effect** with precision and reliability:

- **Smaller N:** High variability, less precise and reliable estimates
- **Larger N:** Lower variability, more precise and reliable estimates

### Why Does Sample Size Matter?

| **Reason**                | Explanation                                                                                   |
|---------------------------|----------------------------------------------------------------------------------------------|
| **Statistical Power**     | Larger samples increase the probability of detecting true effects (higher power).             |
| **Confidence Intervals**  | Bigger samples yield narrower confidence intervals, making estimates more informative.        |
| **Generalizability**      | Adequate sample size ensures findings are more representative of the target population.       |
| **Minimizing Errors**     | Small samples are more prone to random error and outliers, which can distort results.         |

### How to Determine Sample Size?

| **Factor**                  | Impact on Sample Size                                                                                   |
|-----------------------------|--------------------------------------------------------------------------------------------------------|
| **Expected Effect Size**    | Smaller effects require larger samples to detect.                                                      |
| **Outcome Variance**        | More variable outcomes need larger samples for the same precision.                                     |
| **Significance Level (Î±)**  | Lower Î± (e.g., 1%) requires a larger sample than higher Î± (e.g., 10%).                                |
| **Desired Power (1âˆ’Îº)**     | Higher power (e.g., 90%) means a larger sample than lower power (e.g., 80%).                          |
| **Study Design**            | Clustered or stratified designs often require larger samples due to intra-group correlation.           |

### Trade-offs
While larger samples improve precision, they require more resources:

- **Financial:** Survey costs, transport, devices
- **Time:** Recruitment, training, fieldwork
- **Labor:** Enumerators, field staff

### Practical Considerations
- **Minimum Detectable Effect (MDE):** Decide what is the smallest effect worth detectingâ€”this drives sample size.
- **Attrition:** Anticipate and adjust for expected loss of participants over time.
- **Non-compliance:** Plan for imperfect adherence to treatment assignment, which can reduce effective sample size.
- **Ethical Balance:** Avoid exposing more participants than necessary to interventions or control conditions.

::: {.callout-note}
## Key Concept 2
Sample size/power calculations help balance statistical precision with resource constraints:

- **Too small:** Risk missing true treatment effects
- **Too large:** Waste resources unnecessarily
- **Goal:** Find optimal sample to detect meaningful effects reliably
:::

---

## **Understanding Statistical Concepts**

::: {.callout-tip collapse="true"}
### Probability Density Functions (PDFs)

PDFs help visualize the uncertainty around the estimated treatment effect (Î²Ì‚) in a study. They show how likely different values of Î²Ì‚ are, assuming a true treatment effect Î².

![Two overlapping normal distributions representing possible treatment effects](/assets/images/power_4.png){width=75%}
:::

:::{.callout-note}
## Key Concept 3
A narrower distribution of estimates (Î²Ì‚) increases the chances of detecting true effects. You can achieve this by:
- Increasing the sample size (**N**)
- Assigning roughly equal numbers to treatment and control (**p â‰ˆ 0.5**)
- Reducing outcome variance (**ÏƒÂ²**)
:::

::: {.callout-tip collapse="true"}
### Treatment Effects Distribution

If researchers repeated the same RCT many times, they wouldn't get the exact same estimate every time. Instead, the estimated treatment effects (Î²Ì‚) would follow a normal distribution:

$$
\hat{\beta} \sim \text{Normal}(\beta, \sigma^2 / [Np(1-p)])
$$

- **Î²**: True treatment effect
- **ÏƒÂ²**: Variance of the outcome
- **N**: Sample size
- **p**: Proportion assigned to treatment (e.g., 0.5)
:::


:::{.callout-note}
## Key Concept 4
**Type I Error (Î±):** Risk of a false positive
- Usually set at **5% (Î± = 0.05)**
- This means there's a 5% chance of incorrectly detecting an effect when there is none
- A lower Î± reduces false positivesâ€”but increases the risk of missing real effects
- Its complement (1âˆ’Î±) is known as the confidence level -- how likely studies are to avoid a false positive
- While 5% is standard, it can range from 1%-10% in research literature
:::

::: {.callout-tip collapse="true"}
### Hypothesis Testing Framework

To determine whether an effect is statistically significant, researchers use a hypothesis test. This is important because it helps control the risk of making a **Type I error** -- incorrectly concluding that an effect exists when it actually does not, a "false positive." By setting a significance level (Î±), researchers limit the probability of making this error and ensure findings are not due to random chance.

::: {.callout-tip collapse="true"}
## **1. Null Hypothesis, Hâ‚€**
Assumes no effect exists, Î² = 0. This is the starting point for statistical testing and represents the default position that the intervention has no impact.

![Statistical hypothesis testing showing null and alternative distributions](/assets/images/power_ht1.png){width=75%}
:::

::: {.callout-tip collapse="true"}
## **2. Alternative Hypothesis, Hâ‚**
Proposes existence of an effect, Î² â‰  0. Can be one-sided, Î² > 0 or Î² < 0, or two-sided, Î² â‰  0.

![Estimate the treatment effect (ð›½Â Ì‚)](/assets/images/power_ht2.png){width=75%}
:::

::: {.callout-tip collapse="true"}
## **3. Critical Values Computation**
Based on chosen significance level Î±, typically uses Î± = 0.05. Forms rejection regions in the distribution and uses t-distribution or z-distribution.

![Critical Values](/assets/images/power_ht3.png){width=75%}

![Critical Values - Fail To reject](/assets/images/power_ht3_2.png){width=75%}

![Critical Values - Reject](/assets/images/power_ht3_3.png){width=75%}
:::

::: {.callout-tip collapse="true"}
## **4. Effect Estimation and Comparison**
Calculate estimated effect size Î²Ì‚ and compare to critical values. Consider confidence intervals and assess statistical significance.
:::

::: {.callout-tip collapse="true"}
## **5. Statistical Decision**
Reject Hâ‚€ if Î²Ì‚ exceeds critical values. Fail to reject Hâ‚€ if Î²Ì‚  falls within critical bounds. Consider practical significance and report p-values and confidence intervals.

![No rejection of Ho](/assets/images/power_ht4_1.png){width=75%}

![Rejection of Ho](/assets/images/power_ht4_2.png){width=75%}
:::
:::

:::{.callout-note}
## Key Concept 5
**Type II Error (Îº):** Risk of a false negative

- **Îº**: Probability of missing a real effect
- **Power (1âˆ’Îº):** Probability of detecting a true effect (aim for â‰¥80%)
- Lower Î± (false positive rate) increases Îº (false negative rate)
- Power and sample size are directly related: higher power needs a larger sample
:::

---

## Statistical Power in Practice


::: {.callout-tip collapse="true"}
### 1. Power Depends on Study Design

Statistical power is influenced by:

- **Sample size (N)**
- **Effect size (Î²):** how big the true impact is
- **Outcome variance (ÏƒÂ²):** how much outcomes naturally vary

Larger samples and bigger effects increase the chance of detecting something real.

![Low Power: Hard to distinguish effect](/assets/images/power_5.png){width=90%}

![High Power: Clear effect distinction](/assets/images/power_6.png){width=90%}
:::


::: {.callout-tip collapse="true"}
### 2. Power Can Be Visualized

Power is often shown as the overlap between two distributions:

- **Hâ‚€ (Null):** No effect
- **Hâ‚ (Alternative):** A true effect exists
- **Critical value:** The cutoff for statistical significance

The **less overlap** between the two distributions, the **higher the power**.

![Three scenarios showing different power levels based on effect size and sample size](/assets/images/power_7.png){width=90%}
:::

::: {.callout-tip collapse="true"}
### 3. Underpowered vs. Well-Powered Studies

| Scenario              | Underpowered Study (<80%)                     | Well-Powered Study (â‰¥80%)                   |
|-----------------------|-----------------------------------------------|---------------------------------------------|
| **Detection Ability** | Low: true effects may be missed              | High: true effects are likely to be found  |
| **Conclusions**       | Inconclusive or misleading                    | Clear and reliable                          |
| **Resource Use**      | Wasteful: may require redoing the study      | Efficient: leads to actionable findings    |

::: {.callout-warning}
## âš ï¸ Why This Matters
An underpowered study might miss real program effectsâ€”leading to **wrong conclusions** and **poor policy decisions**.
:::
:::

:::{.callout-note}
## Key Concept 6
When power is high (â‰¥80%), studies are more likely to detect true effects and avoid false negatives.
:::
---


## Determinants of Power Estimation

Several key factors determine the statistical power of a study. Understanding and adjusting these determinants is crucial for effective research design:

::: {.callout-tip collapse="true"}
## 1. Minimum Detectable Effect (MDE)
The smallest treatment effect you aim to detect. Smaller MDEs require larger sample sizes to achieve the same power.

![Illustration of MDE: Smaller effects require larger samples to detect](/assets/images/power_8.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 2. Sample Size (N)
The total number of study participants. Larger samples increase statistical power and precision.

![Sample size comparison: Small vs. large sample distributions](/assets/images/power_9.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 3. Outcome Variance
The variability in the outcome measure. Lower variance makes it easier to detect effects.

![Outcome variance: Narrow vs. wide outcome distributions](/assets/images/power_10.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 4. Sample Allocation
The proportion assigned to treatment and control groups. Power is maximized when groups are of equal size.

![Sample allocation: Balanced vs. unbalanced groups](/assets/images/power_11.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 5. Non-compliance

- When participants do not adhere to their assigned group.
- Non-compliance reduces the effective sample size and power.
- Non-compliance reduces power via a smaller effect size, or MDE (under no compliance, the estimated treatment effect collapses to zero).
- As we saw before, a smaller effect (MDE) is harder to detect (so less power).
- Increasing compliance is one of your strongest levers to increase power!

![Non-compliance: Some participants do not follow assignment](/assets/images/power_12.png){width=70%}
:::

::: {.callout-tip collapse="true"}
## 6. Attrition
- Loss of participants during the study. Attrition reduces sample size and can bias results if not random.
- Attrition reduces power via a smaller study sample size.
- If attrition is correlated with the treatment (e.g., more likely in the control group), the estimated effect is no longer unbiased!
- You must try hard to keep your study sample! (rapport, incentives, tracking respondents, etc.)

![Attrition: Participants lost over time](/assets/images/power_13.png){width=70%}
:::

---

## Common Pitfalls and Solutions

::: {.callout-warning collapse="true"}
### Pitfall 1: Ignoring Design Effects

- **Problem:** Using simple formulas for complex designs
- **Solution:** Account for clustering, stratification in calculations
:::

::: {.callout-warning collapse="true"}
### Pitfall 2: Optimistic Assumptions

- **Problem:** Assuming perfect compliance, no attrition
- **Solution:** Build in realistic buffers (typically 20-30%)
:::

::: {.callout-warning collapse="true"}
### Pitfall 3: Post-Hoc Power

- **Problem:** Calculating power after finding null results
- **Solution:** Focus on confidence intervals, not post-hoc power
:::

::: {.callout-warning collapse="true"}
### Pitfall 4: Fixating on 80%

- **Problem:** Treating 80% power as sacred threshold
- **Solution:** Consider contextâ€”sometimes 70% or 90% is appropriate
:::

---

[^1]: Banerjee, A., Cole, S., Duflo, E., & Linden, L. (2007). Remedying Education: Evidence from Two Randomized Experiments in India. The Quarterly Journal of Economics, 122(3), 1235-1264.
[^2]: Banerjee, A., Cole, S., Duflo, E., & Linden, L. (2017). Balsakhi [Dataset]. Harvard Dataverse.
