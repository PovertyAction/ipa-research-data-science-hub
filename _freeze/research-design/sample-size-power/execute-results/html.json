{
  "hash": "f4d09f95e5078d093fa1d2c1a7cd86ad",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Sample and Power Calculations\"\n\n#------------------------------------------------------------------\n# Authors\n#------------------------------------------------------------------\n# Authors are the main creators of the site's content, credited\n# for their work and responsible for its core development,\n# including writing and editing.\n#------------------------------------------------------------------\nauthors-ipa:\n    - \"[Cristhian Pulido](https://poverty-action.org/people/cristhian-pulido)\"\n\n#------------------------------------------------------------------\n# Contributors\n#------------------------------------------------------------------\n# Contributors provide support, such as feedback or supplementary\n# materials for the site. They can also be responsible for\n# updating/maintaining the site.\n#------------------------------------------------------------------\ncontributors:\n  - \"[David Torres](https://poverty-action.org/people/david-francisco-torres-leon)\"\n---\n\n:::{.custom-summary-block}\nThis resource covers essential concepts of statistical power and sample size calculations for randomized evaluations, including hands-on applications using Stata.\n:::\n\n![An IPA survey in Myanmar in 2020 (© IPA)](/assets/images/power-intro.jpg){width=75%}\n\n\n\n:::{.callout-tip appearance=\"simple\"}\n## Key Takeaways\n- **Statistical power** determines our ability to detect true effects in research studies.\n- **Sample size** directly affects the precision and reliability of our estimates.\n- **Statistical concepts** like significance levels, confidence intervals, and effect sizes are crucial for interpreting results.\n- **Practical tools** in Stata help balance statistical power with resource constraints.\n:::\n\n## What is Statistical Power?\n\n**Statistical power** is the probability that a study will detect a true effect when one actually exists. Think of it as your study's ability to avoid missing something important - like having a metal detector sensitive enough to find buried treasure.\n\n### Why Statistical Power Matters?\n\n::: {.callout-tip collapse=\"true\"}\n## 1. Research Quality and Reliability\n- Ensures studies can detect meaningful effects  \n- Reduces risk of false conclusions  \n- Strengthens confidence in research findings  \n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 2. Resource Optimization\n- Prevents wastage of research resources  \n- Avoids underpowered studies that may be inconclusive  \n- Guides efficient allocation of study participants  \n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 3. Policy Impact\n- Enables evidence-based policy decisions  \n- Reduces risk of dismissing effective interventions  \n- Helps identify truly impactful programs  \n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 4. Ethical Considerations\n- Respects participants' time and effort  \n- Justifies the use of research resources  \n- Promotes responsible funding and implementation  \n:::\n\n### Real-World Example: Improving Academic Achievement in India[^1]\n\n<div style=\"background-color: #f7f7f7; padding: 1em; border-radius: 8px; margin-bottom: 1em;\">\n\nIn India, a significant proportion of children age **7 to 12** were struggling with basic academic skills **44%** could not read a simple paragraph and **50%** could not solve a basic subtraction problem—highlighting an urgent need for targeted support. The **Balsakhi tutoring program** addressed this gap by providing **2 hours of daily remedial instruction** in **small groups** of 15 to 20 students. To rigorously evaluate its impact, researchers conducted a **large-scale randomized controlled trial** involving **23,000 students**, randomly assigning them to either a **treatment group** (receiving Balsakhi tutoring) or a **control group** (continuing with the regular curriculum).\n\n### 📊 The Results\n\n- **Test scores improved** by **0.28 standard deviations** for students who received tutoring  \n- The **large sample size** gave the study **high statistical power**, which allowed researchers to **confidently detect** this meaningful effect\n\n![Estimated Treatment Effect of 0.28 SD](/assets/images/power_3.png){width=90%}\n\n### 🔍 Why This Matters: Understanding Statistical Power\n\nThis example shows **why statistical power is critical** in research.  \n\nEven if a program has a real, positive effect, as Balsakhi did, **a study without enough power might miss it**. That's because small or underpowered samples are more likely to produce **inconclusive or misleading results**.\n\nBecause this study was **well-powered**, it gave strong evidence that the tutoring program worked.  \nIf the sample had been smaller, the study might have wrongly concluded that there was no impact.\n\n> 💡 **Bottom line:**  \n> **Statistical power** helps researchers avoid false negatives and ensures that policy decisions are based on **real, reliable evidence**—not just chance.\n\nThis case shows how careful study design and sufficient power can lead to **actionable insights**, helping shape smarter education policies in low-resource settings. [^2]\n</div>\n\n::: {.callout-note}\n## Key Concept 1  \nThe larger the study sample, the more likely we are to estimate the true treatment effect of the program.\n\n::: {layout-ncol=2}\n![Sample Size N=200: Wide estimate range](/assets/images/power_1.png){width=75%}\n\n![Sample Size N=2000: Narrow estimate range](/assets/images/power_2.png){width=75%}\n:::\n:::\n\n---\n\n## **Sample Size: Precision and Reliability**\n\n### Sample Size\nSample size directly influences our ability to estimate the **true treatment effect** with precision and reliability:\n\n- **Smaller N:** High variability, less precise and reliable estimates  \n- **Larger N:** Lower variability, more precise and reliable estimates\n\n### Why Does Sample Size Matter?\n\n| **Reason**                | Explanation                                                                                   |\n|---------------------------|----------------------------------------------------------------------------------------------|\n| **Statistical Power**     | Larger samples increase the probability of detecting true effects (higher power).             |\n| **Confidence Intervals**  | Bigger samples yield narrower confidence intervals, making estimates more informative.        |\n| **Generalizability**      | Adequate sample size ensures findings are more representative of the target population.       |\n| **Minimizing Errors**     | Small samples are more prone to random error and outliers, which can distort results.         |\n\n### How to Determine Sample Size?\n\n| **Factor**                  | Impact on Sample Size                                                                                   |\n|-----------------------------|--------------------------------------------------------------------------------------------------------|\n| **Expected Effect Size**    | Smaller effects require larger samples to detect.                                                      |\n| **Outcome Variance**        | More variable outcomes need larger samples for the same precision.                                     |\n| **Significance Level (α)**  | Lower α (e.g., 1%) requires a larger sample than higher α (e.g., 10%).                                |\n| **Desired Power (1−κ)**     | Higher power (e.g., 90%) means a larger sample than lower power (e.g., 80%).                          |\n| **Study Design**            | Clustered or stratified designs often require larger samples due to intra-group correlation.           |\n\n### Trade-offs  \nWhile larger samples improve precision, they require more resources:\n\n- **Financial:** Survey costs, transport, devices  \n- **Time:** Recruitment, training, fieldwork  \n- **Labor:** Enumerators, field staff\n\n### Practical Considerations\n- **Minimum Detectable Effect (MDE):** Decide what is the smallest effect worth detecting—this drives sample size.\n- **Attrition:** Anticipate and adjust for expected loss of participants over time.\n- **Non-compliance:** Plan for imperfect adherence to treatment assignment, which can reduce effective sample size.\n- **Ethical Balance:** Avoid exposing more participants than necessary to interventions or control conditions.\n\n::: {.callout-note}\n## Key Concept 2\nSample size/power calculations help balance statistical precision with resource constraints:\n\n- **Too small:** Risk missing true treatment effects\n- **Too large:** Waste resources unnecessarily\n- **Goal:** Find optimal sample to detect meaningful effects reliably\n:::\n\n---\n\n## **Understanding Statistical Concepts**\n\n::: {.callout-tip collapse=\"true\"}\n### Probability Density Functions (PDFs)\n\nPDFs help us visualize the uncertainty around the estimated treatment effect (β̂) in a study. They show how likely different values of β̂ are, assuming a true treatment effect β.\n\n![Two overlapping normal distributions representing possible treatment effects](/assets/images/power_4.png){width=75%}\n:::\n\n:::{.callout-note}\n## Key Concept 3  \nA narrower distribution of estimates (β̂) increases our chances of detecting true effects. You can achieve this by:  \n- Increasing the sample size (**N**)  \n- Assigning roughly equal numbers to treatment and control (**p ≈ 0.5**)  \n- Reducing outcome variance (**σ²**)\n:::\n\n::: {.callout-tip collapse=\"true\"}\n### Treatment Effects Distribution\n\nIf we repeated the same RCT many times, we wouldn't get the exact same estimate every time. Instead, the estimated treatment effects (β̂) would follow a normal distribution:\n\n$$\n\\hat{\\beta} \\sim \\text{Normal}(\\beta, \\sigma^2 / [Np(1-p)])\n$$\n\n- **β**: True treatment effect  \n- **σ²**: Variance of the outcome  \n- **N**: Sample size  \n- **p**: Proportion assigned to treatment (e.g., 0.5)\n:::\n\n\n:::{.callout-note}\n## Key Concept 4  \n**Type I Error (α):** Risk of a false positive  \n- Usually set at **5% (α = 0.05)**  \n- This means there's a 5% chance we incorrectly detect an effect when there is none  \n- A lower α reduces false positives—but increases the risk of missing real effects\n- Its complement (1−α) is known as the confidence level (how likely we are to avoid a false positive)\n- While 5% is standard, it can range from 1%-10% in research literature\n:::\n\n::: {.callout-tip collapse=\"true\"}\n### Hypothesis Testing Framework\n\nTo determine whether an effect is statistically significant, we use a hypothesis test. This is important because it helps us control the risk of making a **Type I error** incorrectly concluding that an effect exists when it actually does not (a \"false positive\"). By setting a significance level (α), we limit the probability of making this error and ensure our findings are not due to random chance.\n\n::: {.callout-tip collapse=\"true\"}\n## **1. Null Hypothesis (H₀)**\nAssumes no effect exists (β = 0). This is the starting point for statistical testing and represents the default position that the intervention has no impact.\n\n![Statistical hypothesis testing showing null and alternative distributions](/assets/images/power_ht1.png){width=75%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## **2. Alternative Hypothesis (H₁)**\nProposes existence of an effect (β ≠ 0). Can be one-sided (β > 0 or β < 0) or two-sided (β ≠ 0).\n\n![Estimate the treatment effect (𝛽 ̂)](/assets/images/power_ht2.png){width=75%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## **3. Critical Values Computation**\nBased on chosen significance level (α), typically uses α = 0.05 (5%). Forms rejection regions in the distribution and uses t-distribution or z-distribution.\n\n![Critical Values](/assets/images/power_ht3.png){width=75%}\n\n![Critical Values - Fail To reject](/assets/images/power_ht3_2.png){width=75%}\n\n![Critical Values - Reject](/assets/images/power_ht3_3.png){width=75%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## **4. Effect Estimation and Comparison**\nCalculate estimated effect size (β̂), compare to critical values, consider confidence intervals, and assess statistical significance.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## **5. Statistical Decision**\nReject H₀ if β̂ exceeds critical values, fail to reject H₀ if β̂ within critical bounds. Consider practical significance and report p-values and confidence intervals.\n\n![No rejection of Ho](/assets/images/power_ht4_1.png){width=75%}\n\n![Rejection of Ho](/assets/images/power_ht4_2.png){width=75%}\n:::\n:::\n\n:::{.callout-note}\n## Key Concept 5  \n**Type II Error (κ):** Risk of a false negative\n\n- **κ**: Probability of missing a real effect  \n- **Power (1−κ):** Probability of detecting a true effect (aim for ≥80%)  \n- Lower α (false positive rate) increases κ (false negative rate)  \n- Power and sample size are directly related: higher power needs a larger sample\n:::\n\n---\n\n## Statistical Power in Practice\n\n\n::: {.callout-tip collapse=\"true\"}\n### 1. Power Depends on Study Design\n\nStatistical power is influenced by:\n\n- **Sample size (N)**\n- **Effect size (β):** how big the true impact is\n- **Outcome variance (σ²):** how much outcomes naturally vary\n\nLarger samples and bigger effects increase the chance of detecting something real.\n\n![Low Power: Hard to distinguish effect](/assets/images/power_5.png){width=90%}\n\n![High Power: Clear effect distinction](/assets/images/power_6.png){width=90%}\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n### 2. Power Can Be Visualized\n\nPower is often shown as the overlap between two distributions:\n\n- **H₀ (Null):** No effect  \n- **H₁ (Alternative):** A true effect exists  \n- **Critical value:** The cutoff for statistical significance\n\nThe **less overlap** between the two distributions, the **higher the power**.\n\n![Three scenarios showing different power levels based on effect size and sample size](/assets/images/power_7.png){width=90%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n### 3. Underpowered vs. Well-Powered Studies\n\n| Scenario              | Underpowered Study (<80%)                     | Well-Powered Study (≥80%)                   |\n|-----------------------|-----------------------------------------------|---------------------------------------------|\n| **Detection Ability** | Low: true effects may be missed              | High: true effects are likely to be found  |\n| **Conclusions**       | Inconclusive or misleading                    | Clear and reliable                          |\n| **Resource Use**      | Wasteful: may require redoing the study      | Efficient: leads to actionable findings    |\n\n::: {.callout-warning}\n## ⚠️ Why This Matters\nAn underpowered study might miss real program effects—leading to **wrong conclusions** and **poor policy decisions**.\n:::\n:::\n\n:::{.callout-note}\n## Key Concept 6\nWhen power is high (≥80%), we are more likely to detect true effects and avoid false negatives.\n:::\n---\n\n## Practical Tools and Applications in Stata\n\n### Using the `power` Command in Stata\n\nWe will use Stata's `power` command to perform sample size and power calculations, and to create sensitivity tables and graphs.[^3]\n\n📌 **Type this in Stata for help:**  \n```stata\nhelp power\n```\nWe will revisit the India education intervention example to illustrate three types of power analysis:\n\n::: {.callout-tip collapse=\"true\"}\n## 🧮 Case 1: Estimate Required Sample Size\n**Goal:** Determine how many participants are needed to detect an expected effect.\n\n**Assumptions:**\n\n- Effect size (MDE): 0.2 S.D.\n- Statistical power: 80%\n- Variance of the outcome variable: 1 S.D.\n- A single treatment group (and a control group)\n- A single post-treatment data collection (endline)\n- Program was randomized at the individual (child) level\n- Perfect compliance\n- No attrition\n- No additional controls\n\n**Stata code:**\n\n::: {#4961e284 .cell execution_count=2}\n``` {.python .cell-code}\n%%stata\npower twomeans 0 0.2 , power(0.8) sd(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPerforming iteration ...\n\nEstimated sample sizes for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.8000\n        delta =    0.2000\n           m1 =    0.0000\n           m2 =    0.2000\n           sd =    1.0000\n\nEstimated sample sizes:\n\n            N =       788\n  N per group =       394\n```\n:::\n:::\n\n\nThis tells Stata to calculate the required sample size for each group (treatment and control) to detect a 0.2 S.D. effect. The required study sample size to achieve a power of 80% is 788 students (394 in the treatment group, 394 in the control group).\n\n**Stata code:**\n\nNow, suppose we expect a larger treatment effect of 0.4 standard deviations (SD) instead of 0.2 SD, keeping all other assumptions the same. We can calculate the required sample size for this new scenario as follows:\n\n::: {#7895ba66 .cell execution_count=3}\n``` {.python .cell-code}\n%%stata\npower twomeans 0 0.4 , power(0.8) sd(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPerforming iteration ...\n\nEstimated sample sizes for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.8000\n        delta =    0.4000\n           m1 =    0.0000\n           m2 =    0.4000\n           sd =    1.0000\n\nEstimated sample sizes:\n\n            N =       200\n  N per group =       100\n```\n:::\n:::\n\n\n**Stata code:**\n\nNow, let's change our assumption: **What if we want to increase the statistical power to 90%?**\n\nWe can calculate the required sample size for an effect size of 0.4 SD and 90% power as follows:\n\n::: {#9aaa1b93 .cell execution_count=4}\n``` {.python .cell-code}\n%%stata\npower twomeans 0 0.2 , power(0.9) sd(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPerforming iteration ...\n\nEstimated sample sizes for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.9000\n        delta =    0.2000\n           m1 =    0.0000\n           m2 =    0.2000\n           sd =    1.0000\n\nEstimated sample sizes:\n\n            N =     1,054\n  N per group =       527\n```\n:::\n:::\n\n\nThe required sample size would increase to 1,054.\n\nAdditional exercises are available in the following do file: [Download power.do](/assets/files/sample-size-power.do)\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 🧮 Case 2: Estimate Minimum Detectable Effect (MDE)\n**Goal:**  Find the smallest effect size you can detect given your sample size.\n\n**Assumptions:**\n\n- Sample size: 1,000 students (500 per group)\n- Power: 80%\n- Variance of the outcome variable: 1 S.D\n\n**Stata code:**\n\n::: {#b97c54ff .cell execution_count=5}\n``` {.python .cell-code}\n%%stata\npower twomeans 0 , n(1000) power(0.9) sd(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPerforming iteration ...\n\nEstimated experimental-group mean for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1; m2 > m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.9000\n            N =     1,000\n  N per group =       500\n           m1 =    0.0000\n           sd =    1.0000\n\nEstimated effect size and experimental-group mean:\n\n        delta =    0.2052\n           m2 =    0.2052\n```\n:::\n:::\n\n\nThis calculates the minimum effect size that can be detected with the given sample size.\n\n**Stata code:**\n\nAs we can see in the graph, with 500, 750, 1000, 1250, and 1500 students, we are able to detect smaller treatment effects as the sample size increases.\n\n::: {#4d1b66a6 .cell execution_count=6}\n``` {.python .cell-code}\n%%stata\npower twomeans 0 , n(500(250)1500) power(0.8) sd(1) graph\n```\n\n::: {.cell-output .cell-output-display}\n![](sample-size-power_files/figure-html/cell-7-output-1.svg){}\n:::\n:::\n\n\nAdditional exercises are available in the following do file: [Download power.do](/assets/files/sample-size-power.do)\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 🧮 Case 3: Estimate Power for a Given Sample Size and Effect\n**Goal:** Determine the statistical power based on your sample size and expected effect.\n\n**Assumptions:**\n\n- Study sample size: 1,000\n- Effect size: 0.2 S.D.\n- Variance of the outcome variable: 1 S.D.\n\n**Stata code:**\n\n::: {#3f51f425 .cell execution_count=7}\n``` {.python .cell-code}\n%%stata\npower twomeans 0 0.2 , n(1000) sd(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEstimated power for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n            N =     1,000\n  N per group =       500\n        delta =    0.2000\n           m1 =    0.0000\n           m2 =    0.2000\n           sd =    1.0000\n\nEstimated power:\n\n        power =    0.8848\n```\n:::\n:::\n\n\nThe estimated power under these assumptions is power = 0.8848.\n\n**Stata code:**\n\nSmaller samples lead to less power. As we increase the sample size, we're able to reach a larger probability of avoiding a type II error (false negative).\nIn the graph above, you can see that as the sample size increases, the estimated power also increases. This means you are more likely to detect a true effect with a larger sample.\n\n::: {#a9984e40 .cell execution_count=8}\n``` {.python .cell-code}\n%%stata\npower twomeans 0 0.2 , n(100(100)1000) sd(1) graph\n```\n\n::: {.cell-output .cell-output-display}\n![](sample-size-power_files/figure-html/cell-9-output-1.svg){}\n:::\n:::\n\n\nAdditional exercises are available in the following do file: [Download power.do](/assets/files/sample-size-power.do)\n:::\n\n---\n\n## Determinants of Power Estimation\n\nSeveral key factors determine the statistical power of a study. Understanding and adjusting these determinants is crucial for effective research design:\n\n::: {.callout-tip collapse=\"true\"}\n## 1. Minimum Detectable Effect (MDE)\nThe smallest treatment effect you aim to detect. Smaller MDEs require larger sample sizes to achieve the same power.\n\n![Illustration of MDE: Smaller effects require larger samples to detect](/assets/images/power_8.png){width=70%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 2. Sample Size (N)\nThe total number of study participants. Larger samples increase statistical power and precision.\n\n![Sample size comparison: Small vs. large sample distributions](/assets/images/power_9.png){width=70%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 3. Outcome Variance\nThe variability in the outcome measure. Lower variance makes it easier to detect effects.\n\n![Outcome variance: Narrow vs. wide outcome distributions](/assets/images/power_10.png){width=70%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 4. Sample Allocation\nThe proportion assigned to treatment and control groups. Power is maximized when groups are of equal size.\n\n![Sample allocation: Balanced vs. unbalanced groups](/assets/images/power_11.png){width=70%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 5. Non-compliance\n\n- When participants do not adhere to their assigned group.\n- Non-compliance reduces the effective sample size and power.\n- Non-compliance reduces power via a smaller effect size, or MDE (under no compliance, the estimated treatment effect collapses to zero).\n- As we saw before, a smaller effect (MDE) is harder to detect (so less power).\n- Increasing compliance is one of your strongest levers to increase power!\n\n![Non-compliance: Some participants do not follow assignment](/assets/images/power_12.png){width=70%}\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## 6. Attrition\n- Loss of participants during the study. Attrition reduces sample size and can bias results if not random.\n- Attrition reduces power via a smaller study sample size.\n- If attrition is correlated with the treatment (e.g., more likely in the control group), the estimated effect is no longer unbiased!\n- You must try hard to keep your study sample! (rapport, incentives, tracking respondents, etc.)\n\n![Attrition: Participants lost over time](/assets/images/power_13.png){width=70%}\n:::\n\n---\n\n## Common Pitfalls and Solutions\n\n::: {.callout-warning collapse=\"true\"}\n### Pitfall 1: Ignoring Design Effects\n\n- **Problem:** Using simple formulas for complex designs  \n- **Solution:** Account for clustering, stratification in calculations\n:::\n\n::: {.callout-warning collapse=\"true\"}\n### Pitfall 2: Optimistic Assumptions\n\n- **Problem:** Assuming perfect compliance, no attrition  \n- **Solution:** Build in realistic buffers (typically 20-30%)\n:::\n\n::: {.callout-warning collapse=\"true\"}\n### Pitfall 3: Post-Hoc Power\n\n- **Problem:** Calculating power after finding null results  \n- **Solution:** Focus on confidence intervals, not post-hoc power\n:::\n\n::: {.callout-warning collapse=\"true\"}\n### Pitfall 4: Fixating on 80%\n\n- **Problem:** Treating 80% power as sacred threshold  \n- **Solution:** Consider context—sometimes 70% or 90% is appropriate\n:::\n\n---\n\n## Additional Resources\n\n- [Stata Power and Sample Size Manual](https://www.stata.com/manuals/power.pdf)\n- [Optimal Design Software](https://www.wtgrantfoundation.org/optimal-design-software): Free tool for complex designs\n- [G*Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html): User-friendly power calculator\n- [J-PAL's Power Calculations Guide](https://www.povertyactionlab.org/resource/power-calculations)\n\n[^1]: Banerjee, A., Cole, S., Duflo, E., & Linden, L. (2007). Remedying Education: Evidence from Two Randomized Experiments in India. The Quarterly Journal of Economics, 122(3), 1235-1264.\n[^2]: Banerjee, A., Cole, S., Duflo, E., & Linden, L. (2017). Balsakhi [Dataset]. Harvard Dataverse.\n[^3]: StataCorp. (2023). Stata power and sample size reference manual: Release 18. https://www.stata.com/manuals/power.pdf\n\n",
    "supporting": [
      "sample-size-power_files"
    ],
    "filters": [],
    "includes": {}
  }
}