[
  {
    "objectID": "software/vscode/index.html",
    "href": "software/vscode/index.html",
    "title": "Visual Studio Code",
    "section": "",
    "text": "Visual Studio Code (VS Code) is a versatile code editor used for writing, editing, and developing text-based documents and programs. It is recommended for collaborative code-first data and technology projects at IPA.",
    "crumbs": [
      "Software Guides",
      "Code Environments",
      "VS Code"
    ]
  },
  {
    "objectID": "software/vscode/index.html#how-to-install-vs-code",
    "href": "software/vscode/index.html#how-to-install-vs-code",
    "title": "Visual Studio Code",
    "section": "How to Install VS Code?",
    "text": "How to Install VS Code?\nTo install VS Code, download and install the latest software version from the Visual Studio Code website or run the following from the command line if you have the Windows Package Manager, winget, installed:\n\nWindowsMacOSLinux\n\n\nwinget install Microsoft.VisualStudioCode\n\n\nbrew install --cask visual-studio-code\n\n\nsudo snap install code --classic",
    "crumbs": [
      "Software Guides",
      "Code Environments",
      "VS Code"
    ]
  },
  {
    "objectID": "software/vscode/index.html#using-vs-code",
    "href": "software/vscode/index.html#using-vs-code",
    "title": "Visual Studio Code",
    "section": "Using VS Code",
    "text": "Using VS Code\nThe best starting point for familiarizing yourself with VS Code is the documentation.\nSome particularly helpful references in the documentation are:\n\nEditing with VS Code\nGit Source control in VS Code\nPython in VS Code",
    "crumbs": [
      "Software Guides",
      "Code Environments",
      "VS Code"
    ]
  },
  {
    "objectID": "software/vscode/index.html#recommended-extensions",
    "href": "software/vscode/index.html#recommended-extensions",
    "title": "Visual Studio Code",
    "section": "Recommended Extensions",
    "text": "Recommended Extensions\nThe core software and functionality in VS Code gives you a lot of useful tools for writing, editing, and collaborating. Additionally, you can add functionality through Extensions.\nSome helpful extensions are the following:\n\nPython Extension - language syntax for Python programming\nRuff - Python linting and code formatting\nR Extension - Interacting with R from VS Code\nGitHub Pull Requests - review and manage GitHub pull requests and issues in Visual Studio Code\nJupyter Extension - for developing with jupyter notebooks\nJust - support for Justfiles\nQuarto Extension - support for writing and building Quarto projects\nMarkdown All in One - support for Markdown\nMarkdown Preview Mermaid Support - support for Mermaid diagrams in Markdown\nConventional Commits - support for writing easy to understand commit messages",
    "crumbs": [
      "Software Guides",
      "Code Environments",
      "VS Code"
    ]
  },
  {
    "objectID": "software/stata/data-processing-stata.html#sorting-data",
    "href": "software/stata/data-processing-stata.html#sorting-data",
    "title": "Data Processing in Stata",
    "section": "Sorting data",
    "text": "Sorting data\nNot only could it be useful, but crucial, to sort your observations in a particular way when cleaning or creating outcomes.\nYou can use the sort command in Stata to achieve this. Of course you can order your observation based on ordering one variable, but you can go further and sort your data on multiple variables. For example if you have a long dataset that contains two variables person id and survey round and for each person it has three survey rounds, then if you sort id round you will sort the data by person and within each person you will sort by the survey rounds.\n\nsort sorts observations in ascending order (i.e.¬†lowest to highest)\nMissing values in stata are equivalent to infinity and thus will be sorted to the bottom of your sort if they exist\n\n*Example of points 1 and 2 above\nsysuse bplong, clear\n    sort when patient\n    sort patient when\n    preserve\n    replace when = . if _n == 25\n    sort when patient //where did the missing value get sorted to?\nrestore\n\nYou can flip the order you sort by using gsort and using a negative sign in front of the variable name (i.e.¬†sort largest to smallest)\n\nsysuse bplong, clear\n*Use sort to see how it normally sorts males first (smallest to largest)\nsort sex patient\n\n/*\ngsort by -sex to see how to sort largest to smallest\n(Notice the patient order does not change within gender)\n*/\ngsort -sex patient\n\nIf the observations of the variables you sort on are not unique, Stata will randomize their order in a new randomization every time you sort (i.e.¬†you will not get a consistent order if you re-run your code, even in a script)\n\nsysuse bplong, clear\n/*\nNotice that gender-patient does not uniquely identify\nour observations\n*/\nsort sex patient\n/*\nFlag the row numbers where Stata sorted the \"before\"\nobservations for each person\n*/\ngen flag_before1 = _n if when == 1\n*Do the exact same sort as above\nsort sex patient\n/*\nAgain flag the row numbers where Stata sorted the \"before\"\nobservations for each person this time\n*/\ngen flag_before2 = _n if when == 1\n/*\nNotice that the two flag_before variables are not always equal i.e.\nthe \"before\" observation ended up in a different place even though\nwe did the same sort twice\n*/\n\nYou have two options to make sure your sorts are consistent\n\nUse the option stable to make sure Stata uses the same randomization every time\n\nYou cannot use this option with `gsort‚Äô\n\nThe preferred method is to specify a combination of variables that uniquely identifies your observations. This removes the randomization and makes your sort outcome be exactly what you specify and expect\n\n\nsysuse bplong, clear\n/*\nNotice that gender-patient does not uniquely identify\nour observations\n*/\nsort sex patient when\n/*\nFlag the row numbers where Stata sorted the \"before\"\nobservations for each person\n*/\ngen flag_before1 = _n if when == 1\n*Do the exact same sort as above\nsort sex patient when\n/*\nAgain flag the row numbers where Stata sorted the \"before\"\nobservations for each person this time\n*/\ngen flag_before2 = _n if when == 1\n*Now the flags are always equal\n\nThe by function\nYou can use the by function to create variables within groups, but in order to use by you must sort before hand. Thus, we recommend to use bysort instead.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Data Processing in Stata"
    ]
  },
  {
    "objectID": "software/stata/data-processing-stata.html#bysort-gen-and-egen",
    "href": "software/stata/data-processing-stata.html#bysort-gen-and-egen",
    "title": "Data Processing in Stata",
    "section": "bysort, gen, and egen",
    "text": "bysort, gen, and egen\nbysort combined with gen/egen is probably one of the most useful command combinations when cleaning and creating outcomes.\n\nNotice that your data set will be sorted by all the variables (including those in parenthesis) you specify\nBut you will create new variables by only what variables you specify outside the parenthesis\nPay attention to whether the function you are using needs to specify gen or egen\n\nNotice that sum works for both gen and egen (even though it is not in the egen documentation and works differently\n\negen + sum = creates a total for all values specified in the by\ngen + sum = creates a cumulative sum over the observations specified\n\n\n\nSee help egen to read about all of the egen functions\nFurther: - You can ssc install egen more that has even more functions you can use. - You can ssc install ereplace to be able to use the egen functions but as a replace, so you don‚Äôt have to create multiples variables.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Data Processing in Stata"
    ]
  },
  {
    "objectID": "software/stata/data-processing-stata.html#preservingrestoring-data",
    "href": "software/stata/data-processing-stata.html#preservingrestoring-data",
    "title": "Data Processing in Stata",
    "section": "Preserving/restoring data",
    "text": "Preserving/restoring data\nYou can use collapse when you want to create summary statistics of your data, or some of your variables. Note that collapse works by replacing your data with the summary statistics of each variable that you request. If you are familiar with egen, you can think of collapse as equivalent to egen, except than rather making a new variable it replaces your variables. Additionally, any variables you don‚Äôt specify will be dropped. This means this command erases your data. Because of this destructive nature there are several best practices to use around collapse.\nIt is common you would like to maintain your dataset while outputting some summary statistics. You can quickly do this by preserving, collapsing, and then restoring your data.\nsysuse census, clear\npreserve\ncollapse (mean) pop (median) medage, by(region)\nsave \"example.dta\", replace\nrestore\n\n\n\n\n\n\nExplain why you use the statistic you choose\n\n\n\nIn the comments of your do-file, you should write why you chose the collapse statistic you chose for each variable. This is especially important when multiple statistics would result in the same thing and you chose one arbitrarily. For example, if you have a constant variable for what you are collapsing on, so you pick mean instead of mode or first non missing, this is important for someone to know later. If they are adding in more data and run into errors they need to know why you picked what you did and it will help them understand the data structure and errors.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Data Processing in Stata"
    ]
  },
  {
    "objectID": "software/stata/data-processing-stata.html#asserting-beforehand",
    "href": "software/stata/data-processing-stata.html#asserting-beforehand",
    "title": "Data Processing in Stata",
    "section": "Asserting beforehand",
    "text": "Asserting beforehand\nIt is important you code asserts before you collapse to check that you‚Äôre variables are what you are expecting. For example, if you think you have a constant var among the variables you are collapsing on - you should check prior to collapsing. If you are wrong, you could not know based on the stat you choose, and it is hard to check after the collapse since the data is gone.\n\n\n\n\n\n\nUsing egen, duplicates drop instead\n\n\n\nAn alternative to using collapse is using egen and the dropping duplicates instead. This way once you make your statistics you can do some assert functions to check that everything was created the way you think they were created.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Data Processing in Stata"
    ]
  },
  {
    "objectID": "software/stata/conditions-operators-stata.html",
    "href": "software/stata/conditions-operators-stata.html",
    "title": "Research and Data Science Hub",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "software/quarto/index.html",
    "href": "software/quarto/index.html",
    "title": "Getting started with Quarto",
    "section": "",
    "text": "The following video provides a quick introduction to Quarto:",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Getting started with Quarto"
    ]
  },
  {
    "objectID": "software/quarto/index.html#how-to-install-quarto",
    "href": "software/quarto/index.html#how-to-install-quarto",
    "title": "Getting started with Quarto",
    "section": "How to install Quarto?",
    "text": "How to install Quarto?\n\nWindowsMacOSLinux\n\n\n# Install Quarto\nwinget install Posit.Quarto\n\n\n# Install Quarto\nbrew install --cask quarto\n\n\nTo install Quarto on Linux, see https://quarto.org/docs/get-started/",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Getting started with Quarto"
    ]
  },
  {
    "objectID": "software/quarto/index.html#using-quarto",
    "href": "software/quarto/index.html#using-quarto",
    "title": "Getting started with Quarto",
    "section": "Using Quarto",
    "text": "Using Quarto\nWe recommend using the Quarto Extension for Visual Studio Code. You can install the extension by searching for ‚ÄúQuarto‚Äù in the Extensions view (Ctrl+Shift+X). Alternatively, you can install the extension from the Visual Studio Code Marketplace.\nIf you use Jupyter Lab or RStudio, Quarto works directly with those tools as well.\nThe best place to start for learning how to use Quarto is Quarto‚Äôs tutorial, Hello, Quarto.\nAs an example, this repository uses Quarto to build the handbook.\nIn the root of the git repository, there is a _quarto.yml file that contains the metadata used to compile the handbook. Components of the handbook are written in Markdown, either standard Markdown (.md) or Quarto Markdown (.qmd). The benefit of standard markdown is that it is more portable and works with standard Markdown editors (e.g.¬†GitHub), while Quarto markdown allows for computations that are embedded in the document. If you want more advanced formatting or computations, .qmd may be a preferable choice. The Quarto Documentation provides helpful guides on how to use Markdown as well as the added features that Quarto Markdown offers.\nFor an example of how to use Quarto within common academic and research workflows, see:",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Getting started with Quarto"
    ]
  },
  {
    "objectID": "software/quarto/index.html#learning-resources",
    "href": "software/quarto/index.html#learning-resources",
    "title": "Getting started with Quarto",
    "section": "Learning Resources",
    "text": "Learning Resources\n\nTutorials:\n\nHello, Quarto\nComputations\nAuthoring\n\nQuarto Documentation\nQuarto Gallery of Examples",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Getting started with Quarto"
    ]
  },
  {
    "objectID": "software/python/index.html#how-to-install-python",
    "href": "software/python/index.html#how-to-install-python",
    "title": "Python",
    "section": "How to install Python?",
    "text": "How to install Python?\nThere are many ways to install Python. We recommend using Python in a virtual environment to avoid conflicts with other Python installations on your system.\nWe recommend using uv as a a simple way to create and manage Python virtual environments.\nYou can manage the python packages that are installed in the virtual environment using a pyproject.toml file. See the pyproject.toml example in this repository for an example of how to manage Python packages. To add package dependencies to the virtual environment, using uv, you can run:\nFirst, install uv using winget (Windows) or brew (MacOS/Linux):\n\nWindowsMacOSLinux\n\n\n# Install uv\nwinget install astral-sh.uv\n\n\n# Install uv\nbrew install uv\n\n\n# Install uv\nbrew install uv\n\n\n\nAdd libraries to the virtual environment using uv add ...:\n\n&gt; uv add jupyterlab pandas matplotlib seaborn",
    "crumbs": [
      "Software Guides",
      "Python",
      "Getting Started with Python"
    ]
  },
  {
    "objectID": "software/python/index.html#coding-conventions",
    "href": "software/python/index.html#coding-conventions",
    "title": "Python",
    "section": "Coding Conventions",
    "text": "Coding Conventions\nWe highly recommend working with a virtual environment to manage Python dependencies. The pyproject.toml is the preferred way to keep track of python dependencies as well as project-specific python conventions.\nWe recommend using Ruff to enforce linting and formatting rules. In most cases you can use the default linting and formatting rules provided by ruff. However, you can customize the rules by modifying the [tool.ruff] section of the pyproject.toml file in the root of your project. for more about the configuration options, see the Ruff documentation.\nIf you are working in a virtual environment created in this repository, you automatically have access to Ruff via just lint-py and just fmt-python commands to lint and format your code.\nFor more inspiration, see the GitLab Data Team‚Äôs Python Guide and Google‚Äôs Python Style Guide.",
    "crumbs": [
      "Software Guides",
      "Python",
      "Getting Started with Python"
    ]
  },
  {
    "objectID": "software/python/index.html#example-usage",
    "href": "software/python/index.html#example-usage",
    "title": "Python",
    "section": "Example Usage",
    "text": "Example Usage\nIn the example below, we show how Python can be used to explore and visualize a dataset.\n\n\n\n\n\n\nImportant Note\n\n\n\n\n\nTo follow along, you will need to work in a jupyter notebook with the right libraries installed in your environment. Don‚Äôt worry if you cannot do this now; we just want to show you what is possible here. We will revisit this example in Processing Data in Python.\n\n\n\nLet‚Äôs load an example World Bank data via Gapminder using the causaldata package.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as sm\nfrom causaldata import gapminder\n\nLoad the Gapminder data as a pandas DataFrame:\n\ndf = gapminder.load_pandas().data\n\nWe can check the dimensions of the DataFrame using df.info():\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   continent  1704 non-null   object \n 2   year       1704 non-null   int64  \n 3   lifeExp    1704 non-null   float64\n 4   pop        1704 non-null   int64  \n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(2), int64(2), object(2)\nmemory usage: 80.0+ KB\n\n\nLet‚Äôs take a look at the first few rows of the DataFrame using df.head():\n\ndf.head()\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n\n\nTake a look at the relationship between GDP per Capita and Life Expectancy:\n\nsns.scatterplot(x=\"gdpPercap\", y=\"lifeExp\", hue=\"continent\", data=df).set(\n    xscale=\"log\", ylabel=\"Life Expectancy\", xlabel=\"GDP per Capita\"\n)\n\n\n\n\n\n\n\n\nSeparate the data by year, focusing on 1957 and 2007:\n\nsns.relplot(\n    data=df.where(df[\"year\"].isin([1957, 2007])),\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    col=\"year\",\n    hue=\"continent\",\n    col_wrap=1,\n    kind=\"scatter\",\n    palette=\"muted\",\n).set(xscale=\"log\", ylabel=\"Life Expectancy\", xlabel=\"GDP per Capita\")",
    "crumbs": [
      "Software Guides",
      "Python",
      "Getting Started with Python"
    ]
  },
  {
    "objectID": "software/python/index.html#learning-resources",
    "href": "software/python/index.html#learning-resources",
    "title": "Python",
    "section": "Learning Resources",
    "text": "Learning Resources\n\nThe Python Tutorial\nPython Data Science Handbook\nEfficient Python for Data Scientists\nThe Hitchhiker‚Äôs Guide to Python",
    "crumbs": [
      "Software Guides",
      "Python",
      "Getting Started with Python"
    ]
  },
  {
    "objectID": "software/index.html#what-is-stata",
    "href": "software/index.html#what-is-stata",
    "title": "Software Guides",
    "section": "What is Stata?",
    "text": "What is Stata?\nStata is a statistical software package that is commonly used in the social sciences and economics. It is widely used at IPA for data analysis and management. It offers a comprehensive library of methods for data cleaning, descriptive statistics, and econometric analysis. Stata is very well suited for research data workflows and research design tasks, including power calculations, sample design adjustments, panel data analysis, time series analysis, etc. See Stata Features for a full list of what Stata makes available."
  },
  {
    "objectID": "software/github/index.html#how-to-install-github-software",
    "href": "software/github/index.html#how-to-install-github-software",
    "title": "GitHub",
    "section": "How to install GitHub Software?",
    "text": "How to install GitHub Software?\nThere are two main options to consider for interacting with GitHub from your local computer:\n\nGitHub Desktop (Recommended)- GUI for working with Git repositories.\nGitHub CLI - Command-line interface for working with Git repositories. For advanced usage.\n\nIf you are new to Git or prefer working with a graphical user interface (GUI), we recommend that you start with GitHub Desktop. The Desktop interface provides a more transparent way of understanding source control and interacting with remote code repositories on GitHub.\n\nWindowsMacOSLinux\n\n\n# Install GitHub Desktop\nwinget install GitHub.GithubDesktop\n\n# Install GitHub Commandline Interface (CLI)\nwinget install GitHub.cli\n\n\n# Install GitHub Desktop\nbrew install --cask github\n\n# Install Github Commandline Interface (CLI)\nbrew install gh\n\n\n# GitHub Desktop not available.\n\n\n# Install GitHub Commandline Interface (CLI)\nbrew install gh",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "GitHub"
    ]
  },
  {
    "objectID": "software/github/index.html#authenticating-github",
    "href": "software/github/index.html#authenticating-github",
    "title": "GitHub",
    "section": "Authenticating GitHub",
    "text": "Authenticating GitHub\n\nGitHub Desktop\nSee instructions here for getting started with GitHub Desktop.\nIn the File menu, select ‚ÄúOptions‚Äù and then in the ‚ÄúAccounts‚Äù options select ‚ÄúSign in to GitHub.com‚Äù and ‚ÄúContinue with browser‚Äù to authenticate with your GitHub account.\n\n\n\nGitHub Desktop Authentication\n\n\n\n\nGitHub CLI\nSee the GitHub CLI Manual for more information on how to authenticate with GitHub CLI.\nTo authenticate with GitHub CLI, run the following command in your terminal:\ngh auth login\nThen walk through the prompts:\n\nWhat account do you want to log into? GitHub.com\nWhat is your preferred protocol for Git operations on this host? HTTPS\nAuthenticate Git with your GitHub credentials? (Y/n) Y\nHow would you like to authenticate GitHub CLI? Login with a web browser",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "GitHub"
    ]
  },
  {
    "objectID": "software/github/index.html#using-github",
    "href": "software/github/index.html#using-github",
    "title": "GitHub",
    "section": "Using GitHub",
    "text": "Using GitHub\nWithin a GitHub repository, there are four main aspects that you should be familiar with:\n\nCode: The files and directories that make up your project.\nIssues: A place to discuss and track tasks, bugs, and enhancements for a project.\nPull Requests: A way to propose changes to a repository and discuss them with others.\n\nIn the Code section of a GitHub repository, you can view the files that make up the project codebase. We work with Branches to manage different versions of the codebase. The main branch is the default branch that GitHub uses for the codebase. When you want to make changes to the codebase, you create a new branch from the main branch, make your changes, and then create a Pull Request to merge your changes back into the main branch.\nFor planning changes to the codebase, you can use Issues to track tasks, bugs, and enhancements. Issues can be assigned to team members, labeled, and linked to Pull Requests. Issues can also be used to discuss changes to the codebase before making them.",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "GitHub"
    ]
  },
  {
    "objectID": "software/github/index.html#learning-resources",
    "href": "software/github/index.html#learning-resources",
    "title": "GitHub",
    "section": "Learning Resources",
    "text": "Learning Resources\n\nGitHub Skills provides a lot of relevant resources for learning how to use GitHub. Some good starting points include:\n\nIntroduction to GitHub\nReview Pull Requests\n\nGitHub Foundations Certificate",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "GitHub"
    ]
  },
  {
    "objectID": "research-ethics/irb-review.html",
    "href": "research-ethics/irb-review.html",
    "title": "IRB Review Requirements",
    "section": "",
    "text": "Guidance on when IRB review requirements apply, including exemptions from review requirements, pilot study considerations, and ongoing obligations for exempt research.\nIt can be challenging to determine whether‚Äîor the timing of when‚Äîyou must submit a project for IRB review. This page outlines the different circumstances in which a project may not require IRB review.",
    "crumbs": [
      "Ethics and IPA IRB",
      "IRB review"
    ]
  },
  {
    "objectID": "research-ethics/irb-review.html#is-your-project-human-subjects-research",
    "href": "research-ethics/irb-review.html#is-your-project-human-subjects-research",
    "title": "IRB Review Requirements",
    "section": "Is your project human subjects research?",
    "text": "Is your project human subjects research?\nIRB review requirements apply only to projects considered ‚Äúhuman subjects research.‚Äù\nIn somewhat rare cases, IPA works on projects which fall outside of this category, either because:\n\nThe project does not involve ‚Äúhuman subjects‚Äù according to federal regulations, or\nThe project is not considered ‚Äúresearch‚Äù according to federal regulations.\n\n\nDoes your project involve human subjects?\nFederal regulations define a ‚Äúhuman subject‚Äù as:\nA living individual about whom an investigator conducting research:\n\nObtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or\nObtains, uses, studies, analyzes, or generates identifiable private information or identifiable biospecimens.\n\nThe ‚Äúabout whom‚Äù clause of this definition is important. This means that if you are collecting data from someone, but this data is not in any way ‚Äúabout‚Äù them, they may not count as a human subject.\nFor example: if your project involves interviewing factory workers, and you will only ask them questions about operations at the factory ‚Äì what time it opens, steps in the manufacturing process, etc. ‚Äì these interviewees may not count as human subjects. This applies as long as you do not collect PII or any other information ‚Äúabout‚Äù them.\nThis sometimes comes up in the context of projects which involve interviews with staff or other key players in a program which IPA is evaluating. These interviews may not count as human subjects research if the questions focus only on learning about the program and how it functions‚Äînot anything specific to that interviewee or their experience.\nHowever, if your survey includes questions asking the interviewee for their opinion about something‚Äîe.g., what do you think about this part of the program?‚Äîthis may count as human subjects research.\n\n\nIs your project research?\nFederal regulations define ‚Äúresearch‚Äù as:\nA systematic investigation, including research development, testing, and evaluation, designed to develop or contribute to generalizable knowledge.\nThe end of this definition ‚Äì ‚Äúdesigned to develop or contribute to generalizable knowledge‚Äù ‚Äì is the part most often relevant for determining whether IPA projects are considered research.\n‚ÄúGeneralizable knowledge‚Äù typically means that the project will generate information that expands the knowledge base of a scientific discipline or other scholarly field of study and yield one or both of the following:\n\nResults that are applicable to a larger population beyond the site of data collection or the specific subjects studied.\nResults that you will use to develop, test, or support theories, principles, and statements of relationships, or to tell policy makers beyond the study.*\n\nSome projects that IPA works on fall outside of this category because their purpose is ‚Äúprogram evaluation only.‚Äù This means that the project‚Äôs purpose is only to evaluate a specific program, and the only stakeholders interested in results of the study are program administrators or other entities interested specifically in the development or efficacy of that program. The results of the project will not support conclusions about these types of programs in general.\nProjects which fit the above description may not qualify as ‚Äúresearch‚Äù and thus may not require IRB review.\n*Explanation borrowed from University of Washington Human Subjects Division",
    "crumbs": [
      "Ethics and IPA IRB",
      "IRB review"
    ]
  },
  {
    "objectID": "research-ethics/irb-review.html#do-pilot-activities-need-irb-review",
    "href": "research-ethics/irb-review.html#do-pilot-activities-need-irb-review",
    "title": "IRB Review Requirements",
    "section": "Do pilot activities need IRB review?",
    "text": "Do pilot activities need IRB review?\nSometimes‚Ä¶Read on!\nThe purpose of a pilot is typically to evaluate the feasibility of a study and help researchers refine data collection procedures, instruments, or research design. They are usually small in scale and exploratory in nature.\nPilot activities are typically aimed at preparing for future, core research activities‚Äîthose which will generate results that answer the main research questions.\nPilot activities are thus not research if you intend them only for this purpose and they will not contribute to generalizable knowledge. This means that you will not use results of the pilot as research data.\nPilot studies therefore typically do not require IRB review, with one key exception.\n\n\n\n\n\n\nException\n\n\n\nIf the pilot itself may create notable risks for subjects, IPA IRB may wish to review these activities. If your pilot will involve particularly vulnerable subjects, particularly sensitive questions/data collection, or research methods which may create greater than minimal risks, consult with IPA IRB at humansubjects@poverty-action.org before beginning fieldwork.\n\n\nIf you will use data collected through a pilot study for generalization or publication purposes‚Äîsolely or in combination with other data‚ÄîIRB review requirements apply before data collection begins. You may not later repurpose pilot data collected without IRB approval as study data, and the IRB cannot provide retroactive review.*\n*Explanation borrowed from Georgia Southern University Research Integrity",
    "crumbs": [
      "Ethics and IPA IRB",
      "IRB review"
    ]
  },
  {
    "objectID": "research-ethics/irb-review.html#your-project-is-human-subjects-researchnow-what",
    "href": "research-ethics/irb-review.html#your-project-is-human-subjects-researchnow-what",
    "title": "IRB Review Requirements",
    "section": "Your project is human subjects research‚Äînow what?",
    "text": "Your project is human subjects research‚Äînow what?\nIf your project is human subjects research ‚Äì that is, it involves human subjects, and it is research rather than a pilot ‚Äì you must submit an initial application for IRB review.\nHowever, it is still possible that your project may qualify for exemption from some future IRB oversight requirements.\n\nCategories of exempt research\nFederal regulations outline several categories of research which qualify as ‚Äúexempt.‚Äù These decision charts provide more information.\nWhether your research project fits within one or more of these categories can be difficult to interpret, so the IRB must make this determination‚Äîyou cannot decide that your project is exempt.\nProjects eligible for exemption are those which are low risk. Eliminating unnecessary risks is the best way to increase the likelihood that the IRB will find your project exempt. For example: if you are conducting a study involving only one round of data collection, and it is not necessary to collect PII, opting not to collect any PII will make your study more fit for exemption.\nIf your study involves deception, prisoner subjects, or child subjects, there are additional limits on whether the IRB can determine your study exempt.\n\n\nWhat does exemption mean?\nLet‚Äôs say you‚Äôve submitted an initial application for your project, and the IRB has determined it ‚Äòexempt.‚Äô What does this mean for your IRB obligations going forward?\nA determination of exemption means that the study does not need ongoing IRB oversight. This applies as long as the research remains minimal risk and stays within the bounds of the exemption categories that the IRB determined apply to this study.\nIn plain language, this means that:\n\nYou do not need to submit renewals, as long as the project stays exempt.\nYou do not need to submit amendments, as long as the changes you are making do not change or undermine the study‚Äôs eligibility for exemption.\n\n\n\n\n\n\n\nImportant\n\n\n\nProjects may begin as exempt but change substantively over time such that they no longer qualify for exemption. It is crucial that you submit to the IRB any changes which may impact the exemption status of the study.\n\n\nThese may include:\n\nAdding procedures which do not fit within the original exemption categories\nFor example: if your original plan only involved working with secondary data or educational testing, and you are now planning to survey other participants.\nChanges or new procedures that could affect risks to participants\nRemoval of the consent process, or use of deception or incomplete disclosure.\nSignificant changes to the recruitment procedures.\nCollection of new or additional identifiable information.\nAdding sensitive questions to a survey or interview process**, such as questions regarding illegal activities; traumatic events such as childhood, sexual, or domestic abuse; suicide; or other probing questions that could reasonably place the subjects at risk of criminal or civil liability or damage the subjects‚Äô financial standing, employability, educational advancement, or reputation.\nChanges to the data storage plan which may affect confidentiality.\nAdding new types of participants, especially vulnerable populations: children, individuals with cognitive impairments, prisoners\n\nIf you have any uncertainty about whether a particular change necessitates an amendment submission for your exempt project, contact the IPA IRB at humansubjects@poverty-action.org.\nOther types of IRB submissions are still required:\n\nYou must still submit unexpected events if the research receives any participant complaints or experiences any unanticipated problems that involve risks to participants or others.\nYou must close the study once human subjects activities have concluded.",
    "crumbs": [
      "Ethics and IPA IRB",
      "IRB review"
    ]
  },
  {
    "objectID": "research-ethics/irb-lifecycle.html",
    "href": "research-ethics/irb-lifecycle.html",
    "title": "The IRB Lifecycle",
    "section": "",
    "text": "The typical IRB lifecycle consists of multiple stages including the initial application, amendments, renewals, reporting unexpected events, and closure.",
    "crumbs": [
      "Ethics and IPA IRB",
      "The IRB Lifecycle"
    ]
  },
  {
    "objectID": "research-ethics/irb-lifecycle.html#initial-application",
    "href": "research-ethics/irb-lifecycle.html#initial-application",
    "title": "The IRB Lifecycle",
    "section": "Initial Application",
    "text": "Initial Application\nThe IRB project lifecycle starts with the online submission of the initial application. Research teams submit an initial application to formally apply for IRB review and must secure approval before implementing a new research study. During this process, the IRB determines the review level ‚Äì expedited or full board ‚Äì and frequency of continuing review‚Äîone or three years.\nThe submission should include the following documents:\n\nIPA IRB Initial Application Form‚Äîonce approved, this becomes the study‚Äôs protocol\nInstruments (e.g., surveys, interview/focus group discussion guides, observation forms)\nInformed consent forms, the IPA IRB consent checklist and template helps ensure all required and assessed elements are included\nHuman Subjects Certificates for all Principal Investigators (PIs) and research personnel. Human Subjects training certification must be renewed every three years to remain valid for approval\nIRB approval from local IRBs and/or other institutions\nData Sharing Agreements\nAny other supporting documents\n\nIPA IRB evaluates submissions on a rolling basis in the order they receive them. Upon receipt of an IRB submission for review, the IRB evaluates the research study to determine whether it involves no more than minimal risk to the subjects.\nMinimal risk means that the probability and magnitude of harm or discomfort anticipated in the research are not greater than those ordinarily encountered in daily life or routine procedures.\nYou can expect to receive feedback on your submission‚Äôs risk assessment within 10 business days of your initial submission. If the submission is incomplete or requires further clarification/modification, the approval timeline will depend on the nature of the necessary changes and the responsiveness of the responsible research staff.\nIf the IRB determines your project is minimal risk, it will undergo an expedited review. In expedited review, an IPA IRB representative designated by the IPA IRB Chair conducts the review rather than the full board. The IRB conducts these reviews on an ongoing basis.\nThis means the research team does not need to wait for the next full board meeting for review or approval. Practically speaking, this means that the review usually takes less time than a non-minimal risk study.\nResearch teams cannot request expedited review, as this reflects the study‚Äôs risk level, which only the designated representative of IPA IRB can determine.\nIf a project is non-minimal risk, the IRB Coordinator or the chair will refer the study to the full IRB Board. The Board will review the study at the monthly meeting and then either propose changes or grant a decision.\nIf you think your project may be non-minimal risk, you must submit the complete materials for the study by the deadline associated with the corresponding targeted meeting date. If your submission is missing documents at the deadline, the IRB may not consider it for that meeting date.\nNon-minimal risk applications that may need full board review should allow for 60 days to process. The convened board may have questions or comments for the research staff that need re-discussion at the next meeting before approval.\nPlease note that if your pilot activity is collecting identifiable data that will be used during the study, you must budget enough time for the IRB to approve your application before starting your pilot activities.",
    "crumbs": [
      "Ethics and IPA IRB",
      "The IRB Lifecycle"
    ]
  },
  {
    "objectID": "research-ethics/irb-lifecycle.html#amendments",
    "href": "research-ethics/irb-lifecycle.html#amendments",
    "title": "The IRB Lifecycle",
    "section": "Amendments",
    "text": "Amendments\nAmendments are modifications to the previously approved study protocol or documents. Research teams should have IRB review and approval of these modifications before implementation to ensure that the regulatory criteria for IRB approval are met and that the risk/benefit ratio remains reasonable. Research teams must submit amendments through online submission and should include the following documents:\n\nIPA IRB Amendment Form\nUpdated protocol\nUpdated instruments, if proposing changes\nUpdated informed consent forms, if proposing changes\nUpdated Human Subjects Certificates, if expired or proposing changes to PIs and/or research personnel\nAny other supporting documents\n\nLike application submissions, the IRB does not review all amendment submissions at a full board meeting; only non-minimal risk submissions require full board review. As long as your modification does not increase the risk of the research, the IRB will review it under expedited procedures. Regardless, you should submit your amendment as early as possible.\nNote that if you are also due for a renewal, you can submit for changes and for the renewal ‚Äì which extends the approval by one year or three years ‚Äì with just the renewal form. More information on renewals can be found below.",
    "crumbs": [
      "Ethics and IPA IRB",
      "The IRB Lifecycle"
    ]
  },
  {
    "objectID": "research-ethics/irb-lifecycle.html#renewals",
    "href": "research-ethics/irb-lifecycle.html#renewals",
    "title": "The IRB Lifecycle",
    "section": "Renewals",
    "text": "Renewals\nRenewals allow the IRB to provide continuing oversight of a research study by reviewing its progress and determining whether it continues to meet the regulatory criteria for approval. The IRB determines the renewal frequency in the initial application review. You can find this information in the approval letter, and it depends on risk level, funding and start date.\nThe renewal frequency is every three years for studies that meet the following requirements:\n\nAre minimal risk\nAre not federally funded\nDon‚Äôt include vulnerable populations\nDon‚Äôt include sensitive questions\nStarted after October 2022\n\nAll other studies should be renewed annually.\nResearch teams must submit renewals through online submission and should include the following documents:\n\nIPA IRB Renewal Form\nUpdated protocol (if also proposing changes)\nUpdated instruments, if also proposing changes\nUpdated informed consent forms, if also proposing changes\nUpdated Human Subjects Certificates, if expired or proposing changes to PIs and/or research personnel\nAny other supporting documents\n\n\n\n\n\n\n\nAvoid delays or other issues by following IRB dates\n\n\n\nResearch teams should submit renewals well in advance of the expiration date to avoid lapses in IRB approval. If a study‚Äôs approval expires, all research activities involving human subjects‚Äîdata collection, contact with subjects, analysis of personally identifiable data (PII)‚Äîmust stop until the IRB grants renewal.\nIn the progress report, you must also list the project‚Äôs progress since last review, indicate why you missed the deadline, what human subjects research activities have taken place during the lapsed time, if any, and what steps the PI will take to avoid missing deadlines in the future.\nDepending on the circumstances and length of delay in a continuing review, you may also need to submit an unexpected event report, as this may represent serious or continuing non-compliance with reporting implications. If the renewal includes modifications, the IRB should review and approve these proposed changes before implementation.\n\n\nNote that renewal submissions are due each year or every three years, as applicable, at the time that the IRB approved the Initial Application, not a year or three years after the most recent Amendment approval.",
    "crumbs": [
      "Ethics and IPA IRB",
      "The IRB Lifecycle"
    ]
  },
  {
    "objectID": "research-ethics/irb-lifecycle.html#unexpected-events",
    "href": "research-ethics/irb-lifecycle.html#unexpected-events",
    "title": "The IRB Lifecycle",
    "section": "Unexpected Events",
    "text": "Unexpected Events\nUnexpected events are events that are not listed as risks in the protocol and/or informed consent forms like deviations or adverse events that result in harm to subjects. Research teams must report them to the IRB through online submission within five calendar days and should include the following documents:\n\nIPA IRB Unexpected Event Form\nUpdated protocol, if needed\nAny other supporting documents\n\nIf you miss the deadline for reporting an unexpected event, it is still in the best interest of your project to report the event regardless of when it occurred. IPA is committed to protection of human subjects and will work with all Country Programs to ensure appropriate protections occur and that research teams take appropriate responses when an unexpected event happens.\n\n\n\n\n\n\nTip\n\n\n\nAfter submission, the IRB may require a corrective action plan to address the problem and, depending on the findings, may require additional reporting, for example, to the donor.",
    "crumbs": [
      "Ethics and IPA IRB",
      "The IRB Lifecycle"
    ]
  },
  {
    "objectID": "research-ethics/irb-lifecycle.html#closure",
    "href": "research-ethics/irb-lifecycle.html#closure",
    "title": "The IRB Lifecycle",
    "section": "Closure",
    "text": "Closure\nA closure submission notifies the IRB the study is complete and ends the study team‚Äôs obligation to provide updates, resulting in the end of the IRB oversight. Research teams must submit closures through online submission and should include the following documents:\n\nIPA IRB Closure Form\nAny other supporting documents\n\nResearch teams can close studies when data collection or interaction with subjects is complete and data analysis with PII is complete.\nPlease note also that IPA IRB does not maintain a PII destruction policy and advises the following instead:\n\nResearch teams may never publish identified data in a public repository without respondent consent. Please verify that your dataset does not contain any identifiers before publishing.\nKeeping identified information on file increases the risk of a confidentiality breach. IPA IRB encourages investigators to weigh and re-assess this risk against the benefit of keeping this information, considering their project‚Äôs unique circumstances and content.\nIf the research team keeps identifiable information on file, they must keep this information confidential. IPA IRB assumes that the data security protections for the project PII will remain the same after the study‚Äôs closure, unless the research team has notified them of other arrangements. IPA IRB retains the right to disallow alternative data security and confidentiality procedures for specific projects on a case-by-case basis.\nIf the research team intends to contact subjects past the closure of the study for any reason, the original informed consent must have allowed for the possibility of re-contact for the express purpose proposed by the researchers. The reviewing IRB for the closed study‚Äîand, if applicable, the IRB reviewing any new study‚Äîmust also approve re-contact before the research team may implement it.",
    "crumbs": [
      "Ethics and IPA IRB",
      "The IRB Lifecycle"
    ]
  },
  {
    "objectID": "research-ethics/irb-faqs.html",
    "href": "research-ethics/irb-faqs.html",
    "title": "Research and Data Science Hub",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Ethics and IPA IRB",
      "IRB FAQs üöß"
    ]
  },
  {
    "objectID": "research-ethics/index.html",
    "href": "research-ethics/index.html",
    "title": "About IPA IRB",
    "section": "",
    "text": "Institutional Review Boards exist to protect the rights, safety, and welfare of human subjects involved in research projects. Learn about IPA‚Äôs IRB, coverage options, meeting schedules, and fee structures.",
    "crumbs": [
      "Ethics and IPA IRB",
      "About IPA IRB"
    ]
  },
  {
    "objectID": "research-ethics/index.html#what-is-an-institutional-review-board",
    "href": "research-ethics/index.html#what-is-an-institutional-review-board",
    "title": "About IPA IRB",
    "section": "What is an Institutional Review Board?",
    "text": "What is an Institutional Review Board?\nAn Institutional Review Board (IRB) is a group designated by an institution (e.g., a university or non-profit) to provide ethical and regulatory oversight of research involving human subjects. An IRB reviews, monitors, and, if necessary, requires modifications to these type of studies. Made up of IRB staff and board members, its primary role is to ensure that appropriate measures are taken to protect participants‚Äô rights, safety and welfare.\nIRBs operate based on ethical principles outlined in the Belmont Report‚Äîbeneficence, respect for persons, and justice‚Äîand adhere to minimum standards set by U.S. federal regulations, including the Department of Health and Human Services Regulations (45 CFR part 46). For human subject research conducted internationally, additional local regulations and international standards apply.\n\nHuman Subjects Research\nAccording to 45 CFR part 46, human subjects research means:\n\nResearch: Systematic investigation, including research development, testing and evaluation, designed to develop or contribute to generalizable knowledge.\nHuman subject: Living individual about whom an investigator (whether a professional or student) conducting research (i) obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or (ii) obtains, uses, studies, analyzes, or generates identifiable private information or identifiable biospecimens.\n\n\n\n\n\n\n\nNeed help determining if your project involves Human Subjects Research?\n\n\n\nIRB approval is mandatory for human subjects research. For more information on whether your project constitutes human subjects research, please consult the Office for Human Research Protections decision charts or contact IPA IRB at humansubjects@poverty-action.org.",
    "crumbs": [
      "Ethics and IPA IRB",
      "About IPA IRB"
    ]
  },
  {
    "objectID": "research-ethics/index.html#about-the-ipa-irb",
    "href": "research-ethics/index.html#about-the-ipa-irb",
    "title": "About IPA IRB",
    "section": "About the IPA IRB",
    "text": "About the IPA IRB\nIn 2007, IPA established an IRB that would provide ethical oversight to international studies that were primarily randomized trials with vulnerable populations in low and middle income countries where IPA operates. This IRB is hosted by IPA and its administrative functions are handled by IPA staff, but it is independent of IPA staff, management, and researchers. The membership of the IPA IRB is diverse in affiliations and is composed of members who are external to IPA. The IRB convenes on a monthly basis.\nIRB approval is a fundamental part of IPA‚Äôs research quality protocols. IPA requires that all projects in which IPA is engaged be reviewed by a U.S. or a similar IRB institution, as well as nationally recognized (or equal) IRB in the country where the research is taking place (if such an IRB exists) before the start of any human subjects activities. When deciding how to arrange IRB coverage for an IPA project, the options are the following:\n\n\nIPA IRB: IPA IRB conducts its own review of the project.\nHealth Media Lab (HML) IRB: HML IRB may review the project in place of IPA IRB. Only projects which are minimal risk, involve no particularly sensitive questions or vulnerable subjects, and are not federally funded should submit to HML IRB.\nReliance agreement with other institution: In some cases, IRB review may be required at another institution (e.g., the PI‚Äôs university, or the institution funding the study). If you prefer that IPA IRB cedes oversight to this other institution‚Äôs IRB, we will need to set up a ‚Äúreliance agreement‚Äù formally documenting this arrangement.\n\n\n\nMeeting Dates\nThe IRB at IPA convenes regularly to review non-minimal risk studies and other submissions requiring full board review. To ensure timely consideration, researchers must submit all required materials by the deadlines listed in Table¬†1 below. Submissions received after the deadline will be reviewed at the next available meeting. IRB meetings take place from 15:00 to 16:00 EST on the scheduled dates. Please plan accordingly to avoid delays in your study‚Äôs approval process.\n\n\n\nTable¬†1: IPA IRB Meeting Dates\n\n\n\n\n\n\n\n\n\nMeeting Date\nDeadline for Submission of Materials\n\n\n\n\nThursday, March 27, 2025\nFriday, March 7, 2025, COB\n\n\nThursday, April 24, 2025\nFriday, April 4, 2025, COB\n\n\nThursday, May 29, 2025\nFriday, May 9, 2025, COB\n\n\nThursday, June 26, 2025\nFriday, June 6, 2025, COB\n\n\nThursday, July 31, 2025\nFriday, July 11, 2025, COB\n\n\nThursday, August 28, 2025\nFriday, August 8, 2025, COB\n\n\nThursday, September 25, 2025\nFriday, September 5, 2025, COB\n\n\nThursday, October 30, 2025\nFriday, October 10, 2025, COB\n\n\nThursday, November 20, 2025\nFriday, October 31, 2025, COB\n\n\nThursday, December 18, 2025\nFriday, November 28, 2025, COB\n\n\n\n\n\n\n\n\nIRB Fee Schedule\nThe IPA IRB charges fees for its reviews based on the IPA IRB fee schedule. For more information on these fees, see the fee update documentation. The IRB determines these fees based on several factors. These include staff time and training costs, such as processing submissions, executing reliance agreements, advising projects and responding to questions, preparing templates and customized guidance on consent forms, and recruiting and working with Board members. They also include Board member honoraria, annual CITI organizational subscription for human subjects training, and comparison research to identify fees charged by other IRBs completed in 2024.\nResearch teams should include IRB fees in every research project‚Äôs budget. Consider discussing IRB requirements during the project development stage with your donors, PIs, and implementing partners to ensure adequate budgeting for the appropriate IRB costs.",
    "crumbs": [
      "Ethics and IPA IRB",
      "About IPA IRB"
    ]
  },
  {
    "objectID": "research-design/sample-size-power.html",
    "href": "research-design/sample-size-power.html",
    "title": "Sample and Power Calculations",
    "section": "",
    "text": "Essential concepts of statistical power and sample size calculations for randomized evaluations.",
    "crumbs": [
      "Research Design",
      "Sample and Power Calculations"
    ]
  },
  {
    "objectID": "research-design/sample-size-power.html#what-is-statistical-power",
    "href": "research-design/sample-size-power.html#what-is-statistical-power",
    "title": "Sample and Power Calculations",
    "section": "What is Statistical Power?",
    "text": "What is Statistical Power?\nStatistical power is the probability that a study will detect a true effect when one actually exists. Think of it as your study‚Äôs ability to avoid missing something important - like having a metal detector sensitive enough to find buried treasure.\n\n\n\n\n\n\nNote\n\n\n\nFor a practical introductory guide to power calculations, see Guide to Power Calculations.\n\n\n\nWhy Statistical Power Matters?\n\n\n\n\n\n\n1. Research Quality and Reliability\n\n\n\n\n\n\nEnsures studies can detect meaningful effects\nReduces risk of false conclusions\nStrengthens confidence in research findings\n\n\n\n\n\n\n\n\n\n\n2. Resource Optimization\n\n\n\n\n\n\nPrevents wastage of research resources\nAvoids underpowered studies that may be inconclusive\nGuides efficient allocation of study participants\n\n\n\n\n\n\n\n\n\n\n3. Policy Impact\n\n\n\n\n\n\nEnables evidence-based policy decisions\nReduces risk of dismissing effective interventions\nHelps identify truly impactful programs\n\n\n\n\n\n\n\n\n\n\n4. Ethical Considerations\n\n\n\n\n\n\nRespects participants‚Äô time and effort\nJustifies the use of research resources\nPromotes responsible funding and implementation\n\n\n\n\n\n\nReal-World Example: Improving Academic Achievement in India1\n\nIn India, a significant proportion of children age 7 to 12 were struggling with basic academic skills; 44% could not read a simple paragraph and 50% could not solve a basic subtraction problem‚Äîhighlighting an urgent need for targeted support.\nThe Balsakhi tutoring program addressed this gap by providing two hours of daily remedial instruction in small groups of 15 to 20 students. To rigorously evaluate its impact, researchers conducted a large-scale randomized controlled trial involving 23,000 students. They assigned students to either a treatment group ‚Äì receiving Balsakhi tutoring ‚Äì or a control group ‚Äì continuing with the regular curriculum.\n\nüìä The Results\n\nTest scores improved by 0.28 standard deviations for students who received tutoring\nThe large sample size gave the study high statistical power, which allowed researchers to confidently detect this meaningful effect\n\n\n\n\nEstimated Treatment Effect of 0.28 SD\n\n\n\n\nüîç Why This Matters: Understanding Statistical Power\nThis example shows why statistical power is critical in research.\nEven if a program has a real, positive effect, as Balsakhi did, a study without enough power might miss it. Small or underpowered samples are more likely to produce inconclusive or misleading results.\nBecause this study was well-powered, it gave strong evidence that the tutoring program worked. If the sample had been smaller, the study might have wrongly concluded that there was no impact.\n\nüí° Bottom line: Statistical power helps researchers avoid false negatives and ensures that policy decisions are based on real, reliable evidence ‚Äì not just chance.\n\nThis case shows how careful study design and sufficient power can lead to actionable insights, helping shape smarter education policies in low-resource settings.2\n\n\n\n\n\n\n\n\nKey Concept 1\n\n\n\nThe larger the study sample, the more likely researchers are to estimate the true treatment effect of the program.\n\n\n\n\n\n\nSample Size N=200: Wide estimate range\n\n\n\n\n\n\n\nSample Size N=2000: Narrow estimate range",
    "crumbs": [
      "Research Design",
      "Sample and Power Calculations"
    ]
  },
  {
    "objectID": "research-design/sample-size-power.html#sample-size-precision-and-reliability",
    "href": "research-design/sample-size-power.html#sample-size-precision-and-reliability",
    "title": "Sample and Power Calculations",
    "section": "Sample Size: Precision and Reliability",
    "text": "Sample Size: Precision and Reliability\n\nSample Size\nSample size directly influences the ability to estimate the true treatment effect with precision and reliability:\n\nSmaller N: High variability, less precise and reliable estimates\nLarger N: Lower variability, more precise and reliable estimates\n\n\n\nWhy Does Sample Size Matter?\n\n\n\n\n\n\n\nReason\nExplanation\n\n\n\n\nStatistical Power\nLarger samples increase the probability of detecting true effects (higher power).\n\n\nConfidence Intervals\nBigger samples yield narrower confidence intervals, making estimates more informative.\n\n\nGeneralizability\nAdequate sample size ensures findings are more representative of the target population.\n\n\nMinimizing Errors\nSmall samples are more prone to random error and outliers, which can distort results.\n\n\n\n\n\nHow to Determine Sample Size?\n\n\n\n\n\n\n\nFactor\nImpact on Sample Size\n\n\n\n\nExpected Effect Size\nSmaller effects require larger samples to detect.\n\n\nOutcome Variance\nMore variable outcomes need larger samples for the same precision.\n\n\nSignificance Level (Œ±)\nLower Œ± (e.g., 1%) requires a larger sample than higher Œ± (e.g., 10%).\n\n\nDesired Power (1‚àíŒ∫)\nHigher power (e.g., 90%) means a larger sample than lower power (e.g., 80%).\n\n\nStudy Design\nClustered or stratified designs often require larger samples due to intra-group correlation.\n\n\n\n\n\nTrade-offs\nWhile larger samples improve precision, they require more resources:\n\nFinancial: Survey costs, transport, devices\nTime: Recruitment, training, fieldwork\nLabor: Enumerators, field staff\n\n\n\nPractical Considerations\n\nMinimum Detectable Effect (MDE): Decide what is the smallest effect worth detecting‚Äîthis drives sample size.\nAttrition: Anticipate and adjust for expected loss of participants over time.\nNon-compliance: Plan for imperfect adherence to treatment assignment, which can reduce effective sample size.\nEthical Balance: Avoid exposing more participants than necessary to interventions or control conditions.\n\n\n\n\n\n\n\nKey Concept 2\n\n\n\nSample size/power calculations help balance statistical precision with resource constraints:\n\nToo small: Risk missing true treatment effects\nToo large: Waste resources unnecessarily\nGoal: Find optimal sample to detect meaningful effects reliably",
    "crumbs": [
      "Research Design",
      "Sample and Power Calculations"
    ]
  },
  {
    "objectID": "research-design/sample-size-power.html#understanding-statistical-concepts",
    "href": "research-design/sample-size-power.html#understanding-statistical-concepts",
    "title": "Sample and Power Calculations",
    "section": "Understanding Statistical Concepts",
    "text": "Understanding Statistical Concepts\n\n\n\n\n\n\nProbability Density Functions (PDFs)\n\n\n\n\n\nPDFs help visualize the uncertainty around the estimated treatment effect (Œ≤ÃÇ) in a study. They show how likely different values of Œ≤ÃÇ are, assuming a true treatment effect Œ≤.\n\n\n\nTwo overlapping normal distributions representing possible treatment effects\n\n\n\n\n\n\n\n\n\n\n\nKey Concept 3\n\n\n\nA narrower distribution of estimates (Œ≤ÃÇ) increases the chances of detecting true effects. You can achieve this by: - Increasing the sample size (N) - Assigning roughly equal numbers to treatment and control (p ‚âà 0.5) - Reducing outcome variance (œÉ¬≤)\n\n\n\n\n\n\n\n\nTreatment Effects Distribution\n\n\n\n\n\nIf researchers repeated the same RCT many times, they wouldn‚Äôt get the exact same estimate every time. Instead, the estimated treatment effects (Œ≤ÃÇ) would follow a normal distribution:\n\\[\n\\hat{\\beta} \\sim \\text{Normal}(\\beta, \\sigma^2 / [Np(1-p)])\n\\]\n\nŒ≤: True treatment effect\nœÉ¬≤: Variance of the outcome\nN: Sample size\np: Proportion assigned to treatment (e.g., 0.5)\n\n\n\n\n\n\n\n\n\n\nKey Concept 4\n\n\n\nType I Error (Œ±): Risk of a false positive - Usually set at 5% (Œ± = 0.05) - This means there‚Äôs a 5% chance of incorrectly detecting an effect when there is none - A lower Œ± reduces false positives‚Äîbut increases the risk of missing real effects - Its complement (1‚àíŒ±) is known as the confidence level ‚Äì how likely studies are to avoid a false positive - While 5% is standard, it can range from 1%-10% in research literature\n\n\n\n\n\n\n\n\nHypothesis Testing Framework\n\n\n\n\n\nTo determine whether an effect is statistically significant, researchers use a hypothesis test. This is important because it helps control the risk of making a Type I error ‚Äì incorrectly concluding that an effect exists when it actually does not, a ‚Äúfalse positive.‚Äù By setting a significance level (Œ±), researchers limit the probability of making this error and ensure findings are not due to random chance.\n\n\n\n\n\n\n1. Null Hypothesis, H‚ÇÄ\n\n\n\n\n\nAssumes no effect exists, Œ≤ = 0. This is the starting point for statistical testing and represents the default position that the intervention has no impact.\n\n\n\nStatistical hypothesis testing showing null and alternative distributions\n\n\n\n\n\n\n\n\n\n\n\n2. Alternative Hypothesis, H‚ÇÅ\n\n\n\n\n\nProposes existence of an effect, Œ≤ ‚â† 0. Can be one-sided, Œ≤ &gt; 0 or Œ≤ &lt; 0, or two-sided, Œ≤ ‚â† 0.\n\n\n\nEstimate the treatment effect (ùõΩ¬†ÃÇ)\n\n\n\n\n\n\n\n\n\n\n\n3. Critical Values Computation\n\n\n\n\n\nBased on chosen significance level Œ±, typically uses Œ± = 0.05. Forms rejection regions in the distribution and uses t-distribution or z-distribution.\n\n\n\nCritical Values\n\n\n\n\n\nCritical Values - Fail To reject\n\n\n\n\n\nCritical Values - Reject\n\n\n\n\n\n\n\n\n\n\n\n4. Effect Estimation and Comparison\n\n\n\n\n\nCalculate estimated effect size Œ≤ÃÇ and compare to critical values. Consider confidence intervals and assess statistical significance.\n\n\n\n\n\n\n\n\n\n5. Statistical Decision\n\n\n\n\n\nReject H‚ÇÄ if Œ≤ÃÇ exceeds critical values. Fail to reject H‚ÇÄ if Œ≤ÃÇ falls within critical bounds. Consider practical significance and report p-values and confidence intervals.\n\n\n\nNo rejection of Ho\n\n\n\n\n\nRejection of Ho\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Concept 5\n\n\n\nType II Error (Œ∫): Risk of a false negative\n\nŒ∫: Probability of missing a real effect\nPower (1‚àíŒ∫): Probability of detecting a true effect (aim for ‚â•80%)\nLower Œ± (false positive rate) increases Œ∫ (false negative rate)\nPower and sample size are directly related: higher power needs a larger sample",
    "crumbs": [
      "Research Design",
      "Sample and Power Calculations"
    ]
  },
  {
    "objectID": "research-design/sample-size-power.html#statistical-power-in-practice",
    "href": "research-design/sample-size-power.html#statistical-power-in-practice",
    "title": "Sample and Power Calculations",
    "section": "Statistical Power in Practice",
    "text": "Statistical Power in Practice\n\n\n\n\n\n\n1. Power Depends on Study Design\n\n\n\n\n\nStatistical power is influenced by:\n\nSample size (N)\nEffect size (Œ≤): how big the true impact is\nOutcome variance (œÉ¬≤): how much outcomes naturally vary\n\nLarger samples and bigger effects increase the chance of detecting something real.\n\n\n\nLow Power: Hard to distinguish effect\n\n\n\n\n\nHigh Power: Clear effect distinction\n\n\n\n\n\n\n\n\n\n\n\n2. Power Can Be Visualized\n\n\n\n\n\nPower is often shown as the overlap between two distributions:\n\nH‚ÇÄ (Null): No effect\nH‚ÇÅ (Alternative): A true effect exists\nCritical value: The cutoff for statistical significance\n\nThe less overlap between the two distributions, the higher the power.\n\n\n\nThree scenarios showing different power levels based on effect size and sample size\n\n\n\n\n\n\n\n\n\n\n\n3. Underpowered vs.¬†Well-Powered Studies\n\n\n\n\n\n\n\n\n\n\n\n\n\nScenario\nUnderpowered Study (&lt;80%)\nWell-Powered Study (‚â•80%)\n\n\n\n\nDetection Ability\nLow: true effects may be missed\nHigh: true effects are likely to be found\n\n\nConclusions\nInconclusive or misleading\nClear and reliable\n\n\nResource Use\nWasteful: may require redoing the study\nEfficient: leads to actionable findings\n\n\n\n\n\n\n\n\n\n‚ö†Ô∏è Why This Matters\n\n\n\nAn underpowered study might miss real program effects‚Äîleading to wrong conclusions and poor policy decisions.\n\n\n\n\n\n\n\n\n\n\n\nKey Concept 6\n\n\n\nWhen power is high (‚â•80%), studies are more likely to detect true effects and avoid false negatives.",
    "crumbs": [
      "Research Design",
      "Sample and Power Calculations"
    ]
  },
  {
    "objectID": "research-design/sample-size-power.html#determinants-of-power-estimation",
    "href": "research-design/sample-size-power.html#determinants-of-power-estimation",
    "title": "Sample and Power Calculations",
    "section": "Determinants of Power Estimation",
    "text": "Determinants of Power Estimation\nSeveral key factors determine the statistical power of a study. Understanding and adjusting these determinants is crucial for effective research design:\n\n\n\n\n\n\n1. Minimum Detectable Effect (MDE)\n\n\n\n\n\nThe smallest treatment effect you aim to detect. Smaller MDEs require larger sample sizes to achieve the same power.\n\n\n\nIllustration of MDE: Smaller effects require larger samples to detect\n\n\n\n\n\n\n\n\n\n\n\n2. Sample Size (N)\n\n\n\n\n\nThe total number of study participants. Larger samples increase statistical power and precision.\n\n\n\nSample size comparison: Small vs.¬†large sample distributions\n\n\n\n\n\n\n\n\n\n\n\n3. Outcome Variance\n\n\n\n\n\nThe variability in the outcome measure. Lower variance makes it easier to detect effects.\n\n\n\nOutcome variance: Narrow vs.¬†wide outcome distributions\n\n\n\n\n\n\n\n\n\n\n\n4. Sample Allocation\n\n\n\n\n\nThe proportion assigned to treatment and control groups. Power is maximized when groups are of equal size.\n\n\n\nSample allocation: Balanced vs.¬†unbalanced groups\n\n\n\n\n\n\n\n\n\n\n\n5. Non-compliance\n\n\n\n\n\n\nWhen participants do not adhere to their assigned group.\nNon-compliance reduces the effective sample size and power.\nNon-compliance reduces power via a smaller effect size, or MDE (under no compliance, the estimated treatment effect collapses to zero).\nAs we saw before, a smaller effect (MDE) is harder to detect (so less power).\nIncreasing compliance is one of your strongest levers to increase power!\n\n\n\n\nNon-compliance: Some participants do not follow assignment\n\n\n\n\n\n\n\n\n\n\n\n6. Attrition\n\n\n\n\n\n\nLoss of participants during the study. Attrition reduces sample size and can bias results if not random.\nAttrition reduces power via a smaller study sample size.\nIf attrition is correlated with the treatment (e.g., more likely in the control group), the estimated effect is no longer unbiased!\nYou must try hard to keep your study sample! (rapport, incentives, tracking respondents, etc.)\n\n\n\n\nAttrition: Participants lost over time",
    "crumbs": [
      "Research Design",
      "Sample and Power Calculations"
    ]
  },
  {
    "objectID": "research-design/sample-size-power.html#common-pitfalls-and-solutions",
    "href": "research-design/sample-size-power.html#common-pitfalls-and-solutions",
    "title": "Sample and Power Calculations",
    "section": "Common Pitfalls and Solutions",
    "text": "Common Pitfalls and Solutions\n\n\n\n\n\n\nPitfall 1: Ignoring Design Effects\n\n\n\n\n\n\nProblem: Using simple formulas for complex designs\nSolution: Account for clustering, stratification in calculations\n\n\n\n\n\n\n\n\n\n\nPitfall 2: Optimistic Assumptions\n\n\n\n\n\n\nProblem: Assuming perfect compliance, no attrition\nSolution: Build in realistic buffers (typically 20-30%)\n\n\n\n\n\n\n\n\n\n\nPitfall 3: Post-Hoc Power\n\n\n\n\n\n\nProblem: Calculating power after finding null results\nSolution: Focus on confidence intervals, not post-hoc power\n\n\n\n\n\n\n\n\n\n\nPitfall 4: Fixating on 80%\n\n\n\n\n\n\nProblem: Treating 80% power as sacred threshold\nSolution: Consider context‚Äîsometimes 70% or 90% is appropriate",
    "crumbs": [
      "Research Design",
      "Sample and Power Calculations"
    ]
  },
  {
    "objectID": "research-design/sample-size-power.html#footnotes",
    "href": "research-design/sample-size-power.html#footnotes",
    "title": "Sample and Power Calculations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBanerjee, A., Cole, S., Duflo, E., & Linden, L. (2007). Remedying Education: Evidence from Two Randomized Experiments in India. The Quarterly Journal of Economics, 122(3), 1235-1264.‚Ü©Ô∏é\nBanerjee, A., Cole, S., Duflo, E., & Linden, L. (2017). Balsakhi [Dataset]. Harvard Dataverse.‚Ü©Ô∏é",
    "crumbs": [
      "Research Design",
      "Sample and Power Calculations"
    ]
  },
  {
    "objectID": "research-design/measurement.html#what-is-stata",
    "href": "research-design/measurement.html#what-is-stata",
    "title": "Research Design",
    "section": "What is Stata?",
    "text": "What is Stata?\nStata is a statistical software package that is commonly used in the social sciences and economics. It is widely used at IPA for data analysis and management. It offers a comprehensive library of methods for data cleaning, descriptive statistics, and econometric analysis. Stata is very well suited for research data workflows and research design tasks, including power calculations, sample design adjustments, panel data analysis, time series analysis, etc. See Stata Features for a full list of what Stata makes available."
  },
  {
    "objectID": "research-design/how-to-randomization.html",
    "href": "research-design/how-to-randomization.html",
    "title": "Guide to Randomization",
    "section": "",
    "text": "Step-by-step instructions for implementing stratified randomization in Stata, including reproducible seed setting, balance checking, and troubleshooting common issues."
  },
  {
    "objectID": "research-design/how-to-randomization.html#problem-assign-participants-to-treatment-groups",
    "href": "research-design/how-to-randomization.html#problem-assign-participants-to-treatment-groups",
    "title": "Guide to Randomization",
    "section": "Problem: Assign Participants to Treatment Groups",
    "text": "Problem: Assign Participants to Treatment Groups\nYou need to randomly assign participants to treatment and control groups while maintaining balance across key characteristics like grade level, school, or other strata.\nPrerequisites:\n\nStata installed with randtreat package\nDataset with participant identifiers and stratification variables\nBasic familiarity with Stata commands\n\nWhat you‚Äôll accomplish:\n\nSet up reproducible randomization with proper seed management\nImplement stratified randomization to achieve balance\nVerify treatment assignment worked correctly"
  },
  {
    "objectID": "research-design/how-to-randomization.html#step-1-set-up-your-environment",
    "href": "research-design/how-to-randomization.html#step-1-set-up-your-environment",
    "title": "Guide to Randomization",
    "section": "Step 1: Set Up Your Environment",
    "text": "Step 1: Set Up Your Environment\nSet Stata version and seed for reproducibility:\n\nUse a different seed for each project\nChoose seeds through random methods‚Äîsuch as from random.org or dice rolls\nSet only one seed per do-file\nAlways set the Stata version at the beginning since algorithms change between versions"
  },
  {
    "objectID": "research-design/how-to-randomization.html#step-2-prepare-your-data",
    "href": "research-design/how-to-randomization.html#step-2-prepare-your-data",
    "title": "Guide to Randomization",
    "section": "Step 2: Prepare Your Data",
    "text": "Step 2: Prepare Your Data\nExample scenario: School supply program evaluation\n\n500 students across 10 schools\nBudget to treat 250 students\nNeed to ensure balance across grades\n\nCreate your dataset with participant identifiers and strata:\n\n%%stata\nclear\nset obs 500\ngen student_id = _n\ngen school_id = ceil(_n/50)         // 10 schools, 50 students each\ngen grade = mod(_n-1,5) + 1         // Grades one to five, evenly distributed\n\n\n. clear\n\n. set obs 500\nNumber of observations (_N) was 0, now 500.\n\n. gen student_id = _n\n\n. gen school_id = ceil(_n/50)         // 10 schools, 50 students each\n\n. gen grade = mod(_n-1,5) + 1         // Grades one to five, evenly distributed\n\n."
  },
  {
    "objectID": "research-design/how-to-randomization.html#step-3-set-random-seed",
    "href": "research-design/how-to-randomization.html#step-3-set-random-seed",
    "title": "Guide to Randomization",
    "section": "Step 3: Set Random Seed",
    "text": "Step 3: Set Random Seed\nSet a random seed for replicability:\n\n%%stata\nset seed 12345"
  },
  {
    "objectID": "research-design/how-to-randomization.html#step-4-verify-your-strata",
    "href": "research-design/how-to-randomization.html#step-4-verify-your-strata",
    "title": "Guide to Randomization",
    "section": "Step 4: Verify Your Strata",
    "text": "Step 4: Verify Your Strata\nCheck that your stratification variable covers all participants:\n\n%%stata\ntab grade\n\n\n      grade |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |        100       20.00       20.00\n          2 |        100       20.00       40.00\n          3 |        100       20.00       60.00\n          4 |        100       20.00       80.00\n          5 |        100       20.00      100.00\n------------+-----------------------------------\n      Total |        500      100.00"
  },
  {
    "objectID": "research-design/how-to-randomization.html#step-5-implement-stratified-randomization",
    "href": "research-design/how-to-randomization.html#step-5-implement-stratified-randomization",
    "title": "Guide to Randomization",
    "section": "Step 5: Implement Stratified Randomization",
    "text": "Step 5: Implement Stratified Randomization\nAssign half of students to treatment within each grade‚Äîtotal treated equals 250. Generate a treatment variable where 1 equals treated and 0 equals control:\n\n%%stata\n\n* Install randtreat if not already installed\ncap ssc install randtreat\n\n* Use randtreat for stratified randomization by grade\nrandtreat, strata(grade) unequal(1/2 1/2) generate(treatment)\n\n\n. \n. * Install randtreat if not already installed\n. cap ssc install randtreat\n\n. \n. * Use randtreat for stratified randomization by grade\n. randtreat, strata(grade) unequal(1/2 1/2) generate(treatment)\nassignment produces 0 misfits\n\n."
  },
  {
    "objectID": "research-design/how-to-randomization.html#step-6-verify-balance",
    "href": "research-design/how-to-randomization.html#step-6-verify-balance",
    "title": "Guide to Randomization",
    "section": "Step 6: Verify Balance",
    "text": "Step 6: Verify Balance\nTabulate treatment by grade and school to verify balance:\n\n%%stata\ntab treatment grade\ntab treatment school_id\n\n\n. tab treatment grade\n\n           |                         grade\n treatment |         1          2          3          4          5 |     Total\n-----------+-------------------------------------------------------+----------\n         0 |        50         50         50         50         50 |       250 \n         1 |        50         50         50         50         50 |       250 \n-----------+-------------------------------------------------------+----------\n     Total |       100        100        100        100        100 |       500 \n\n. tab treatment school_id\n\n           |                       school_id\n treatment |         1          2          3          4          5 |     Total\n-----------+-------------------------------------------------------+----------\n         0 |        26         27         25         24         18 |       250 \n         1 |        24         23         25         26         32 |       250 \n-----------+-------------------------------------------------------+----------\n     Total |        50         50         50         50         50 |       500 \n\n\n           |                       school_id\n treatment |         6          7          8          9         10 |     Total\n-----------+-------------------------------------------------------+----------\n         0 |        25         28         25         26         26 |       250 \n         1 |        25         22         25         24         24 |       250 \n-----------+-------------------------------------------------------+----------\n     Total |        50         50         50         50         50 |       500 \n\n."
  },
  {
    "objectID": "research-design/how-to-randomization.html#troubleshooting-common-issues",
    "href": "research-design/how-to-randomization.html#troubleshooting-common-issues",
    "title": "Guide to Randomization",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\nNot setting a random seed: Always use set seed in Stata to ensure your results are replicable.\nFailing to check balance: After randomization, verify that treatment and control groups have balance on key variables.\nConfusing unit identifiers: Double-check IDs‚Äîsuch as village, school, or participant names‚Äîto avoid misassignment.\nContamination: Monitor to prevent control group members from receiving the treatment.\nPoor documentation: Keep detailed records of your randomization procedure for transparency and reproducibility.\n\n\n\n\n\n\nSorting Considerations\n\n\n\n\n\nEnsuring Reproducible Sorting\nThe sort command can produce non-reproducible results if the sorting variables don‚Äôt uniquely identify observations. Always include a unique ID as the last sorting variable:\n* Check if ID is unique\nisid unique_id\n\n* Sort by unique ID before generating random numbers\nsort unique_id\ngen rand = runiform()\n\n* When sorting by strata and random number, include unique ID last\nsort region rand unique_id\nThis prevents Stata from breaking ties inconsistently, ensuring your randomization is reproducible.\n\n\n\nFor more detailed guidance on randomization and power calculations in Stata, see the Stata power and sample size reference manual Release 18 1.\nAdditional Resources:\n\nIPA Randomization Guide with Example Do-files\nWilliam Gould on Random Number Seeds\nStataCorp Blog Posts on Pseudo-Random Number Generators"
  },
  {
    "objectID": "research-design/how-to-randomization.html#footnotes",
    "href": "research-design/how-to-randomization.html#footnotes",
    "title": "Guide to Randomization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStataCorp. 2023. Stata power and sample size reference manual: Release 18. https://www.stata.com/manuals/power.pdf‚Ü©Ô∏é"
  },
  {
    "objectID": "index_backup.html",
    "href": "index_backup.html",
    "title": "Research and Data Science Hub",
    "section": "",
    "text": "Welcome to IPA‚Äôs central research hub - your gateway to open-source tools, methodological resources, and collaborative projects driving evidence-based policymaking worldwide."
  },
  {
    "objectID": "index_backup.html#research-and-data-science-hub",
    "href": "index_backup.html#research-and-data-science-hub",
    "title": "Research and Data Science Hub",
    "section": "",
    "text": "Welcome to IPA‚Äôs central research hub - your gateway to open-source tools, methodological resources, and collaborative projects driving evidence-based policymaking worldwide."
  },
  {
    "objectID": "index_backup.html#about-the-hub",
    "href": "index_backup.html#about-the-hub",
    "title": "Research and Data Science Hub",
    "section": "About the Hub",
    "text": "About the Hub\nThe IPA Research Data Science Hub serves as the nexus for:\n\nData Cleaning: Best practices and tools for data validation and cleaning\nData Collection: Guidelines for field data collection and digital tools\nData Security: Protocols and tools for secure data handling\nIRB and Ethics: Research ethics guidelines and IRB processes\nResearch Design: RCT design, sampling, and implementation methods\nSoftware: Open-source tools and code repositories for research\n\n\n\n\n\n\n\nTip\n\n\n\nGet Involved\nüìß researchsupport@poverty-action.org üîó Contribution Guidelines"
  },
  {
    "objectID": "data-quality/survey-plan.html",
    "href": "data-quality/survey-plan.html",
    "title": "Survey Planning",
    "section": "",
    "text": "A survey plan is an operational plan that covers timelines, staffing needs, logistics, and procurement for your data collection efforts. This guide outlines the key components needed to develop a comprehensive survey plan, including questionnaire development, personnel requirements, timeline planning, logistics coordination, and financial management.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Survey Planning"
    ]
  },
  {
    "objectID": "data-quality/survey-plan.html#developing-a-survey-plan",
    "href": "data-quality/survey-plan.html#developing-a-survey-plan",
    "title": "Survey Planning",
    "section": "Developing a Survey Plan",
    "text": "Developing a Survey Plan\nWrite your survey plan as you design your questionnaire and decide whether it should be electronic or paper-based. You will need to share your survey plan with others working with you like the Country Director, PIs, country level and HQ survey support staff.\nFor paper surveys, give a heads up to the in-house data entry staff as early as possible. If you have a data entry unit in your country or use a data entry company, make sure your timeline fits their schedule so you can get the data quickly after fieldwork finishes. The same applies for dedicated programming staff if your survey is digital.\nYour survey plan can work as a running document where you fill details as you determine them. It should include the following:\n\nQuestionnaire Development: piloting questionnaires, translation, back translation, who will be involved at these stages, where will you pilot the survey ‚Äì consider not going to actual survey respondents but finding people similar‚Äîand how will you analyze the data.\nPersonnel: who do you need to hire for surveying and quality control and what are the payment structures for staff. When will you hire people ‚Äì think about the structure and composition you want for your team. You might want to hire some senior people to help you with piloting the questionnaire and hiring other surveyors who will serve as team leaders/field managers when you start actual surveying. You might decide to go with a survey company. If you do, consider what they will take care of and what you will. Read the guide to deciding between a survey company and hiring your own team, and the quality control guide.\nTimeline for data collection: when will piloting the questionnaire happen, when will training happen, when will surveying start and finish. Read the section on timeline below.\nLogistics: how will you move teams across survey regions, transportation, accommodation, food etc. If you are surveying electronically, how you will charge, store and secure your devices. Read the section on logistics below.\nSurvey Tracking: how will you stay on top of the progress made in surveying, how many surveys should be completed per day, monitor data collection and keep track of which surveys are being checked.\nRespondent tracking: what information do you want to collect, who tracks hard to find respondents, how much time do you devote, how do you decide when to drop a respondent, what is your replacement plan. Read the guideline on tracking respondents.\nStorage: where will surveys or electronic devices be stored‚ÄîDo you need a field office?\nTraining Surveyors: where will training happen, how long will it go on for, what should be on the training agenda, how will you incorporate as much field training as possible, how many surveyors do you want to recruit. Read the materials on training surveyors and creating a survey manual.\nWhat equipment and other items do you need to buy: gifts for respondents, any equipment for taking anthropometric measurements, other survey material such as stationery for surveyors. Read the section on survey materials below.\nYour time: You are responsible for the whole survey and there are several things happening simultaneously that require your attention. It might be a good idea to designate time for things‚Äîfor example, Friday look at back check data, Monday go out to the field‚Äîso that you do not forget anything during data collection.\n\n\nTimeline\nWhile estimating when the survey will be complete is an inexact art, it is essential to do it well because data is often time-sensitive and must be collected while a program is running. Data collection must occur within a specific period to be comparable to others in the same survey or your budget restricts you to a long or short survey period. That means you need to plan realistically what your team can accomplish, learn immediately if you are falling behind your timeline and adapt quickly with a revised plan.\nSpecific plans vary according to the programs being evaluated, the survey, team structures, and geography. The core of your plan is deciding how many surveys you expect each person to complete each day. The following are some general principles that should guide this calculation:\n\nTraveling to Respondents: You need to develop a good sense of how long it‚Äôll take to find a specific respondent. This may vary across the regions, districts or neighborhoods in your sample. Before fielding the survey, figure out which areas will be the most difficult to find respondents and which will be relatively easier. Consult maps with roads and anyone with local knowledge, from local partners to shop owners. Also, think about how long it‚Äôll take surveyors to get to the survey area from their accommodation and include this in your calculation of the working day. It‚Äôs tempting to not include travel time in your work day, but you do this at your own peril. Travel time is one of the most common factors that delay a survey.\nFinding Hard to Find Respondents: Finding all respondents in a cluster often requires several attempts and needs to be factored into your expectations of how many surveys your team will perform each day. Identify what is the best time to find respondents in your survey sample- e.g., if it‚Äôs a period of heavy agricultural activity most people may be at their farms for some duration of the day and try to find out what time this might be. Finding respondents in the follow-up/endline survey will take more time than the baseline as some respondents may have moved and your team will need to sleuth out additional information to find them. Read the guide on Respondent Tracking.\nSurvey Time: From the piloting of the questionnaire, you should have a sense of how long it takes to complete a single questionnaire. Based on this estimate as well as travel and tracking time, set a target number of surveys to be completed per day by each surveyor. Keep in mind that during the initial days of the survey, most surveyors will not reach their target. However, it is advisable that this only be factored into your timeline and average daily return calculations, but not into the daily target you set for your team. Surveyors should be given target estimates based on what is expected to be completed per day after the initial slowness of data collection is over.\nHolidays: Be sure to factor in national and local holidays. Consult with partner staff about which holidays they follow and could affect respondent availability. Some locations have so many holidays it may seem impossible to work!\nTreatment/Control Balance: Finally, be sure that treatment and control respondents/areas are surveyed in a balanced manner. Don‚Äôt survey all the controls and then all the treatment, but rather try to survey them simultaneously and as evenly as possible over the survey period.\n\n\n\nLogistics\nAs a field RA, your job is to make sure everything happens when it is supposed to, which means you need to be a mean multitasker and be able to inspire the best out of your survey team. Delegating the logistics in large surveys to a survey company or in-country management teams reduces some of the management burden, but you will still need to verify proper planning and timely execution. If surveying is considered a low paying, low skilled job in your area it might not attract people who take the initiative to recognize whether they have everything they need to do their job. Think about building in accountability systems into your field plan to incentivize people to meet targets and tell you when they hit a roadblock in the field.\nIn general, when planning the movement of teams, start with easy places to survey. Surveyors generally take more time in the early days to reach their target number of respondents and working in villages that are close by or where you‚Äôll be able to find respondents easily reduces stress on you and your team. If the survey launches in several regions of the country simultaneously, consider a phased start of surveying so you can spend time with each team as they launch to be able to put out fires and improve practices before the whole team starts surveying.\nHere are some things to consider when planning logistics:\n\nTransportation Methods: How will the teams travel to villages and sometimes even within villages? Will you give them a travel allowance and they organize their own transportation or will you need to organize a communal method? If you‚Äôre organizing, how many teams can fit in a minivan? Does every team need their own vehicle?\nDirections to survey locations: Maps may not always give you the most accurate directions to a village. How will you find villages? How will you make sure that it is the right village? Who is responsible for making sure the team is in the right location?\nAccommodation: Where will teams stay? Will they find their own accommodation or will you need to organize? Are there convenient accommodation options close to the villages you survey? If not, should you think about teams staying in villages? In this case, arrangements (mattresses, cooking equipment) will need to be organized. It is advisable for teams to stay together when they need to stay outside their home base. This allows you to make sure everyone arrives on time and coordinate evening meetings when the team comes back. If they make their own accommodation arrangements, consider imposing penalties for late arrivals because it slows down the whole team.\nFood: Where will people have their lunch? How long are they allowed to break on the field? Are there options in the village or will they need to travel far to find food? Encourage teams to pack their own food if this is the case.\nCommunicating with Local Authorities: You may need to get permission to survey from district officials. Print letters that can be given to officials- decide who will go meet them.\nFinding the right respondent: How will the surveyors a) find the respondent and b) make sure it‚Äôs the right respondent? One option is to send mobilizers (well-informed and trusted community members or leaders) before the survey team to find respondents and slot interview appointments so that the surveyors don‚Äôt waste time finding respondents.\nEquipment: how are you printing all survey materials? How will you distribute them to staff? For phone surveys, how are you distributing SIM cards? How are you managing the sign-in and sign-out of devices/tablets/phones? Is there any other equipment that enumerators need to complete surveys?\nProductivity: How many surveys should each enumerator strive to complete per day? How will they know which respondents to attempt on each working day?\nInternet access: For digital data collection in particular, you should consider what speed of internet is available and how reliable it is. You also need to think about whether the internet is sufficient for uploading the types of files that you have. For example, audio files require much more bandwidth than survey data. For remote areas, consider setting up a local area network at your field office, to enable analysis of collected data without connecting to the internet. SurveyCTO offline sync simplifies this process. Be sure that you have a plan for sending airtime to enumerators to conduct and send in surveys once they are completed. If you are offering airtime as a respondent gift, you should set up the system for sending to specific phone numbers.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Survey Planning"
    ]
  },
  {
    "objectID": "data-quality/survey-plan.html#financial-plan",
    "href": "data-quality/survey-plan.html#financial-plan",
    "title": "Survey Planning",
    "section": "Financial Plan",
    "text": "Financial Plan\nManaging your finances is one of the biggest, hidden headaches of keeping your field team moving. How you‚Äôre going to get money to the team, when they will be paid and how much should definitely be part of your field plan. Here are a few things to consider when creating your financial plan:\n\nPayment Lag: Keep in mind that there is often a lag between when surveyors start work and they get paid. If you are paying surveyors per diems and transport allowance, this can be a problem if they are required to buy their own transportation fuel or food upfront. Consider providing these until they are paid, giving a start of project bonus or loan (which can be automatically deducted from their first payment). Failing to plan for this can slow the launch of a survey.\nPaying in Cash vs.¬†Direct Deposit: Definitely discuss this decision with your in-country management team. In countries where you can deposit funds into bank accounts, remember that it can take a significant amount of time to withdraw money from a bank. If you‚Äôre paying per diems this way, it can lead to field delays and needs to be accounted for in your timeline. While cash may be easier to make happen logistically and result in few delays, it is also a major security risk. Make sure you have procedures in place to get the money safely from the bank and then hold your team accountable so you can de-incentivize them from embezzling funds and catch any money problems early.\nSending Airtime/Internet to Enumerators and Respondents: Create a system for sending internet to respondents if you are offering airtime as a respondent gift. Ideally, you should have someone in your office or give enumerators the ability to send airtime directly to a phone number. Enumerators must have enough internet to send in surveys or airtime to conduct surveys if they are conducting phone surveys.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Survey Planning"
    ]
  },
  {
    "objectID": "data-quality/survey-plan.html#survey-materials",
    "href": "data-quality/survey-plan.html#survey-materials",
    "title": "Survey Planning",
    "section": "Survey Materials",
    "text": "Survey Materials\nMake field departure and return checklists of things people need to do before they go out into the field and close for the day. Consider pasting these procedures to your office on the door so people can check if they have everything they need before they head out.\nThe field leadership team, i.e.¬†Team Leader, Field Managers, is responsible for making sure this is followed. You may want to consider creating tracking forms, sign in/out sheets to help hold them and the team accountable. This is especially important in electronic surveying as you want to be sure that equipment like phones, PDAs, and netbooks are well-taken care of and you can hold the right person accountable if one is lost.\n\nEquipment\nMany projects need to purchase equipment for the field office or to collect anthropomorphic measures such as height, weight, or health indicators. Many institutions require getting price quotes from three or more vendors, which is a good practice, and will help you develop your negotiating skills. You should take an inventory of your office equipment, including the condition, at the beginning and end of the survey period to make sure nothing is lost or damaged.\nFor all equipment going to the field, it is highly recommended to put serial numbers on each piece and have surveyors check them out and check them in regularly for inspection. These machines are often subjected to rough conditions, and depreciate quickly. One way to improve your data is to have surveyors record the serial number of the machine used for each survey. That will help you identify systematic errors in machines.\n\n\nGPS Units\nSome surveys use GPS (Global Positioning System) units to record GPS readings to aid in locating and verifying the right household in subsequent surveys. Often team leaders or people doing the census exercise are responsible for taking GPS readings to minimize the number of units required. Make sure you send them with additional batteries and budget for a few more GPS units than the people taking GPS readings to allow for units breaking on the field. Be sure to set the same coordinate format on all units before sending them to the field so you don‚Äôt have to convert them later.\nOne point to note about GPS readings is that they are helpful in finding respondents in subsequent surveys but may not be sufficient. Unless you have GPS units that are accurate to within 5 meters there is bound to be some confusion in subsequent surveys. Traditional tracking methods ‚Äì such as hand-drawn maps, collecting phone numbers and contacts ‚Äì should be used to complement the GPS readings and provide a secondary method to find respondents. These methods should not be abandoned simply because GPS readings are taken.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Survey Planning"
    ]
  },
  {
    "objectID": "data-quality/survey-plan.html#tablets-and-phones",
    "href": "data-quality/survey-plan.html#tablets-and-phones",
    "title": "Survey Planning",
    "section": "Tablets and phones",
    "text": "Tablets and phones\nIPA recommends buying devices from trusted brands such as Samsung or Google. It is important to buy sturdy cases to avoid breaking your devices. Many projects will find that extra battery packs are a necessity in the field. You should do field piloting of your devices to work out the kinks early on so you can budget for the necessary equipment when making your survey plan. You should also consider buying a gasoline or diesel generator if power is unreliable.\nIf you are conducting a phone survey, be sure that enumerators also have headphones, battery packs, and charger cables. If you are asking enumerators to use their own phones, SIM (Subscriber Identity Module) cards should be provided so enumerators are not using their own phone numbers.\n\nEquipment for surveyors\nYou will need to make sure that your team has all the basic equipment it needs for the field. This includes:\n\nBags and Folders: for surveyors to organize their questionnaire. Consider getting something durable and rain proof to protect equipment and/or questionnaires.\nUmbrellas/Rain Coats: If you‚Äôre asking your team to work through rainy season, this can help them persevere through a drizzle.\nStationery Items: include pencils, sharpeners and erasers and, potentially, a stapler per team.\nID cards for surveyors: This helps surveyors establish credibility when surveying. T-shirts and hats with the IPA logo can also help.\n\n\n\nRespondent gifts/ incentives\n\nChoosing gifts\nOften surveys give the respondent gifts or incentives to compensate them for their time. Some surveys have given money but it is recommended that it be used rarely and preferably only when you are taking someone‚Äôs time when they would otherwise be working. Sometimes the money for respondent gifts is used to play games eliciting risk aversion, however, this runs the risk of some respondents feeling under-compensated and moreover surveyors might like to be nice to respondents and let them ‚Äúwin‚Äù coin tosses.\nGenerally, you do not want to choose a gift that is extremely valuable to the survey respondent. This can run into issues with ethics since it is important to avoid having undue influence when a respondent feels they have no choice but to be interviewed because of the incentive involved. Because of this, survey gifts must be approved by your IRB.\nSome of the items that have been given by past IPA projects include sugar, cooking oil, washing soap bars, pencils, and sharpeners for school kids. Some projects have also given IPA T-shirts and IPA key chains but be careful with these. In a project in Ghana, farmers were given IPA T-shirts and some farmers wanted to form an association of IPA respondents in a village!\nItems for the household are a safe bet because they are useful no matter which respondent receives the gift. Some of these items could be bulky, making it a struggle to carry them on the field. You need to factor in how the team will carry these items on the field, since spillage or leaking may damage surveys or expensive equipment if they are carried in the same bag. One common gift phone cards since are easy to cart around, and they facilitate phone back checks or tracking later on. With all gifts, and phone cards in particular, the challenge is to set up a system to verify that respondents received their gifts and they were not taken by the surveyors. Consider including a question in your back check survey about receiving the gift or a shorter phone follow up.\n\n\nTreatment v. Control Gifts\nIf you are working on a project where a loan or capital is given as one of the treatments, you may want to consider a) how you present the cash to the respondents and b) a nice gift for the control group as well to maintain good relations. It can be hard to maintain a good relationship with control respondents when they hear that others participating in the study received 100GHC and they did not. No matter how many times you explain it is random, like winning the lottery, you will still have some disgruntled respondents. Projects have given control groups nicer gifts from flashlights to nice sets of stationery to compensate for the disappointment of not receiving a big prize. Another thing to consider is how the gift is presented. You may want to allow respondents to rip open an envelope or scratch off a scratch card to heighten the feeling of winning a prize, rather than simply being given a gift for their time.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Survey Planning"
    ]
  },
  {
    "objectID": "data-quality/index.html",
    "href": "data-quality/index.html",
    "title": "Data Quality",
    "section": "",
    "text": "Every research project at IPA is required to follow research protocols, or ‚ÄòMinimum Must Dos,‚Äô to ensure that IPA produces high-quality research. These protocols are organized into data management, data quality, data security and ethics, and knowledge management categories.\n    \n\n\n\n\n\n\nIPA Research Protocols for High-Quality Research (¬Æ David Torres)\n\n\nIPA‚Äôs Data Quality Protocols are organized into four main categories. Click on each protocol below to learn more.\n\n\n\n\n\n\nData Management System - DMS\n\n\n\n\nIPA Data Management System (IPA-DMS)\n\nThe IPA Data Management System (IPA-DMS) is a fundamental tool that standardizes data quality practices across IPA projects. It provides a structured framework for data cleaning, documentation, and quality control throughout the research lifecycle.\n\n\n\n\n\n\n\n\nData Quality\n\n\n\n\nSurvey Plan\nData Quality Action Plan\nBench Test\nPilot Survey\nAccompany Surveyors\nHigh Frequency Checks\nBackchecks\nDouble Entry\n\n\n\n\n\n\n\n\n\nData Security and Research Ethics\n\n\n\n\nMaintain active IRB approval\nCreate data security protocols\nFollow data security protocols\nData anonymization\nComplete IRB closeout process\n\n\n\n\n\n\n\n\n\nKnowledge Management and Transparency\n\n\n\n\nMaintain data backups\nStore all files on Box\nRegister with AEA\n\n\n\n\n\n\n Back to topReuseCC BY 4.0",
    "crumbs": [
      "Data Quality",
      "About Data Quality"
    ]
  },
  {
    "objectID": "data-quality/high-frequency-checks.html",
    "href": "data-quality/high-frequency-checks.html",
    "title": "High Frequency Checks",
    "section": "",
    "text": "Learn about IPA‚Äôs High Frequency Checks‚Äîsystematic data quality checks performed during data collection to identify and address issues early. This how-to guide covers the types of checks, implementation using the ipacheck Stata package, and best practices for ensuring reliable survey data.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "High-Frequency Checks"
    ]
  },
  {
    "objectID": "data-quality/high-frequency-checks.html#what-are-high-frequency-checks",
    "href": "data-quality/high-frequency-checks.html#what-are-high-frequency-checks",
    "title": "High Frequency Checks",
    "section": "What are High Frequency Checks?",
    "text": "What are High Frequency Checks?\nHigh Frequency Checks are systematic checks performed on survey data at regular intervals, such as daily or weekly, during data collection. These checks help identify and correct issues or mistakes in the survey process, ensuring the data collected is of high quality. By catching errors early, teams can make necessary adjustments to the survey or data collection methods, leading to more reliable and accurate data.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "High-Frequency Checks"
    ]
  },
  {
    "objectID": "data-quality/high-frequency-checks.html#why-it-is-important-to-implement-high-frequency-checks",
    "href": "data-quality/high-frequency-checks.html#why-it-is-important-to-implement-high-frequency-checks",
    "title": "High Frequency Checks",
    "section": "Why it is important to implement High Frequency Checks",
    "text": "Why it is important to implement High Frequency Checks\nHigh-frequency checks evaluate different aspects of the data collection process and run at regular intervals as teams collect new data. At IPA/J-PAL, HFCs are typically implemented in Stata after the data flow is complete. These checks provide insights into:\n\nThe quality of the data\nEnumerator performance\nErrors in the electronic survey program\nSystematic flaws in the data flow\n\nGiven their importance, high-frequency checks are one of the major benefits of Computer-Assisted Interviewing (CAI). Unlike CAI logic checks, which are pre-programmed into the survey tool, HFCs run post-data collection to identify trends across surveys.\n\nTypes of High Frequency Checks\n\nDaily Logic Checks\n\nVerify the survey form version\nDetect duplicate observations and unique variable duplicates‚Äîfor example, GPS, phone number\nEnsure critical variables are not missing\nIdentify variables with all missing values\nReview ‚ÄúOther specify‚Äù values\nDetect outliers in numeric variables\nReview field comments\nTrack survey progress\n\n\n\nEnumerator Performance Checks\n\nPercentage of ‚ÄúDon‚Äôt know‚Äù and ‚ÄúRefuse to answer‚Äù\n‚ÄúYes‚Äù percentage for filter questions\nEnumerator productivity\nAverage interview duration\nActive hours\nStatistics for numeric variables\n\n\n\nSurvey Dashboard Checks\n\nSurvey consent rate\nPercentage of missing survey values\nPercentage of ‚ÄúDon‚Äôt know‚Äù and ‚ÄúRefuse to Answer‚Äù\nNumber and percentage of ‚ÄúOther specify‚Äù values\nVariables with all missing values\nSurvey productivity",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "High-Frequency Checks"
    ]
  },
  {
    "objectID": "data-quality/high-frequency-checks.html#introduction-to-ipas-hfcs-package",
    "href": "data-quality/high-frequency-checks.html#introduction-to-ipas-hfcs-package",
    "title": "High Frequency Checks",
    "section": "Introduction to IPA‚Äôs HFCs Package",
    "text": "Introduction to IPA‚Äôs HFCs Package\nThe Stata program ipacheck is a set of user-written commands developed and maintained by IPA. It consists of three types of files: an input file, a do-file, and output files. Teams can modify these files to fit their project‚Äôs needs. Each check produces either a standalone workbook or a dedicated sheet in the HFC output workbook, displaying respondent IDs, enumerator details, and relevant flagged variables.\n\nHow are High Frequency Checks different from the Data Management System?\nThe Data Management System provides a structured approach to managing research data, assisting with:\n\nCreating project folder structures\nManaging data flow\nRunning high-frequency checks\nDe-duplicating and replacing data\nTracking survey progress\nRunning back checks\nGenerating logs for project closure\n\nHigh-frequency checks are just one component of the broader capabilities of the DMS, running alongside other project management tools.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "High-Frequency Checks"
    ]
  },
  {
    "objectID": "data-quality/high-frequency-checks.html#installation",
    "href": "data-quality/high-frequency-checks.html#installation",
    "title": "High Frequency Checks",
    "section": "Installation",
    "text": "Installation\nipacheck is IPA‚Äôs Stata package for running high-frequency checks. The package includes various programs categorized as main or ancillary.\n\nMain Programs\n\nipacheckcorrections - Make corrections to data.\nipacheckspecifyrecode - Recode ‚ÄúOther specify‚Äù values.\nipacheckversions - Export survey form version statistics and flag outdated submissions.\nipacheckids - Identify duplicate survey IDs.\nipacheckdups - Detect duplicates in non-ID variables.\nipacheckmissing - Generate missingness and distinctness statistics.\nipacheckoutliers - Identify outliers in numeric variables.\nipacheckspecify - Extract ‚ÄúOther specify‚Äù values.\nipacheckcomments - Export field comments from SurveyCTO.\nipachecktextaudit - Analyze survey duration using SurveyCTO text audit files.\nipachecktimeuse - Export engagement statistics from SurveyCTO audit files.\nipachecksurveydb - Generate general dataset statistics.\nipacheckenumdb - Evaluate enumerator performance.\nipatracksurvey - Generate a dashboard for tracking survey progress.\nipabcstats - Compare survey data with back check data.\n\n\n\nAncillary Programs\n\nipacodebook - Export a codebook to Excel with labeled variable notes.\n\nipacheck comes with a structured project folder setup, including a master do-file, global variables file, preparation do-file, and Excel-based input sheets. The package generates outputs as well-formatted Excel spreadsheets for field team distribution. Since version four, ipacheck incorporates programs from the ipahelper package.\n\n\nInstallation Steps\n* Install ipacheck from GitHub\nnet install ipacheck, all replace from(\"https://raw.githubusercontent.com/PovertyAction/high-frequency-checks/master\")\n\n* Update ipacheck anytime\nipacheck update\n\n* Create a new project with folder structure and input files\nipacheck new, surveys(\"SURVEY_NAME_1\") folder(\"path/to/project\")\n\n* Create a project with multiple surveys using subfolders\nipacheck new, surveys(\"SURVEY_NAME_1\" \"SURVEY_NAME_2\") folder(\"path/to/project\") subfolders\n\n* Obtain fresh copies of the master do-file and Excel inputs\nipacheck new, filesonly\n\n* Access IPA's training exercise (includes sample data and instructions)\nipacheck new, exercise\n\n* Verify the installed version\nipacheck version\n\n\n\n\n\n\nHFCs in practice\n\n\n\nCheck out the IPA High Frequency Check Exercise! These exercises help familiarize users with the setup and use of IPA‚Äôs Data Management System, which includes High-Frequency Checks, Survey Tracking, and Back Check comparison. Learn more",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "High-Frequency Checks"
    ]
  },
  {
    "objectID": "data-quality/high-frequency-checks.html#conclusion",
    "href": "data-quality/high-frequency-checks.html#conclusion",
    "title": "High Frequency Checks",
    "section": "Conclusion",
    "text": "Conclusion\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD In conclusion, High Frequency Checks (HFCs) are an essential component of IPA‚Äôs data quality assurance framework. By implementing systematic and regular checks on survey data, HFCs help ensure the accuracy, reliability, and overall quality of the data collected. The ipacheck Stata package provides a comprehensive set of tools to facilitate these checks, making it easier for research teams to identify and address issues promptly. By integrating HFCs into your data collection process, you can significantly enhance the integrity of your research findings and contribute to the overall success of your projects.\n======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; c6c7bfe (edits to data quality pages) ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; ab5cde4 (edits to data quality pages) High Frequency Checks are essential for maintaining data quality during collection. The ipacheck Stata package provides comprehensive tools to implement these checks systematically, helping research teams identify issues early and ensure reliable results.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "High-Frequency Checks"
    ]
  },
  {
    "objectID": "data-quality/data-management-system.html",
    "href": "data-quality/data-management-system.html",
    "title": "IPA‚Äôs Data Management System",
    "section": "",
    "text": "IPA‚Äôs Data Management System, or DMS, is a package of Stata and Excel tools that automate and assist your project‚Äôs data flow.",
    "crumbs": [
      "Data Quality",
      "IPA's Data Management System"
    ]
  },
  {
    "objectID": "data-quality/data-management-system.html#what-is-the-dms",
    "href": "data-quality/data-management-system.html#what-is-the-dms",
    "title": "IPA‚Äôs Data Management System",
    "section": "What is the DMS?",
    "text": "What is the DMS?\nIPA‚Äôs Data Management System, or DMS, is a Stata package that automates and assists your project‚Äôs data flow. The DMS assists you with the following functionality:\n\nCreating folders and do-files: Automatically creates a folder structure, Stata files and documentation, including for projects with multiple surveys‚Äîcreates standardized folder structure, input files, do-files and readme files\nDeciding which quality checks are important to your project: A simple global do-file with a detailed description of the different check features as well as a section to turn on and off specific checks based on your own needs‚Äîbackchecks, field comments, survey tracking, duplicates, missing values, outliers, text audits, and more\nDe-duplicating: Automatically flags all duplicates, and provides a summary of the differences of each duplicate group. Also, saves a new de-duplicated clean dataset so high-frequency checks can run.\nChecking and reporting on data quality: Runs high-frequency checks to flag issues with data quality. The system asks you to identify important checks and variables in the global do-file and Excel-based inputs sheets, then automatically generates reports for interpretation and follow-up actions to improve data quality.\nChecking and reporting on inconsistencies in the dataset: Compares and reports on the differences in responses in the Survey and Backchecks datasets.\nCorrections to the dataset: Includes an easy-to-use excel-based tool for making and logging changes to data including replacements and values, dropping of observations and flagging HFC outliers values as correct.\nReporting on enumerator performance: Generates enumerator reports with indicators of enumerator productivity and other general enumerator performance including Don‚Äôt Know/Refuse/Missing/Other specify responses rate and summary statistics by specified variables.\nReporting on survey progress: Generates progress reports on survey completion, by day/week/month and by a customizable filter variable‚Äîprogress reports by region, district, village, treatment status",
    "crumbs": [
      "Data Quality",
      "IPA's Data Management System"
    ]
  },
  {
    "objectID": "data-quality/data-management-system.html#how-to-install-the-dms",
    "href": "data-quality/data-management-system.html#how-to-install-the-dms",
    "title": "IPA‚Äôs Data Management System",
    "section": "How to install the DMS?",
    "text": "How to install the DMS?\nTo use the DMS, you will need Stata 17 or above. If you are installing the DMS for the first time, use this command in Stata:\n* install ipacheck\nnet install ipacheck, from(\"https://raw.githubusercontent.com/PovertyAction/high-frequency-checks/master\") replace\nipacheck update\n\n* after initial installation, ipacheck can be updated at any time via\nipacheck update\nTo start your project‚Äôs folder structure, use:\nipacheck new, surveys(\"NAME OF SURVEY\") folder(‚Äúpath/to/file/destination\")\nNote that if you do not include the folder option, the system will create the folder structure in your current working directory. There is also an option to make subfolders for multiple surveys:\nipacheck new, surveys(\"Survey1\" \"Survey2\") subfolders",
    "crumbs": [
      "Data Quality",
      "IPA's Data Management System"
    ]
  },
  {
    "objectID": "data-quality/data-management-system.html#learning-to-use-the-dms",
    "href": "data-quality/data-management-system.html#learning-to-use-the-dms",
    "title": "IPA‚Äôs Data Management System",
    "section": "Learning to use the DMS",
    "text": "Learning to use the DMS\nThe DMS includes a practice exercise. This exercise can help you learn how to fill out the globals do-file and the input sheets of the DMS. You can also work through the different output sheets and make corrections. If you have already updated your ipacheck package using the code in the previous section, you can do the exercise on your computer using this code:\nipacheck new, exercise folder(\"path/to/file/destination\")\nNote that if you do not include the folder option, the system will create the folders in your current directory. This code will download the folder structure and input and replacement files as ipacheck new would, as well as:\n\nPractice survey data (in 4_data/2_survey folder)\nSample data (in 4_data/2_survey)\nBackcheck data (in 4_data/3_backcheck)\nText audit media files (in 4_data/2_survey/media)\nComment media files (in 4_data/2_survey/media)\nAudio Audit files (in 4_data/2_survey/media)\n\nIf you already use an existing file structure, you can download the files only using:\nipacheck new, files folder(‚Äúpath/to/file/destination\")\nThe folders and input structure of the DMS have changed in version 4.0. You will need to make some changes to the folder structure of do-files.",
    "crumbs": [
      "Data Quality",
      "IPA's Data Management System"
    ]
  },
  {
    "objectID": "data-quality/data-management-system.html#adapting-the-dms-to-your-project",
    "href": "data-quality/data-management-system.html#adapting-the-dms-to-your-project",
    "title": "IPA‚Äôs Data Management System",
    "section": "Adapting the DMS to your project",
    "text": "Adapting the DMS to your project\nTable¬†1 describes some features that comprise the DMS, and when you should incorporate them based on the stage of your IPA project\n\n\n\nTable¬†1: Example DMS features by data collection stage\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nDescription\nBefore data collection\nDuring data collection\nAfter data collection\n\n\n\n\nipacheck new\nCreates a folder structure and/or input files for DMS. Also used to create DMS exercise\nX\nX\n\n\n\nREADME files\nEvery folder has a readme file to list files, authors, changes, and anything necessary for a successful handover\nX\nX\nX\n\n\nDMS inputs file\nInitializes inputs for outliers, other specify and enumerator stats checks\nX\nX",
    "crumbs": [
      "Data Quality",
      "IPA's Data Management System"
    ]
  },
  {
    "objectID": "data-quality/data-management-system.html#dms-training",
    "href": "data-quality/data-management-system.html#dms-training",
    "title": "IPA‚Äôs Data Management System",
    "section": "DMS Training",
    "text": "DMS Training\nSee the following excerpt for a demonstration of the DMS!\n\n\n\n\n\n\n\nDo you need support with the DMS?\n\n\n\nIf your research project has limited time or capacity to set up or run the DMS, the Global Research and Data Science team is available to provide direct technical support to any project. If interested or need support installing the DMS, email researchsupport@poverty-action.org.",
    "crumbs": [
      "Data Quality",
      "IPA's Data Management System"
    ]
  },
  {
    "objectID": "data-quality/data-anonymization.html",
    "href": "data-quality/data-anonymization.html",
    "title": "Data Anonymization",
    "section": "",
    "text": "This guide explains what data are considered personally identifiable information, what materials require anonymization, and methods for removing PII from datasets to protect research participants‚Äô privacy.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Anonymization"
    ]
  },
  {
    "objectID": "data-quality/data-anonymization.html#what-is-data-anonymization",
    "href": "data-quality/data-anonymization.html#what-is-data-anonymization",
    "title": "Data Anonymization",
    "section": "What is Data Anonymization?",
    "text": "What is Data Anonymization?\nData anonymization involves removing Personally Identifiable Information (PII) from study materials. Before starting any analyses, consider whether you need to collect and retain PII. Most analyses do not require PII. Typically, researchers use PII only to link data from different sources, multiple rounds of a longitudinal study,or for analyses that rely on geographic location data.\nIf PII is not necessary for the analysis, then split it from the data immediately. Usually, you do not need to share PII with principal investigators. Instead, anonymize the data immediately using the split_pii Stata program and share only the anonymized data with principal investigators. You can download this program through the Statistical Software Components‚ÄîSSC‚Äîarchive.\nPII includes information that researchers can use to specifically identify an individual. Due to the nature of research IPA conducts, staff manage PII of research study participants. Research teams must not release this information to external individuals, usually including principal investigators and users who access publicly accessible materials, such as from IPA‚Äôs Data Repository. Therefore, staff need access to strategies to remove this data to ensure the safety of research subjects.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Anonymization"
    ]
  },
  {
    "objectID": "data-quality/data-anonymization.html#what-data-count-as-personally-identifiable-information",
    "href": "data-quality/data-anonymization.html#what-data-count-as-personally-identifiable-information",
    "title": "Data Anonymization",
    "section": "What data count as Personally Identifiable Information?",
    "text": "What data count as Personally Identifiable Information?\nIPA conforms to the HIPAA guidelines for what data count as PII. Included in the list are:\n\nNames\nGeographic areas with a population of 20,000 or fewer\nBirth date\nContact information such as phone numbers\nLicense numbers\nIndirect identifiers that, in combination with several sets of information, can uniquely identify an individual\n\nBecause IPA conducts studies in countries outside of the US, research teams should adapt the list of PII to the relevant country context.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Anonymization"
    ]
  },
  {
    "objectID": "data-quality/data-anonymization.html#what-materials-require-anonymization",
    "href": "data-quality/data-anonymization.html#what-materials-require-anonymization",
    "title": "Data Anonymization",
    "section": "What materials require anonymization?",
    "text": "What materials require anonymization?\nAside from the names of principal investigators and, sometimes, current project staff, ensure that you remove PII from:\n\nData\nCode\nSurvey instruments\nAny other documentation that you will share with non-IRB-approved staff and external users\n\nPrioritize data and code because you will find most PII in those two sets of files. However, also review all other documents that you will share.\nThis guide discusses automated searches for PII in Stata datasets below, but code, surveys, and other documentation typically require manual review.\nAnalysts with advanced programming skills can use Python or R code to extract relevant information from documents. However, manual checks remain recommended in case the automated search misses certain keywords.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Anonymization"
    ]
  },
  {
    "objectID": "data-quality/data-anonymization.html#how-to-search-datasets-for-personally-identifiable-information",
    "href": "data-quality/data-anonymization.html#how-to-search-datasets-for-personally-identifiable-information",
    "title": "Data Anonymization",
    "section": "How to Search Datasets for Personally Identifiable Information",
    "text": "How to Search Datasets for Personally Identifiable Information\nTo ensure a Stata dataset does not contain PII, review the variables it contains. A variable may not immediately appear to contain PII. Conduct at least two sweeps of one or more datasets for clear instances of PII. You can perform sweeps using either Stata code or manual review.\n\nUsing Stata\nA quick way to screen for PII in Stata is the lookfor command. It searches all variable names and labels in a dataset for one or more keywords.\nlookfor name\nThis command lists all variables whose name or variable label contains the string ‚Äúname.‚Äù For example, the command would list a variable named fname ‚Äî for ‚Äúfirst name‚Äù ‚Äî because ‚Äúname‚Äù is a substring of fname. lookfor also stores the list of variables in the saved result r(varlist).\nTo search more than one dataset, use the lookfor_all command, available on the Statistical Software Components‚Äî] (SSC) archive. To install, type:\nssc install lookfor_all\n\n\nKeywords to Search For\nBelow is a list of keywords to consider searching for‚Äînote this list is not exhaustive:\n\nname\nbirth ‚Äî to find variables related to the respondent‚Äôs birthdate\nphone\ndistrict\ncounty\nsubcounty\nparish\nlc ‚Äî to find variables related to the respondent‚Äôs ‚Äúlocal council,‚Äù a geographical unit in some countries\nvillage\ncommunity\naddress or gps\nlat ‚Äî to find variables related to latitude\nlon ‚Äî to find variables related to longitude\ncoord ‚Äî to find variables related to GPS coordinates\nlocation\nhouse\ncompound\nschool\nsocial\nnetwork\ncensus\ngender ‚Äî in limited cases\nsex ‚Äî in limited cases\nfax or email\nip ‚Äî for IP addresses\nurl ‚Äî for Web addresses\nspecify or comment\n\n\n\nSearching Through String Variables\nSearch through string variables and their values to screen for PII. Field officers may collect non-uniform answers in string variables, which can contain PII such as names, small locations, and contact information.\nTo display the full length of string values:\nlevels of varname\nIf strings contain PII with standardized values, recode them using:\nreplace varname = subinstr(varname, \"PII VALUE\", \"ANONYMIZED VALUE\", .)\nIf strings contain PII but the values are irregular, manually recode them:\nreplace varname = \"STRING VALUE WITHOUT PII\" if varname == \"STRING VALUE WITH PII\"\nIn both cases, maintain the integrity of the string value if it contains useful information. Otherwise, create a blank or ‚Äúother‚Äù value based on the variable‚Äôs structure.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Anonymization"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html",
    "href": "data-quality/backchecks.html",
    "title": "Back Checks",
    "section": "",
    "text": "This guide covers IPA‚Äôs format for backchecks, which are essential quality control processes in survey data collection. It provides detailed protocols for planning, executing, and analyzing backchecks to ensure data accuracy, verify survey reliability, and improve fieldwork performance through systematic monitoring and feedback.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#what-are-backchecks",
    "href": "data-quality/backchecks.html#what-are-backchecks",
    "title": "Back Checks",
    "section": "What are Backchecks?",
    "text": "What are Backchecks?\nBackchecks, also known as field audits or reinterviews, are a quality control process in survey data collection to ensure accuracy and reliability. They involve re-contacting a subset of respondents to administer a mini-survey with selected questions from the original questionnaire. The responses are then compared to the initial survey to identify discrepancies, assess surveyor performance, and evaluate the robustness of the survey instrument. Backchecks help hold surveyors accountable and improve data quality by verifying how questions are asked and detecting potential errors in data collection.\nThis critical step in data quality enables researchers to monitor both the performance of the survey team and the questionnaire in the field. Back checks are just as critical to producing high-quality data as double entry of data is to ensuring data entry accuracy. Well-done back checks can lead to:\n\nQuestionnaire revisions.\nImproved training.\nAdditional rounds of surveying.\nChanges to the survey team.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#why-is-important-to-backcheck",
    "href": "data-quality/backchecks.html#why-is-important-to-backcheck",
    "title": "Back Checks",
    "section": "Why is important to Backcheck?",
    "text": "Why is important to Backcheck?\nSurveying can be arduous and exhausting. To get out of the heat and back to the office, surveyors are tempted to cut corners in the field. The most common shortcuts are:\n\nSkipping sections or entire surveys.\nFailing to prompt correctly.\nModifying examples or informed consent scripts.\nPrematurely classifying respondents as missing or away.\n\nIn your mission to collect high-quality data, you need to develop a systematic way to detect poor surveying and incentivize high-quality fieldwork. Essentially, researchers need a disincentive for surveyors to take shortcuts and falsify data.\nBeyond poor administration, researchers also want to methodically monitor how well your questionnaire is performing in the field. Researchers need to determine\n\nAre respondents changing their answers to questions that shouldn‚Äôt change?\nDo key outcomes vary significantly?\n\nTo understand whether your questionnaire accurately captures the key outcomes of your study, you need a tool that measures the quality of your measures.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#ipa-sample-selection-protocols",
    "href": "data-quality/backchecks.html#ipa-sample-selection-protocols",
    "title": "Back Checks",
    "section": "IPA Sample Selection Protocols",
    "text": "IPA Sample Selection Protocols\n\nRecommended Best Practices\nThe following guidelines are recommended as best practices by IPA:\n\nSample Size: Back check questionnaires should be administered to at least 10% of surveys, or 40 surveys per week, whichever is higher. Larger surveys may back check less than 10%, while smaller surveys may need to back check between 15-20%.\nProportional Coverage: Ensure you are back checking a similar proportion of surveys for enumerators and geographic areas. For CATI surveys, back check more than 10%.\nTiming: Every team and every surveyor should be back checked as soon as possible, ideally within the first week of surveying, and regularly after that. Aggressively back check during the first few weeks, then reduce to 10% for the rest of the surveying phase.\nFrequency: Each enumerator should be checked at least once per week throughout the survey.\nMissing and Replacement Respondents: Include a proportional number of missing and replacement respondents in the back check sample.\nRandom Selection: The selection of households for back checks must be random. Stratify the sampling when appropriate.\nMultiple Versions: Use multiple versions of the back check questionnaire, covering the entire survey. Each version should include a minimum of 10 questions or 10% of the total survey instrument.\nSurvey Length: Keep the back check survey significantly shorter than the main survey. Phone back checks should be no longer than 10 minutes.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#making-it-happen-how-to-design-the-back-check-survey",
    "href": "data-quality/backchecks.html#making-it-happen-how-to-design-the-back-check-survey",
    "title": "Back Checks",
    "section": "Making it Happen: How to Design the Back Check Survey",
    "text": "Making it Happen: How to Design the Back Check Survey\n\nDevelop Your Plan Before You Start Surveying\nThe key to a quality back check is planning. Multiple analyses conducted by IPA staff have shown that the time elapsed since the original survey is the most common statistically significant predictor of the number of errors in back checks across projects and contexts. The back check team should be ready to begin fieldwork on day one of the survey period.\nCraft your back check plan at the same time as your survey work plan and maintain it as a running document, updating details as the survey work plan evolves. This ensures that at the end of the survey, you have an accurate description of how the back checks were carried out and any actions taken.\nWhen developing your plan, address the following questions:\n\n\n\n\n\n\nDesign\n\n\n\n\n\n\nHow will you conduct your back checks? Can you do them in person?\nHow many surveys will you back check? 10% or more? Will you change the percentage during the survey period?\nHow will you spread them across your enumeration areas, surveyors, and survey period?\nHow many questionnaires will you use? Multiple questionnaires are optimal, especially for longer, more complex surveys.\n\n\n\n\n\n\n\n\n\n\nBudget\n\n\n\n\n\n\nHow much money do you have for your back check? Note: Back checks should be budgeted during the project development stage.\nIf your budget is tight, can you implement phone back checks or other low-cost strategies?\n\n\n\n\n\n\n\n\n\n\nLogistics and Team Structure\n\n\n\n\n\n\nWhen will the back checks be done? Ideally, within 1-2 days. One week is the maximum.\nHow big will your back check team be, and how will you train them?\nAre there enough back checkers to check all enumerators each week? Do you need to hire more back checkers during the first few weeks of surveying?\nHow will the team get to the field? If conducting back checks by phone, do you have enough phones?\n\n\n\n\n\n\n\n\n\n\nAnalysis and Action\n\n\n\n\n\n\nHow will you define and calculate errors?\nHow will you deal with discrepancies? Create a plan with your field management team.\nHow will you log back check errors? Aim to integrate this into your master tracking system.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#selecting-and-managing-the-back-check-team",
    "href": "data-quality/backchecks.html#selecting-and-managing-the-back-check-team",
    "title": "Back Checks",
    "section": "Selecting and Managing the Back Check Team",
    "text": "Selecting and Managing the Back Check Team\n\nSelecting the Right Team\nA back check team member should be more qualified than your regular surveyors. In detail, a member of the back check team should be:\n\nExperienced: Consider retaining the team of surveyors used during the pilot as the back check team.\nTrustworthy: Prefer someone you‚Äôve worked with before.\nIndependent and Enterprising: Members may have to travel to villages individually.\n\n\n\nTraining\nThe back check team should attend the main training with the entire team to ensure everyone understands the questions in the same way. Consider using them as leaders at the training of the main survey team.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#executing-the-back-check",
    "href": "data-quality/backchecks.html#executing-the-back-check",
    "title": "Back Checks",
    "section": "Executing the Back Check",
    "text": "Executing the Back Check\n\nTiming and Revisits\n\nAim to complete back checks within 1-2 days of the original survey.\nDecide on the number of revisits the back check team should aim to do. Ideally, the team will do the same number of revisits as the original surveyors.\n\n\n\nSelecting Respondents\n\nThe back check team should never select the households. Use Stata or Excel to select respondents to back check using random selection, stratified by surveyor.\nIf your project requires replicable randomization for back checks, use Stata. If not, use SurveyCTO randomization.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#producing-and-acting-on-results",
    "href": "data-quality/backchecks.html#producing-and-acting-on-results",
    "title": "Back Checks",
    "section": "Producing and Acting on Results",
    "text": "Producing and Acting on Results\n\nAnalysis Framework\n\nDuring questionnaire design, create your analysis framework and define what you consider an error.\nUse IPA‚Äôs user-written Stata command bcstats to compare survey data and back check data.\n\n\n\nAcceptable Range of Deviation\n\nEstablish a range of acceptable deviation for every back check question. For example, age +/-5 years.\n\n\n\nAnalysis for Type 1, Type 2, Type 3\n\nType 1: Look at the overall error rate. If it is greater than 10%, this is a red flag.\nType 2: Perform the same analysis as Type 1, but examine these variables individually.\nType 3: Examine the overall error rates by question and perform stability checks.\n\n\n\nWhen and How to Take Action on Results\n\nResults of the back check comparison should be discussed with the field leadership team.\nSet clear standards for when to fire surveyors, when to follow up with respondents, and when to re-do surveys.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#digital-back-checks",
    "href": "data-quality/backchecks.html#digital-back-checks",
    "title": "Back Checks",
    "section": "Digital Back Checks",
    "text": "Digital Back Checks\nFor CATI surveys, audio audits can be used to monitor enumerator performance. However, audio audits will not fulfill the back check‚Äôs function of testing the reliability of a survey question. If your PI is concerned with the stability and quality of the survey questions, a back check is required.\n\n\n\n\n\n\nBack Checks in practice\n\n\n\nCheck out the IPA High Frequency Check Exercise! These exercises help familiarize users with the setup and use of IPA‚Äôs Data Management System, which includes High-Frequency Checks (HFCs), Survey Tracking, and Back Check comparison. Learn more",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/backchecks.html#conclusion",
    "href": "data-quality/backchecks.html#conclusion",
    "title": "Back Checks",
    "section": "Conclusion",
    "text": "Conclusion\nBack checks are a critical component of ensuring high-quality data collection. By following the guidelines outlined in this manual, you can effectively implement back checks, analyze the results, and take appropriate actions to improve your survey process.\nBack checks are a critical component of ensuring high-quality data collection. By following the guidelines outlined in this manual, researchers can implement back checks, analyze the results, and take appropriate actions to improve the survey process.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Backchecks"
    ]
  },
  {
    "objectID": "data-quality/action-plan.html",
    "href": "data-quality/action-plan.html",
    "title": "Data Quality Action Plan",
    "section": "",
    "text": "This guide outlines the Data Quality Action Plan, a tool designed to help IPA research staff systematically manage data quality throughout the data lifecycle.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Quality Action Plan"
    ]
  },
  {
    "objectID": "data-quality/action-plan.html#what-is-the-data-quality-action-plan",
    "href": "data-quality/action-plan.html#what-is-the-data-quality-action-plan",
    "title": "Data Quality Action Plan",
    "section": "What is the Data Quality Action Plan?",
    "text": "What is the Data Quality Action Plan?\nPreparing for data collection often involves juggling multiple tasks, which can limit the ability of research staff to fully use tools like the DMS four-point-zero. The Data Quality Action Plan (DQAP) addresses this challenge by offering a structured framework for planning and executing data quality activities. It helps teams:\n\nIdentify and resolve data quality issues.\nMaximize the use of the DMS four-point-zero.\nEnhance the actionability of data quality checks.\n\nThe DQAP focuses on eight core data quality tasks, each designed to address specific issues during data collection. This template serves as a centralized repository for planning and tracking these activities, ensuring a comprehensive approach to data quality management.\n\n\n\nMain data quality activities",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Quality Action Plan"
    ]
  },
  {
    "objectID": "data-quality/action-plan.html#purpose-of-the-dqap",
    "href": "data-quality/action-plan.html#purpose-of-the-dqap",
    "title": "Data Quality Action Plan",
    "section": "Purpose of the DQAP",
    "text": "Purpose of the DQAP\nThe DQAP aims to establish a systematic approach to data quality management, with the following objectives:\n\nFacilitate Training and Tool Adoption: Provide clear guidelines for research staff to plan, program, and act on data quality checks.\nEnhance Actionability: Offer actionable insights and recommendations for addressing data quality issues.\nBoost Accountability: Clearly define roles and responsibilities for Principal Investigators (PIs), Research Managers, Research Associates, and Field Managers in data quality activities.\nPromote DMS Four-Point-Zero Use: Ensure teams leverage the full potential of the DMS four-point-zero in all IPA data collection initiatives.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Quality Action Plan"
    ]
  },
  {
    "objectID": "data-quality/action-plan.html#the-dqap-tool",
    "href": "data-quality/action-plan.html#the-dqap-tool",
    "title": "Data Quality Action Plan",
    "section": "The DQAP Tool",
    "text": "The DQAP Tool\nThe DQAP tool is a Google Sheets template with five sheets, three of which require input: DataQuality_Tasks, DMS, and Issues and Actions.\n\n\n\nMain Page of the Data Quality Action Plan\n\n\nThe DQAP template is available to IPA staff on Box. You can access it using the following link: DQAP Template on Box.\nIf you are unable to access the file, contact Research Support at researchsupport@poverty-action.org.\n\nBelow is an overview of each sheet\n\n\n\n\n\n\nHow-to Guide\n\n\n\n\n\nThis sheet provides step-by-step instructions for filling out the DQAP. It uses color-coding for clarity:\n\nGreen: Fields that require editing.\nBlue: Fields that you should not modify.\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\nThis sheet summarizes the finalized DQAP for sharing with PIs, Research Managers, and Field Coordinators. It highlights the active tasks and checks, ensuring alignment and accountability across the team.\n\n\n\n\n\n\n\n\n\nData Quality Tasks\n\n\n\n\n\nThis sheet outlines the eight core data quality tasks shown in the figure above. It includes:\n\nBlue Columns (Pre-filled):\n\nDescription: Explains each task.\nHow Can This Activity Help?: Describes how the task identifies issues.\nWhat Does This Activity Require?: Lists prerequisites for implementation.\n\nGreen Columns (Research staff complete these):\n\nActivity Active in This Project?: Checkbox to indicate if the task is in use.\nWhat Will the Project Check?: A brief description of the task‚Äôs purpose for the project.\nResources: Links to manuals and guides for implementing each task.\n\n\n\n\n\n\n\n\n\n\n\nDMS - Data Management System\n\n\n\n\n\nThis sheet details the checks available in the DMS four-point-zero. It includes:\n\nPre-filled Columns:\n\nGlobal do-file section: Identifies the section of the do-file to modify.\nInput File/Sheet: Specifies the input files and sheets required.\nOutput File/Sheet: Names the output files and sheets.\nDescription: Explains the purpose of each check.\nHow Can This Information Be Used?: Suggests potential uses for the output.\nRecommended Resources: Links to IPA resources.\n\nGreen Columns (Research staff complete these):\n\nCheck Active?: Checkbox to indicate if the check is in use.\nSurvey Variables and Information for Checks: Lists variables or parameters needed for the check.\n\n\n\n\n\n\n\n\n\n\n\nIssues and Actions\n\n\n\n\n\nThis sheet serves as a reference for identifying and resolving common data quality issues. It includes:\n\nIssues:\n\nIssue Descriptions: Describes potential data quality issues.\n\nChecks:\n\nIdentifies the data quality activity or DMS output that can detect the issue.\n\nActions:\n\nProgramming Actions: Solutions for issues related to data management and cleaning.\nField Protocol Actions: Recommendations for improving enumerator performance or field logistics.\nPerson Responsible: Specifies the team member responsible for each action (editable).",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Quality Action Plan"
    ]
  },
  {
    "objectID": "data-quality/action-plan.html#how-to-design-a-dqap-for-your-project",
    "href": "data-quality/action-plan.html#how-to-design-a-dqap-for-your-project",
    "title": "Data Quality Action Plan",
    "section": "How to Design a DQAP for Your Project",
    "text": "How to Design a DQAP for Your Project\nFollow these steps to create a tailored DQAP for your data collection initiative:\n\nSteps for DQAP\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\n1\nCreate a copy of the DQAP template in your project‚Äôs Box folder. Keep it as a Google Sheet to preserve conditional formatting.\n\n\n2\nReview and fill out the DataQuality_Tasks sheet. Mark the tasks you plan to use and describe what to check.\n\n\n3\nReview and fill out the DMS sheet. Activate relevant checks and specify the required variables.\n\n\n4\nReview the Issues and Actions sheet. Filter active checks and add or modify issues and actions as needed.\n\n\n5\nAssign responsibilities for each action in the Issues and Actions sheet based on your team‚Äôs structure.\n\n\n6\nShare the finalized DQAP with your team using the Summary sheet. Gather feedback to ensure alignment and accountability.\n\n\n7\nBegin programming your DMS based on the finalized plan.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Quality Action Plan"
    ]
  },
  {
    "objectID": "data-quality/action-plan.html#conclusion",
    "href": "data-quality/action-plan.html#conclusion",
    "title": "Data Quality Action Plan",
    "section": "Conclusion",
    "text": "Conclusion\nThe Data Quality Action Plan is a powerful tool for ensuring high-quality data collection. By following this structured approach, research teams can proactively address data quality issues, maximize the use of the DMS four-point-zero, and ensure accountability across all stages of the data lifecycle.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Quality Action Plan"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-surveys-twilio.html",
    "href": "data-collection/whatsapp/whatsapp-surveys-twilio.html",
    "title": "Setting up Twilio for WhatsApp Surveys",
    "section": "",
    "text": "This article provides a step-by-step guide to setting up Twilio for WhatsApp surveys, offering practical insights into its implementation and pricing considerations.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Twilio Setup"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-surveys-twilio.html#overview-of-twilio-as-a-tool",
    "href": "data-collection/whatsapp/whatsapp-surveys-twilio.html#overview-of-twilio-as-a-tool",
    "title": "Setting up Twilio for WhatsApp Surveys",
    "section": "Overview of Twilio as a Tool",
    "text": "Overview of Twilio as a Tool\nTwilio is a versatile cloud communications platform that enables developers and organizations to build, scale, and operate communication solutions across various channels, including SMS, voice, video, email, and WhatsApp. Its robust APIs and user-friendly interface make it a preferred choice for integrating communication functionalities into applications and workflows.\n\nWhy Choose Twilio Over Alternatives?\nWhile there are several alternatives like EngageSpark, Twilio stands out due to its:\n\nScalability: Twilio‚Äôs infrastructure handles high volumes of communication, making it suitable for both small-scale and enterprise-level projects.\nFlexibility: With support for multiple communication channels, Twilio allows seamless integration across platforms, enabling organizations to diversify their outreach strategies.\nDeveloper-Friendly APIs: Twilio provides comprehensive documentation and SDKs, making it easier for developers to implement and customize solutions.\nGlobal Reach: Twilio‚Äôs extensive network ensures reliable communication across regions, with localized support for various countries.\nAdvanced Features: Features like real-time analytics, message tracking, and two-way communication enhance the overall user experience.\nCost-Effectiveness: Twilio‚Äôs pay-as-you-go pricing model ensures that organizations only pay for what they use, making it a budget-friendly option.\n\nIn comparison, EngageSpark, while user-friendly, is more limited in terms of scalability, advanced features, and integration capabilities. Twilio‚Äôs robust ecosystem and flexibility make it a more comprehensive solution for organizations looking to implement sophisticated communication workflows.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Twilio Setup"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-surveys-twilio.html#twilio-for-whatsapp-surveys",
    "href": "data-collection/whatsapp/whatsapp-surveys-twilio.html#twilio-for-whatsapp-surveys",
    "title": "Setting up Twilio for WhatsApp Surveys",
    "section": "Twilio for WhatsApp Surveys",
    "text": "Twilio for WhatsApp Surveys\nIn today‚Äôs digital age, reaching out to participants through efficient and reliable communication channels is crucial for data collection and engagement. WhatsApp, with its widespread usage and user-friendly interface, presents an excellent platform for conducting surveys. Twilio, a cloud communications platform, offers robust tools and APIs to integrate WhatsApp into your survey workflows seamlessly.\nBy leveraging Twilio‚Äôs capabilities, researchers and organizations can automate survey distribution, track responses in real-time, and ensure higher engagement rates. This guide will walk you through the process of setting up Twilio for WhatsApp surveys, providing practical insights into its implementation and pricing considerations.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Twilio Setup"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-surveys-twilio.html#key-twilio-concepts-for-whatsapp-surveys",
    "href": "data-collection/whatsapp/whatsapp-surveys-twilio.html#key-twilio-concepts-for-whatsapp-surveys",
    "title": "Setting up Twilio for WhatsApp Surveys",
    "section": "Key Twilio Concepts for WhatsApp Surveys",
    "text": "Key Twilio Concepts for WhatsApp Surveys\nTo use Twilio for WhatsApp surveys, it is important to understand the following key concepts:\n\nTwilio Console: The centralized interface where users manage and control various Twilio services. It is the primary dashboard for configuring, monitoring, and optimizing communication functionalities.\nWhatsApp Functionality: Enhanced communication capabilities unlocked by upgrading a Twilio account. This includes leveraging WhatsApp for surveys and other interactive communication.\nWhatsApp Sender Management: Within the Twilio Console, users manage WhatsApp senders through the ‚ÄúMessaging‚Äù section. This functionality provides oversight and control over sender-related activities.\nAPI Access Request: Users can create new senders within the Twilio Console, request access to the WhatsApp API, and submit necessary information for approval. This process enables integration with WhatsApp services.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Twilio Setup"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-surveys-twilio.html#setting-up-a-twilio-console",
    "href": "data-collection/whatsapp/whatsapp-surveys-twilio.html#setting-up-a-twilio-console",
    "title": "Setting up Twilio for WhatsApp Surveys",
    "section": "Setting up a Twilio Console",
    "text": "Setting up a Twilio Console\nFollow these steps to set up a Twilio console for WhatsApp data collection:\n\nGo to the Twilio website and sign up for an account. Provide the required information and complete the registration process.\nAfter signing up, log in to your Twilio account. Verify your email and log in.\n\n\n\nTwilio setup and login\n\n\nAfter logging in, navigate to the Twilio Console, the main dashboard for managing your Twilio services.\nClick on the ‚ÄúPhone Numbers‚Äù dropdown in the top-left corner and select ‚ÄúManage.‚Äù Then select ‚ÄúBuy a number‚Äù and pick a phone number in the required location.\n\n\n\nBuying a phone number in the Twilio Console\n\n\nUpgrade the account to access WhatsApp functionalities.\n\n\n\nUpgrading the Twilio Console",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Twilio Setup"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-surveys-twilio.html#twilio-pricing-and-fees",
    "href": "data-collection/whatsapp/whatsapp-surveys-twilio.html#twilio-pricing-and-fees",
    "title": "Setting up Twilio for WhatsApp Surveys",
    "section": "Twilio Pricing and Fees",
    "text": "Twilio Pricing and Fees\nUnderstanding Twilio‚Äôs pricing structure is crucial for budgeting WhatsApp survey projects. Below are the key pricing details:\nFor the most up-to-date and detailed pricing information, see the Twilio WhatsApp Pricing Page. If you are planning to conduct a WhatsApp survey, consider using the calculations included on the Twilio page to estimate your costs accurately.\n\nConversation initiation cost: USD$0.010 per 24 hours\nUser-initiated conversation cost: USD$0.006 per 24 hours\nFixed monthly cost per WhatsApp number: USD$1.00\nCost per message sent: $0.005\n\nThese prices might vary depending on the region where the survey is going to take place.\nTwilio requires purchasing a phone number, and automatic top-ups are enabled by default to ensure service continuity.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Twilio Setup"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-surveys-twilio.html#verifying-your-twilio-console",
    "href": "data-collection/whatsapp/whatsapp-surveys-twilio.html#verifying-your-twilio-console",
    "title": "Setting up Twilio for WhatsApp Surveys",
    "section": "Verifying Your Twilio Console",
    "text": "Verifying Your Twilio Console\nThe Twilio console verification process takes approximately one week. After approval, you can proceed with setting up your WhatsApp profile.\n\nNavigate to ‚ÄúMessaging‚Äù in the Console and select the ‚ÄúSenders‚Äù tab to view WhatsApp senders.\nClick on ‚ÄúCreate New Sender‚Äù and fill out the necessary fields, selecting the purchased phone number.\nSubmit the request for WhatsApp API access.\n\n\n\n\nCreate WhatsApp sender\n\n\nFor increased trust and improved response rates, consider Meta Business Verification to obtain a green tick for your WhatsApp sender.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Twilio Setup"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-surveys-twilio.html#creating-a-whatsapp-profile",
    "href": "data-collection/whatsapp/whatsapp-surveys-twilio.html#creating-a-whatsapp-profile",
    "title": "Setting up Twilio for WhatsApp Surveys",
    "section": "Creating a WhatsApp Profile",
    "text": "Creating a WhatsApp Profile\nFinalize your Twilio setup by configuring your WhatsApp profile:\n\nNavigate to the created sender and update the profile with an image, name, and description.\nSave changes and start your first WhatsApp survey.\n\n\n\n\nCreate WhatsApp profile",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Twilio Setup"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-deploy.html",
    "href": "data-collection/whatsapp/whatsapp-deploy.html",
    "title": "Deploying a WhatsApp Survey",
    "section": "",
    "text": "Learn how to prepare, configure, and deploy WhatsApp surveys using Twilio integration with Google Sheets for data collection. This guide covers essential steps to configure your computer and tools, create a cases file, manage data collection, and launch surveys.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Deploying a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-deploy.html#step-1-setting-up-your-computer",
    "href": "data-collection/whatsapp/whatsapp-deploy.html#step-1-setting-up-your-computer",
    "title": "Deploying a WhatsApp Survey",
    "section": "Step 1: Setting Up Your Computer",
    "text": "Step 1: Setting Up Your Computer\nBefore deploying WhatsApp surveys, ensure you have:\n\nWhatsApp Business API access and approval\nTwilio account with active phone number\nGoogle account for Google Sheets integration\nBasic familiarity with command-line interface\nStatistical software such as Stata, Python, R, or similar for data preparation\n\nDownload and install the essential programs required for WhatsApp survey deployment. The IPA GitHub user guide provides detailed instructions for streamlining your computer setup with the necessary tools.\n\n IPA GitHub user guide\n\n\nAfter completing the installation, proceed to create your cases file.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Deploying a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-deploy.html#step-two-creating-a-cases-file",
    "href": "data-collection/whatsapp/whatsapp-deploy.html#step-two-creating-a-cases-file",
    "title": "Deploying a WhatsApp Survey",
    "section": "Step Two: Creating a Cases File",
    "text": "Step Two: Creating a Cases File\nAfter you complete the console setup and obtain the necessary approvals from WhatsApp, you are ready to deploy a survey. This method involves initiating communication from your profile, prompting user engagement. This method is different from waiting for user-initiated communication, where you will have an open chat and you don‚Äôt have a specific target sample.\nThe first step to start the process is creating a ‚Äúcases file.‚Äù This file is a dataset with participants‚Äô phone numbers and any additional information relevant to your survey. If you consider including personalized messages or details to populate survey fields, you should include this information as dataset variables. Another recommendation is to incorporate an identifier to keep track of submissions without using any PII for effective tracking.\nFollow these steps to construct your cases file:\n\nThe file should be in xlsx format. Use Stata, Python, R, or your preferred statistical software to create this document to reduce manual input errors.\nEnsure the Excel file includes a column or variable named ‚ÄúNumber,‚Äù with a capital N, structured as ‚Äúwhatsapp:+‚Äù followed by the phone number and country identifier, as depicted in the image.\nName other variables according to your preference. If numeric values are present, store them as text to prevent Excel from altering them.\nStore the ‚Äúcases‚Äù dataset in a secure and encrypted folder on your computer to ensure data security.\n\nYour cases file should look like this:\n\n\n\nCases file example",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Deploying a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-deploy.html#step-three-managing-data-with-google-sheets",
    "href": "data-collection/whatsapp/whatsapp-deploy.html#step-three-managing-data-with-google-sheets",
    "title": "Deploying a WhatsApp Survey",
    "section": "Step Three: Managing Data with Google Sheets",
    "text": "Step Three: Managing Data with Google Sheets\nA final step before launching a WhatsApp survey is to configure the Google Sheet, where you‚Äôll collect data from your surveys and WhatsApp interactions. Follow these steps to ensure a smooth data collection setup:\n\nCreate a Google Sheet: Initiate the process by creating a Google Sheet to serve as the repository for collected data.\nConfigure Twilio Functions:\n\nAccess the Twilio console‚Äôs submenu labeled ‚ÄúFunctions and Assets.‚Äù\nNavigate to ‚ÄúFunctions (Classic)‚Äù and select ‚ÄúConfigure.‚Äù\nIn the ‚ÄúDependencies‚Äù section, complete the specified fields, as illustrated in the provided image.\n\n\n\n\n\nDependencies configuration snapshot\n\n\n\nCreate two environment variables:\n\nclient_email: twilio-data-connector-to-gsuit@twilio-gsuite-connection.iam.gserviceaccount.com\nsheetId: Retrieve this from the link of your Google Sheet. It is the alphanumeric group in the link after ‚Äú/d/‚Äù and before the next ‚Äú/.‚Äù\n\n\n\n\n\nEnvironment variables configuration snapshot\n\n\n\nCreate a New Function:\n\nProceed to the ‚ÄúList‚Äù submenu and generate a new function from a blank interface named ‚Äúfunction_gsheets.‚Äù\nCopy the provided function code into the code section.\n\n\n\n publish gsheet code\n\n\n\nSave and Deploy: Save the function, and deployment will occur automatically.\nShare Google Sheet: Share your Google Sheet with the email from the client_email environment variable.\nIntegrate the Function into Your Flow:\n\nAdd the newly created function within your flow at the end, as it will compile answers from the entire flow.\nThe arguments from the function have a key‚Äîthe name of the variable and value the way Twilio recognizes the answer inputted by a participant:\n\nIf you want to publish the answer to a question widget you have on your flow, you need to use the following value in the function: {widgets.name_of_the_widgets_variable.inbound.body}\nIf you want to publish data from your ‚Äúcases‚Äù file, use the following value in the function: {flow.data.name_of_the_variable}, remember that you need to include this variable in the cases and the launch code\nIf you want to publish variables you created inside the Twilio flow, you need to use the following value in the function: {flow.variables.name_of_the_variable}\n\n\nHeader Alignment in Google Sheet: Align the variable names defined in step 7 with the headers of your Google Sheet columns to let Twilio know how to populate the data in the document.\n\n\n\n\nGoogle Sheet example\n\n\nWith these steps completed, you can deploy your survey.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Deploying a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-deploy.html#step-four-launching-your-survey",
    "href": "data-collection/whatsapp/whatsapp-deploy.html#step-four-launching-your-survey",
    "title": "Deploying a WhatsApp Survey",
    "section": "Step Four: Launching Your Survey",
    "text": "Step Four: Launching Your Survey\nTo launch your survey, you will use the following Python code. Follow these next steps:\n\nOpen a Shell Prompt: Open a command prompt ‚Äì Bash, PowerShell, or other ‚Äì on your computer to start the process.\nRun the Python Script:\n\ncd requests_to_twilio\n\npython twilio_launcher.py --account_sid your_account_sid --account_token your_account_token --twilio_number your_twilio_number --flow_id flow_id --input_file full_path_to_input_file --batch_size batch_size --sec_between_batches sec_between_batches --columns_with_info_to_send caseid,name,city,gender,age\nEnsure you replace the placeholders; here is where you can find the information needed:\n\nyour_account_sid: In your Twilio‚Äôs console main page\nyour_account_token: In your Twilio‚Äôs console main page\nyour_twilio_number: In your Twilio‚Äôs console main page\nflow_id: In the flow menu in the console\nfull_path_to_input_file: The computer directory where your ‚Äúcases‚Äù file exists\nbatch_size: You must define how many surveys you will send. Set it at 20 per batch as recommended. You will need to replace it with just the number. This is a recommendation based on infield experience when deploying WhatsApp surveys. This number of messages prevents the API from overloading and won‚Äôt crash.\nsec_between_batches: You must define how many seconds you will wait between batches. The recommended interval is 10 seconds. You will need to replace it with just the number.\nInclude Additional Information: If your file contains extra columns with pertinent information, specify them in the columns_with_info_to_send argument, separating column names by commas.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you encounter trouble running this code, email:  researchsupport@poverty-action.org",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Deploying a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/index.html",
    "href": "data-collection/whatsapp/index.html",
    "title": "WhatsApp Surveys: An Innovative and Low-Cost Solution for Remote Data Collection",
    "section": "",
    "text": "WhatsApp has become a powerful tool for data collection, offering a cost-effective, scalable, and user-friendly alternative to traditional survey methods. This guide provides an introduction to WhatsApp surveys, their benefits, limitations, and best practices based on IPA‚Äôs experience across various projects.\nWhatsApp has become a powerful tool for data collection, offering a cost-effective, scalable, and user-friendly alternative to traditional survey methods. At IPA, WhatsApp surveys have helped overcome challenges in reaching participants, especially during the COVID-19 pandemic. This page provides an introduction to WhatsApp surveys, their benefits, limitations, and best practices based on IPA‚Äôs experience.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "About WhatsApp Surveys"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/index.html#why-use-whatsapp-for-surveys",
    "href": "data-collection/whatsapp/index.html#why-use-whatsapp-for-surveys",
    "title": "WhatsApp Surveys: An Innovative and Low-Cost Solution for Remote Data Collection",
    "section": "Why Use WhatsApp for Surveys?",
    "text": "Why Use WhatsApp for Surveys?\n\n\n\n\n\n\nWide User Base\n\n\n\n\n\nWhatsApp has over two billion active users globally and is widely adopted across different demographics and regions.\n\n\n\n\n\n\n\n\n\nHigh Engagement and Familiarity\n\n\n\n\n\nPeople are already comfortable using WhatsApp for communication. Surveys can be completed on smartphones, anytime, and anywhere.\n\n\n\n\n\n\n\n\n\nCost-Effective Solution\n\n\n\n\n\nWhatsApp reduces costs compared to traditional survey methods, creating savings on printing, fieldwork, and call center expenses.\n\n\n\n\n\n\n\n\n\nRich Media Capabilities\n\n\n\n\n\nWhatsApp supports images, videos, and audio files for interactive surveys, enhancing comprehension and engagement.\n\n\n\n\n\n\n\n\n\nReal-Time Responses\n\n\n\n\n\nData collection happens instantly, allowing for real-time monitoring and quick decision-making based on responses.\n\n\n\n\n\n\n\n\n\nHigher Response Rates\n\n\n\n\n\nFamiliarity with WhatsApp leads to increased participation. Evidence from IPA projects in Colombia, Senegal, and Guinea supports this finding.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "About WhatsApp Surveys"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/index.html#whatsapp-surveys-at-ipa",
    "href": "data-collection/whatsapp/index.html#whatsapp-surveys-at-ipa",
    "title": "WhatsApp Surveys: An Innovative and Low-Cost Solution for Remote Data Collection",
    "section": "WhatsApp Surveys at IPA",
    "text": "WhatsApp Surveys at IPA\nIPA has successfully implemented WhatsApp surveys in various projects across different regions.\n\n\n\nWhatsApp Surveys: Temporary Statute of Protection for Venezuelans\n\n\n\nProject Examples\n\nMonitoring Migration and Legal Status: A study by IPA Colombia on Colombia‚Äôs policy to grant legal status to Venezuelan migrants employed WhatsApp surveys to reach a mobile population. Even when phone numbers changed, WhatsApp numbers remained stable, improving follow-up success.\nTracking Social Protection Interventions: Research teams used WhatsApp to communicate project details and schedule phone surveys for studies on entrepreneurship training and social protection during COVID-19.\nAssessing Program Spillovers: A project on socio-emotional skills employed WhatsApp surveys to track whether control group participants had received materials related to the intervention.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "About WhatsApp Surveys"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/index.html#advantages-and-limitations",
    "href": "data-collection/whatsapp/index.html#advantages-and-limitations",
    "title": "WhatsApp Surveys: An Innovative and Low-Cost Solution for Remote Data Collection",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nKey Benefits\n\nVerified WhatsApp Business Account: IPA uses a verified account with the organization‚Äôs logo as a profile picture, improving participant recognition and trust. The verified account displays IPA‚Äôs name instead of an unknown number, increasing response rates.\nMessage Tracking: WhatsApp‚Äôs double-check marks allow researchers to monitor survey progress by identifying:\n\nFailed deliveries due to inactive phone numbers.\nMessages that participants received and read‚Äîsubject to user privacy settings.\n\nScalability and Multi-Format Engagement: The ability to send images, voice messages, and documents makes communication more effective.\nReal-Time Data Collection: Responses arrive instantly, allowing researchers to adapt their approach or follow up as needed.\n\n\n\nChallenges and Considerations\n\nParticipant Requirements: WhatsApp surveys work best with populations that have internet access, smartphones, and some level of technological literacy. This may limit sampling to specific geographic, age, and socioeconomic groups.\nNo Enumerator Supervision: Without an enumerator present, verifying that participants understand survey questions can be difficult. To mitigate this, researchers should design surveys with clear, simple questions.\nPrivacy Limitations: Some WhatsApp privacy settings may prevent researchers from knowing whether participants have read messages.\nManaging Expectations: In some cases, participants expected real-time human responses. Clarifying automated interactions upfront helps mitigate confusion.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "About WhatsApp Surveys"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/index.html#the-role-of-twilio-in-whatsapp-surveys",
    "href": "data-collection/whatsapp/index.html#the-role-of-twilio-in-whatsapp-surveys",
    "title": "WhatsApp Surveys: An Innovative and Low-Cost Solution for Remote Data Collection",
    "section": "The Role of Twilio in WhatsApp Surveys",
    "text": "The Role of Twilio in WhatsApp Surveys\nTwilio is a cloud communication platform that allows organizations to send automated WhatsApp messages and conduct surveys. At IPA, Twilio supports:\n\nWhatsApp and SMS surveys.\nInteractive Voice Response (IVR) surveys.\nAutomated reminders for data collection visits.\nVirtual call center support through WhatsApp, voice, and video.\n\nTwilio has undergone testing in multiple IPA projects, including Peace and Education Program, Think Equal, Unidades Econ√≥micas Productivas de Antioquia, Ingreso Solidario, Play to Learn, Estatuto Temporal de Protecci√≥n para Venezolanos, Sisb√©n, and Lego. Due to its success, other IPA regional offices have adopted it for data collection and participant engagement.\nThe Global Research and Data Science, GRDS, team provides support in planning and programming surveys or interventions through WhatsApp, SMS, and IVR using Twilio.\nEvery data collection method has its trade-offs, and this guide does not compare all available options. For IPA, particularly during the disruptions of COVID-19, WhatsApp surveys proved to be a versatile and effective tool for remote data collection. The more experience researchers gain, the better they become at using them. If WhatsApp surveys might be useful for your study, consider giving them a try.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "About WhatsApp Surveys"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/interviews.html",
    "href": "data-collection/qualitative-methods/interviews.html",
    "title": "Qualitative Interviews",
    "section": "",
    "text": "Practical guidance for researchers and practitioners on how to plan and conduct qualitative interviews, including team structuring, logistics planning, interview techniques, and managing common challenges.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Interviews"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/interviews.html#what-are-qualitative-interviews",
    "href": "data-collection/qualitative-methods/interviews.html#what-are-qualitative-interviews",
    "title": "Qualitative Interviews",
    "section": "What are qualitative interviews?",
    "text": "What are qualitative interviews?\nInterviews are a qualitative method of gathering information through a formal dialog between two people‚Äîthe interviewer and the interviewee‚Äîguided by a specific research goal. Unlike conventional conversations, which involve symmetrical communication and relationship between the interlocutors1, a qualitative interview uses a structured approach. In this setting, the interviewee permits the interviewer to direct the conversation and guide the dialog in accordance with the objectives of the study. The interviewee thus becomes the primary source of information.2\nThis collection method offers flexibility. While there is a script or a list of specific topics to investigate, the interviewer may follow emerging topics or delve deeper into particular aspects according to the interviewee‚Äôs answers. The structure of this data collection technique can vary widely. At one end of the spectrum are highly structured interviews, which follow a defined sequence of topics and number of questions. At the other end are semi-structured interviews, where the moderator intuitively delves into the topics of interest.3 The research objectives and the type of data needed influence the structure of the interview.\n\n\n\n\n\n\nData from qualitative interviews\n\n\n\nQualitative interviews seek in-depth answers and the person‚Äôs interpretive perspective. In general, interviews have some advantages for certain profiles in contrast to focus groups. These include experts in different fields, populations that experienced traumatic events, persons deprived of liberty, geographically dispersed populations, and people with disabilities. Moreover, the types of data gathered through interviews can include:\n\nFeelings and Emotions: Understanding the emotional responses and feelings of the interviewee.\nOpinions and Perspectives: Gaining insights into the interviewee‚Äôs viewpoints and beliefs.\nExperiences and Narratives: Collecting detailed accounts of the interviewee‚Äôs personal experiences.\nKnowledge and Expertise: Gathering information from experts or individuals with specialized knowledge.\n\n\n\nIn-depth interviews help to learn about the interviewees‚Äô feelings, opinions, perspectives, beliefs, and experiences. The interviewees express themselves in their own words and in an active way, pointing out their point of view on the issues raised by the interviewer. This collection method works well for discussing complex or sensitive topics that some people may be reluctant to discuss in a group setting.4 Qualitative interviews provide unique insights from participants‚Äô experiences and knowledge, translated into detailed narratives and descriptions that reveal how people interpret their experiences and the world around them.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Interviews"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/interviews.html#planning-a-qualitative-interview",
    "href": "data-collection/qualitative-methods/interviews.html#planning-a-qualitative-interview",
    "title": "Qualitative Interviews",
    "section": "Planning a qualitative interview",
    "text": "Planning a qualitative interview\nProper preparation is essential for the success of an interview, ensuring that the research activity meets its field data collection objectives. This section provides detailed guidance on the planning steps that should be undertaken before conducting an interview.\n\n\nStructure your team\n\n\nDuring qualitative interviews, the fieldwork team includes a moderator and a facilitator. The facilitator‚Äôs participation is crucial to support logistical and note-taking processes. While budget restrictions may sometimes require the moderator to work individually. If you have a facilitator, it is important that you identify the functions they will perform.\n\n\n\n\n\n\nWhen not to have a facilitator\n\n\n\nWhen an interview involves sensitive or personal issues, such as when the interviewee is hesitant to share intimate details, the interviewer‚Äôs approach can significantly impact the conversation. For example, if a facilitator does not value the importance of mental health, this may become evident in how they take notes during an interview about traumatic experiences related to mental health.\n\n\n\n\nRecognize the skills of the field team\n\n\nThe quality of data produced during qualitative research activities relates to the skills of the moderator and facilitator.5 These competencies, outlined in Table¬†1, enable teams to recognize and adequately represent the diversity within qualitative data.6 While these skills are typically developed through years of study and practice, field teams often consist of individuals from multidisciplinary backgrounds. These individuals have varying levels of experience. Regardless of experience level, it is important to review and discuss these skills with your team to identify possible gaps and opportunities for improvement.\nCurrently, there are no standardized metrics to measure the prevalence of these skills among qualitative fieldwork moderators. Therefore, teams should view the skills in Table¬†1 as a resource for reference and reflection. Ensuring that everyone on the team understands how these competencies contribute to engaging with the population is essential for collecting high-quality information.\n\n\n\nTable¬†1: Skills of the field team conducting interviews\n\n\n\n\n\n\n\n\n\n\nSkill\nDescription\nWhy is this skill relevant?\n\n\n\n\nCognitive empathy\nThe field team‚Äôs ability to understand and communicate participants‚Äô situations from their perspectives, understanding how they see the world and their roles within it.\nAllows researchers to connect with participant‚Äôs realities and experiences. Helps to create a relationship of trust and respect with the participants. Seeks to avoid generalizations and stereotypes that may arise from preconceptions or external influences such as previous studies. Enhances understanding of participants‚Äô situations without resorting to pity.\n\n\nFollow-up\nThe field team‚Äôs ability to recognize when additional information is needed to answer the questions initially posed and those that arise during the research process. This ability implies curiosity and a willingness to explore new issues or doubts that emerge as data collection progresses.\nIncreases the quality and robustness of data by allowing a more detailed exploration of the studied phenomenon. Contributes to obtaining deeper responses from participants. Enables exploration of emerging themes during data collection. Helps detect and validate patterns observed in the field.\n\n\nSelf-awareness and reflexivity\nThe field team‚Äôs ability to continuously reflect on how their presence, background, and assumptions influence data collection, interpretation, and analysis. This ongoing self-reflection ensures that the qualitative field team is mindful of its impact on the research process and the participants.\nHelps maintain ethics in the researcher-participant relationship. Facilitates understanding of personal limitations in connecting with participants. Aids in developing strategies to overcome communication barriers and create an environment where participants feel comfortable sharing sensitive information.\n\n\nHeterogeneity*\nThe field team‚Äôs ability to represent and reflect the diversity within the group being studied. This skill involves recognizing and documenting the differences and variations among individuals or subgroups during qualitative research, typically applied during the data analysis phase.\nContributes to challenging generalized and simplistic patterns. Ensures that data reflect both common and atypical experiences. Demonstrates the field team‚Äôs ability to identify, recognize, and document heterogeneity in the population studied.\n\n\nPalpability\nThe field team‚Äôs ability to provide detailed descriptions in their field notes or diaries, making the data tangible and clear. This involves avoiding abstract descriptions and, instead, offering vivid accounts that allow the research team to visualize and understand participants‚Äô experiences and contexts.\nReliable findings are supported by specific details that depict the events and situations studied. Helps to avoid abstraction in the data, grounding conclusions in concrete evidence.\n\n\n\n\n\n\n\n\nPlan logistics activities\n\n\nLogistical activities play a crucial role in the success of interviews. Careful planning ensures that all necessary elements are in place to conduct interviews effectively. The following list presents general activities that should happen before implementing interviews. However, this list could be more comprehensive based on the specific research needs of the fieldwork. Ensure you cover all critical aspects to complete your research activities successfully. Based on experience in qualitative field operations across various contexts, the IPA team has identified key practices that guide research teams in effective planning:\n\n\n\n\n\n\nSchedule Participants\n\n\n\n\n\nSchedule interviews at least one week in advance. Send reminders before the activity.\nDuring scheduling, tell participants about the objective, scope, leading organization, and confidentiality of the interview to align expectations and avoid confusion.\n\n\n\n\n\n\n\n\n\nVerify the location of the Interview\n\n\n\n\n\nVerify the location and conditions of the meeting site. Remember that it is important to schedule a place whose location does not pose a risk to the participant or the team.\nThis place should have appropriate seating and furniture so the participant feels comfortable during the interview.\n\n\n\n\n\n\n\n\n\nPrepare essential materials\n\n\n\n\n\nEnsure you have all the necessary materials for the session. This includes incentives, refreshments/snacks, attendance sheet, recording equipment, and batteries.\nTest the audio equipment in advance to ensure clear recording of the interview.\n\n\n\n\n\n\n\n\n\nReview the strategies for taking notes\n\n\n\n\n\nHave a note-taking strategy to identify topics not recorded in the audio, capture nonverbal communication from the participant, and document the session in case the participant does not consent to recording. The facilitator supports this task.\n\n\n\n\n\nDesign and study the script\n\n\nUnderstanding the research questions is crucial for effective interviewing. The script is a guiding tool that helps keep the conversation within the research objectives. However, each dialog is unique, so it‚Äôs important to study the script to adapt to the conversation‚Äôs context without losing sight of the main topics.\nDuring the script preparation, you should identify the main questions to address the research objectives and strategically use follow-up and probing questions to delve deeper into relevant issues7:\n\nMain questions: These are the questions that guide the interview. They are defined beforehand and specified in the script.\nFollow-up questions: These are used to obtain more details and depth on specific topics, concepts, or events mentioned by the person being interviewed\n\nWhat do you think would happen if [situation]?\nCould you give an example of [topic]?\nHow did you decide to do it that way?\nHow did you determine what was important?\nHow did you conclude what you are telling me?\n\nProbing questions: They help validate what the participant is saying and also encourage the dynamic flow throughout the conversation.\n\nIs this what you said [statement]?\nDid I understand you when you said [statement]?\nWhat is another way you could [action]?\nDid I paraphrase accurately what you said?\n\n\nThis will allow you to obtain the essential information directly related to the study‚Äôs objectives. It also allows you to explore additional areas that can provide context and enrich the understanding of the data. It is also important to understand the research goals and main questions. It will allow you to go beyond the questionnaire and approach questions or dimensions of the problem that were not initially known.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Interviews"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/interviews.html#conducting-a-qualitative-interview",
    "href": "data-collection/qualitative-methods/interviews.html#conducting-a-qualitative-interview",
    "title": "Qualitative Interviews",
    "section": "Conducting a qualitative interview",
    "text": "Conducting a qualitative interview\nTo begin the interview, always start by introducing yourself and the objective of the session. Read the informed consent form. Ensure that the interviewee understands the purpose of the activity, its confidentiality, and the potential risks and benefits of participating in it. Use simple words that the interviewee understands.\nThen, emphasize the data security measures your team is taking to keep the information confidential. This will help the interviewee feel confident that they can discuss their experiences and will be heard.\nCreate a comfortable and calm atmosphere. Throughout the activity, it is essential to maintain this atmosphere that allows the interviewee‚Äôs opinions to be spontaneous.\n\n\n\n\n\n\nRole of the facilitator in this phase\n\n\n\n\nEnsure that recording equipment is working throughout the activity.\nAsk counter-questions to the interviewee.\nVerify that no relevant topics are left out.\nAssist if the interviewee‚Äôs emotions prevent them from continuing with the interview. For example, in actions such as offering a glass of water or proposing a pause.\nTake notes of the entire activity, especially the interviewee‚Äôs nonverbal reactions.\nSupport the moderator in time management.\n\n\n\n\nBeing an effective interviewer?\nThe interviewer guides the discussion and ensures the flow of the dialog with the interviewee while focusing on the relevant issues. An effective interviewer not only facilitates a natural discussion, but also creates an environment where the participant feels comfortable and safe expressing their opinions openly, without fear of criticism or judgment. Table¬†2 presents skills of effective qualitative interviewers.\n\n\n\nTable¬†2: Desirable skills of an interviewer\n\n\n\n\n\n\n\n\n\n\nSkill\nDescription\nWhy is this skill relevant?\n\n\n\n\nProvide for participants‚Äô well-being\nCreate safe and comfortable environment. Prioritize interviewee‚Äôs well-being, respect and value. Adapt environment and rhythm to reduce stress/discomfort.\nHelps establish trust and encourages open dialog. Shows respect for participant‚Äôs comfort and dignity. Body language that promotes trust is essential.\n\n\nBe empathetic\nExpress empathy and validate interviewee‚Äôs emotions to create atmosphere of openness and sincerity. Practice effective communication.\nCreates atmosphere of trust. Helps interview flow well. Express gratitude for participation. Match facial expressions and tone to emotions shared.\n\n\nUse few interruptions\nAvoid abrupt interruptions. Guide conversation back on track subtly when needed. Let interviewee establish narrative order.\nRespects participant as main source of information. Allows natural flow of conversation while maintaining focus. Can use techniques like paraphrasing to redirect gently.\n\n\nListen actively8\nShow full attention and interest. Consider both verbal and nonverbal communication. Ask thoughtful follow-up questions.\nDemonstrates respect and engagement. Helps ensure accurate understanding. Allows deeper exploration of topics through appropriate follow-up.\n\n\nAvoid value judgments\nMaintain neutral, inquiring stance. Don‚Äôt express agreement/disagreement that could influence responses.\nPrevents biasing responses. Keeps focus on participant‚Äôs perspective. Use neutral phrases to encourage elaboration.\n\n\nTrain your memory\nDevelop ability to remember key details for meaningful connections during conversation.\nEnables better follow-up questions. Helps maintain conversation flow. Allows tracking of important themes.\n\n\nUse silences constructively\nHandle silences constructively to allow reflection time. Avoid filling silence with filler words.\nGives participants time to think. Shows respect for responses. Allows natural pace of conversation.\n\n\n\n\n\n\n\n\nPotential challenges in a qualitative interview\nThere are situations that could take place during a qualitative interview that you should prepare for. Here are some examples:\n\n\n\n\n\n\nIncomplete, superficial or monosyllabic responses\n\n\n\n\n\nIn some interviews, particularly at the beginning, the interviewee may be shy or inhibited, which prevents them from providing detailed and in-depth answers to the questions posed. To identify an incomplete or superficial response, you can look for aspects such as:\n\nanswers that are too general and do not provide specific details\nanswers that are limited to one or a few words\nhaste in answering without going into detail.\n\nInvest time at the beginning of the activity to build rapport with the interviewee before addressing more in-depth questions. This initial trust may encourage fuller responses. You can use ‚Äúicebreaker‚Äù activities to do this. Ask open-ended questions that require a more elaborate response and prepare follow-up questions to provide more details on initial responses. Ask for clarification by using counter-questions like ‚ÄúHow did you feel about this?‚Äù or ‚ÄúWhat was your reaction when this happened?‚Äù to encourage more detailed answers. Explain how your answers make a valuable contribution to the study or project. Ask easy-to-understand questions in simple language and rephrase some if necessary.\n\n\n\n\n\n\n\n\n\nEmotional overflow from participants\n\n\n\n\n\nParticipants may experience intense emotions during an interview, which can be challenging to manage due to the personal nature of discussions. Be empathetic and avoid abrupt changes in discussion topics during emotionally charged moments. Allow participants to express their emotions while guiding the discussion constructively. Offer the option to pause or continue based on the participant‚Äôs comfort. If emotions are overwhelming, take a short break and show support. Provide relevant mental health care information per research protocols. Ensure the field team has training in psychological first aid if sensitive issues will be discussed.\n\n\n\n\n\n\n\n\n\nFatigue\n\n\n\n\n\nFatigue can affect both the interviewee and interviewer, leading to a loss of concentration, energy, and quality in responses. This may result in missed questions or premature assumptions. Limit the interview length and be flexible with scheduling if you notice signs of fatigue. Ensure the interview environment is comfortable, with good lighting and low noise. Schedule breaks during the interview if necessary, or continue at another time if needed. Consider active breaks or refreshments to help rejuvenate participants.\n\n\n\n\n\n\n\n\n\nDeviation from the central theme\n\n\n\n\n\nConversations may deviate from the interview‚Äôs objectives, requiring the moderator to tactfully redirect the discussion while balancing the exploration of valuable but off-topic insights. Direct the conversation subtly and without abruptly interrupting the interviewee to return to the main topic. Establish clear objectives at the beginning. Before starting the interview, explain the objectives and topics to be covered so that the interviewee is clear about the necessary focus.\n\n\n\n\n\n\n\n\n\nHostile interviewee\n\n\n\n\n\nAn interviewee may dominate the conversation with aggressive or derogatory language, creating a tense atmosphere. Prepare some key concepts about the topic that can help you respond confidently to the interviewee‚Äôs objections or challenges. In the event of verbal or other kind of aggression, end the interview, thanking the participant for their time.\n\n\n\n\n\nCommon interview mistakes\nWhen conducting qualitative interviews, avoid some mistakes so that the research activity, objectives, and the integrity of the person being interviewed are protected. Some mistakes are described here:\n\n\n\nCommon mistakes made when conducting interviews",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Interviews"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/interviews.html#after-the-interview",
    "href": "data-collection/qualitative-methods/interviews.html#after-the-interview",
    "title": "Qualitative Interviews",
    "section": "After the interview",
    "text": "After the interview\nThe interview concludes when the interviewer determines that all the topics outlined in the script have been covered or if the interviewee indicates that the session should end due to time constraints or emotional reasons. To wrap up the working session, it is recommended to:\n\nProvide space for reflection in which the interviewee can express comments, suggestions, or questions about the discussion\nExpress gratitude to the participants for their time and willingness to participate, emphasizing the importance of their opinions and the information they provided to the study\nSecure the data by prioritizing the safe storage of the recording and notes from the session.\n\nIt is also important to start working on the interview notes as soon as possible once it has been completed because if too much time passes between field activities and developing analysis, there is a risk of accumulating information and losing the opportunity to enrich the study research team insights.\n\nDocument Research Activities\nAt the end of the interview, it is important to document the information obtained in detail. How this documentation is done is a decision linked to the research design and researchers determine this before fieldwork begins. Examples of products you can use for this purpose are:\n\nFull transcripts of the discussions\nDetailed notes taken during the research activity\nField notes\nField diaries\n\n\n\n\n\n\n\nRemember\n\n\n\nThe lack of initial documentation products risks all information processing components and, thus, the quality of the data obtained.\n\n\nInformation should be recorded as soon as the interview concludes, to prevent the loss of crucial details. This immediate action is vital to the research process. In addition, the products derived from the focus group must follow information storage protocols, which may include anonymization and encryption to avoid compromising the confidentiality of participants.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Interviews"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/interviews.html#online-interviews",
    "href": "data-collection/qualitative-methods/interviews.html#online-interviews",
    "title": "Qualitative Interviews",
    "section": "Online interviews",
    "text": "Online interviews\nRemote interviews through a digital platform such as meets, Zoom, or telephone are an alternative when face-to-face interviews are not feasible due to distance, time, and budget constraints or when the interviewee prefers a remote setting. These interviews are beneficial for connecting participants from distant geographic locations or those who cannot attend in person, while also providing a safe and discrete environment. However, remote settings come with specific challenges that must be considered.\n\n\n\nTable¬†3: Considerations and challenges in virtual interviews\n\n\n\n\n\n\n\n\n\n\nChallenges\nDescription\nPossible solutions\n\n\n\n\nBuilding trust and empathy\nIn remote interviews it is often more difficult to establish trust.\nInvest time in the initial conversation to establish a personal connection. Be expressive and attentive to compensate for the lack of more subtle nonverbal cues. Avoid distractions that may be present. In the case of virtual interviews, ask the participant to use a camera during the activity.\n\n\nConnectivity problems\nConnectivity problems include situations that may cause people to interrupt their participation in the interview.\nDuring the scheduling, validate the participants‚Äô connection difficulties; if they require it, make internet recharges. Avoid that the research activity generates connectivity costs for the participants. If necessary, consider interviewing in person.\n\n\nFatigue due to constant use of screens\nFatigue associated with prolonged screen use or time on the phone can affect the concentration of both the interviewer and the interviewee.\nPlan breaks during lengthy interviews. Limit the duration of each session to a reasonable time, approximately one hour. Plan several sessions if necessary.\n\n\nUsers multitasking\nThe person being interviewed may perform other tasks while participating. This may affect their participation in the activity.\nSuggest that the interviewee have their camera turned on. During scheduling, remember that the interview is an activity that involves full attention to listening to others, and engagement with the questions.\n\n\nConfidentiality\nSome interviewees may carry out the activity in spaces shared with family members and acquaintances. Depending on the sensitivity of the topics, this situation may place participants in uncomfortable or risky situations.\nAdvise interviewees on how to select an ideal space to have the interview. Inform the participant when the recording starts, pauses or stops.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Interviews"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/interviews.html#a-practical-guide-for-conducting-interviews",
    "href": "data-collection/qualitative-methods/interviews.html#a-practical-guide-for-conducting-interviews",
    "title": "Qualitative Interviews",
    "section": "A Practical Guide for Conducting Interviews",
    "text": "A Practical Guide for Conducting Interviews\nThe content in this resource adapts from IPA Colombia‚Äôs ‚ÄúPractical Guide for Conducting Qualitative Interviews‚Äù. This practical guide provides some tools to conduct qualitative fieldwork through interviews. However, it is essential to note that data collection is one of the first steps in the process. After data collection, a necessary process of analyzing the information collected follows. This analysis allows the results to be interpreted and understood in depth, facilitating the identification of patterns, themes, and meanings crucial for qualitative research.\n\n\nUnable to display PDF file. Download instead.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Interviews"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/interviews.html#footnotes",
    "href": "data-collection/qualitative-methods/interviews.html#footnotes",
    "title": "Qualitative Interviews",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRuiz, A. (n.d.). Entrevista cualitativa: la conversaci√≥n como forma de acceso al conocimiento [Paper presentation]. II Jornada de Investigaci√≥n en Disciplines Art√≠sticas y Proyectuales. http://sedici.unlp.edu.ar/handle/10915/39236‚Ü©Ô∏é\nL√≥pez, R., & Deslauriers, J. P. (2011). The qualitative interview as a means of research in Social Work. M√°rgen, (61).‚Ü©Ô∏é\nRyan, F., Coughlan, M., & Cronin, P. (2009). Interviewing in qualitative research: The one-to-one interview. International Journal of Therapy and Rehabilitation, 16. https://doi.org/10.12968/ijtr.2009.16.6.42433‚Ü©Ô∏é\nMack, N., Woodsong, C., MacQueen, K. M., Guest, G., & Namey, E. (2005). Module 3 In-Depth Interviews. In Qualitative Research Methods: A Data Collector‚Äôs Field Guide (pp.¬†29-50).‚Ü©Ô∏é\nSmall, M. L., & Calarco, J. (2022). Qualitative Literacy: A guide to evaluating ethnography and interview research. University of California Press.‚Ü©Ô∏é\nSmall, M. L., & Calarco, J. (2022). Qualitative Literacy: A guide to evaluating ethnography and interview research. University of California Press.‚Ü©Ô∏é\nRubin, H. J., & Rubin, I. S. (2012). Structure of the responsive interview. In Qualitative Interviewing: The Art of Hearing Data (3rd ed., pp.¬†115-129). SAGE Publications.‚Ü©Ô∏é\nKvale, S. (2007). Conducting an interview. In Doing Interviews (pp.¬†52-66). SAGE Publications.‚Ü©Ô∏é",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Interviews"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/focus-groups.html",
    "href": "data-collection/qualitative-methods/focus-groups.html",
    "title": "Conducting Focus Groups",
    "section": "",
    "text": "Practical guidance for researchers and practitioners on how to plan and conduct focus groups for qualitative data collection in research and evaluation contexts. Covers team structure, moderator skills, logistics, script design, moderation techniques, and managing challenges in both in-person and virtual settings.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Focus Groups"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/focus-groups.html#what-is-a-focus-group",
    "href": "data-collection/qualitative-methods/focus-groups.html#what-is-a-focus-group",
    "title": "Conducting Focus Groups",
    "section": "What is a focus group?",
    "text": "What is a focus group?\nFocus groups are one of the most widely used techniques in social research, particularly within qualitative studies. This method involves creating a discussion space where participants engage in an in-depth conversation about one or more topics.1 Focus groups represent an artificially configured environment whose main goal is to gather information on the interaction among participants. This includes exploring their perceptions, opinions, and attitudes toward specific topics.2\nUnlike individual interviews that capture personal perspectives in isolation, focus groups reveal how opinions form, evolve, and develop through social interaction. They are particularly valuable when you need to understand community norms, examine areas of consensus or disagreement, or explore how people collectively make meaning of experiences.\n\n\n\n\n\n\nFocus groups with migrant populations\n\n\n\nIn a focus group with Venezuelan migrants in Colombia, several common perceptions about the healthcare system emerged. While some participants acknowledged the accessibility of essential medical services as an advantage of the Colombian system, others expressed concerns about the quality of care, long wait times, and challenges in accessing specialized services, particularly due to their migratory status. The group dynamics revealed how participants validated each other‚Äôs experiences, creating a more comprehensive picture than individual interviews might have provided.\n\n\nIn a typical focus group, a moderator leads the discussion. This person ensures active interaction among participants. The moderator‚Äôs role is to ask questions that stimulate discussion, guide the conversation to gather valuable insights, and build trust with the participants. This trust is crucial to making participants feel comfortable expressing themselves freely, ensuring the discussion remains productive and focused on relevant topics. Focus groups allow researchers to gather data on how participants communicate, share their opinions and experiences, and react to the contributions of others.\nFocus group dynamics allow researchers to identify what information participants censor or encourage among themselves.3 Group conversation does not necessarily imply consensus. During the conversation, participants may misinterpret each other‚Äôs accounts, question themselves, and attempt to persuade each other.4 Disagreement is also a key component of this collection technique, as it allows exploring the diversity of opinions and the reasons behind participants‚Äô beliefs and attitudes. Overall, focus groups make it possible to identify both shared perceptions and divergences among participants on a specific topic.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Focus Groups"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/focus-groups.html#planning-a-focus-group",
    "href": "data-collection/qualitative-methods/focus-groups.html#planning-a-focus-group",
    "title": "Conducting Focus Groups",
    "section": "Planning a focus group",
    "text": "Planning a focus group\nProper preparation is essential for the success of focus groups, ensuring that the research activity meets its field data collection objectives. This section provides detailed guidance on the planning steps that should be undertaken before conducting a focus group.\n\n\nStructure your team\n\n\nIdeally, focus group teams should consist of at least two roles: a moderator and a facilitator. The moderator‚Äôs primary responsibility is to lead the discussion during the focus group, ensuring that the conversation remains on track and that all participants engage actively. The facilitator handles logistical activities such as setting up recording equipment, providing refreshments and incentives, setting up the space, as well as, taking notes. It is crucial to clearly define and assign these roles in advance to ensure a smooth execution of the focus group.\n\n\n\n\n\n\nBudget vs.¬†quality trade-offs\n\n\n\nIf budget constraints prevent hiring a facilitator, it is important to recognize that this could negatively impact the quality of the activity and the data collected. Overburdening the moderator with both discussion and logistical duties may compromise their ability to maintain participant focus and engagement. Researchers should be aware of these potential drawbacks when planning qualitative data collection without a dedicated facilitator.\n\n\n\n\nRecognize the skills of the field team\n\n\nThe quality of data produced during qualitative research activities relates to the skills of the moderator and facilitator. These competencies, outlined in Table¬†1, enable teams to recognize and adequately represent the diversity within qualitative data. While these skills are typically developed through years of study and practice, field teams often consist of individuals from multidisciplinary backgrounds with varying levels of experience. Regardless of experience level, it is important to review and discuss these skills with your team to identify possible gaps and opportunities for improvement.\nCurrently, there are no standardized metrics to measure the prevalence of these skills among qualitative fieldwork moderators. Therefore, teams should view the skills in Table¬†1 as a resource for reference and reflection. Ensuring that everyone on the team understands how these competencies contribute to engaging with the population is essential for collecting high-quality information.\n\n\n\nTable¬†1: Skills of field staff conducting focus groups\n\n\n\n\n\n\n\n\n\n\nSkill\nDescription\nWhy is this skill relevant?\n\n\n\n\nCognitive empathy\nThe field team‚Äôs ability to understand and communicate participants‚Äô situations from their perspectives, understanding how they see the world and their roles within it.\nAllows researchers to connect with participant‚Äôs realities and experiences. Helps to create a relationship of trust and respect with the participants. Seeks to avoid generalizations and stereotypes that may arise from preconceptions or external influences such as previous studies. Enhances understanding of participants‚Äô situations without resorting to pity.\n\n\nFollow-up\nThe field team‚Äôs ability to recognize when additional information is needed to answer the questions initially posed and those that arise during the research process. This ability implies curiosity and a willingness to explore new issues or doubts that emerge as data collection progresses.\nIncreases the quality and robustness of data by allowing a more detailed exploration of the studied phenomenon. Contributes to obtaining deeper responses from participants. Enables exploration of emerging themes during data collection. Helps in detecting and validating patterns observed in the field.\n\n\nSelf-awareness and reflexivity\nThe field team‚Äôs ability to continuously reflect on how their presence, background, and assumptions influence data collection, interpretation, and analysis. This ongoing self-reflection ensures that the qualitative field team is mindful of its impact on the research process and the participants.\nHelps maintain ethics in the researcher-participant relationship. Facilitates understanding of personal limitations in connecting with participants. Aids in developing strategies to overcome communication barriers and create an environment where participants feel comfortable sharing sensitive information.\n\n\nHeterogeneity\nThe field team‚Äôs ability to represent and reflect the diversity within the group being studied. This skill involves recognizing and documenting the differences and variations among individuals or subgroups during qualitative research, typically applied during the data analysis phase.\nContributes to challenging generalized and simplistic patterns. Ensures that data reflect both common and atypical experiences. Demonstrates the field team‚Äôs ability to identify, recognize, and document heterogeneity in the population studied.\n\n\nPalpability\nThe field team‚Äôs ability to provide detailed descriptions in their field notes or diaries, making the data tangible and clear. This involves avoiding abstract descriptions and, instead, offering vivid accounts that allow the research team to visualize and understand participants‚Äô experiences and contexts.\nThe palpable field notes and diaries are accompanied by textual quotations, images, or other audiovisual resources that show events, situations, and actors that support the research findings. Reliable findings are supported by specific details that clearly depict the events and situations studied. Helps to avoid abstraction in the data, grounding conclusions in concrete evidence.\n\n\n\n\n\n\n\n\nPlan the logistics of your focus group\n\n\nLogistical activities are important to ensure the correct implementation of focus groups. Careful logistical planning ensures that all necessary elements are in place to carry out the activities, participants receive timely scheduling, and all fieldwork details remain clear. Based on the experience of conducting qualitative field operations in various contexts, the IPA Colombia team has identified several key practices that are essential for the logistical preparation of a focus group:\n\n\n\n\n\n\nSchedule Participants\n\n\n\n\n\nSchedule activities and participants at least one week in advance. Send reminders before the activity.\nDuring scheduling, tell participants about the objective, scope, leading organization, and confidentiality of the focus group to align expectations and avoid confusion.\n\n\n\n\n\n\n\n\n\nVerify the location of the focus group\n\n\n\n\n\nVerify the location and conditions of the meeting site. Remember that it is important to schedule a place whose location does not pose a risk to the participants or the team.\nThis place should have the necessary furniture (chairs and tables) so that participants feel comfortable and can interact with each other.\n\n\n\n\n\n\n\n\n\nPrepare essential materials\n\n\n\n\n\nEnsure you have all the necessary materials for the session. This includes incentives (if applicable), refreshments/snacks, attendance list, recording equipment, and batteries.\nTest the audio equipment in advance to ensure clear recording and facilitate the group dynamics.\n\n\n\n\n\n\n\n\n\nReview the strategies for taking notes\n\n\n\n\n\nHave a note-taking strategy to: identify topics not recorded in the audio, capture nonverbal interactions among participants, and document the session in case participants do not consent to recording. The facilitator supports this task.\n\n\n\nIt is worth noting that there is no particular order/progression to these activities. We suggest the sequencing presented above, though it may change according to the context in which you conduct the focus groups. The listed activities exclude logistical tasks that the field and research team had to undertake to ensure the feasibility of conducting research activities, such as guaranteeing a space, contacting leaders, purchasing materials, etc. You should make a comprehensive list of the critical aspects to ensure the success of your qualitative data collection at different levels. As fieldwork approaches, consult the issues previously mentioned.\n\n\nDesign and study the script\n\n\nThe discussion script is a support tool that seeks to guide the conversation during the focus group according to the research objectives. It contains open-ended questions and topics designed to encourage reflection and discussion. You should study the script several times before the activity so that you will be able to explore new topics without losing sight of the objectives of the session. During this process of studying the script, identify:\n\nThe objective of the research activity.\nThe logical order of the focus group guide.\nPossible words or expressions that may confuse the participants so that you have time to change them to more understandable ones.\n\nWhen you know the topics to explore in detail, the script serves as a reminder during the talk. This allows more flexible and deeper conversations while maintaining focus on research objectives.\n\n\n\n\n\n\nThe role of the facilitator\n\n\n\nIn addition to understanding the script, the facilitator must also understand the research questions of the project. This will allow them to probe beyond the guide and see, during the execution of the activity, possible questions or dimensions of the problem that were not known when the script was developed.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Focus Groups"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/focus-groups.html#conducting-a-focus-group",
    "href": "data-collection/qualitative-methods/focus-groups.html#conducting-a-focus-group",
    "title": "Conducting Focus Groups",
    "section": "Conducting a focus group",
    "text": "Conducting a focus group\nWhen initiating a focus group, always start by introducing yourself, explaining the role of each member of the work team, and the overall dynamics of the session. Read the informed consent, ensuring all participants clearly understand the purpose of the activity and the potential risks and benefits of their participation. During this process, emphasize the confidentiality and anonymity of the information collected during the focus group. The moderator should explicitly obtain the participants‚Äô authorization to record the session. Throughout the activity, create a comfortable and calm environment that allows participants to express their opinions freely and without censorship. To maintain a favorable atmosphere, show interest, cordiality, and respect for all interventions. Maintain an attitude of curiosity, even when topics extend beyond the planned scope or when opinions differ from yours.\n\nBeing an effective moderator\nTable¬†2 presents key principles to moderate focus groups. 5 The moderator plays a key role guiding the discussion, ensuring equal participation from all members, and keeping the conversation focused on relevant topics. An effective moderator facilitates a smooth and natural discussion, creating an environment where participants feel comfortable and safe to express their opinions openly, without fear of criticism or judgment. The moderator also manages potentially challenging group dynamics, ensuring that all participants have equal opportunities to contribute, and preventing any one participant from dominating the conversation. Other key responsibilities of an effective moderator include:\n\nEnsuring the recording equipment works throughout the activity.\nTaking comprehensive notes on the entire activity, particularly focusing on participants‚Äô interactions and nonverbal reactions.\nEnsuring that the seating arrangement of the participants is appropriate for effective communication.\nProviding logistical support to the moderator by delivering refreshments and incentives (if applicable) and attending to the specific needs of participants, such as assisting visually impaired individuals or accommodating participants with children.\nSupporting the moderator in managing the session‚Äôs timing.\n\n\n\n\nTable¬†2: Principles of focus group moderation\n\n\n\n\n\n\n\n\n\nActivity\nDescription\n\n\n\n\nShow interest in participants\nAs a moderator, show respect and genuine interest in each participant‚Äôs contributions. Recognize that everyone has valuable knowledge, regardless of their education level, experience or background. Acknowledge contributions with expressions such as ‚ÄúThank you for sharing that‚Äù or ‚ÄúThat is an interesting point.‚Äù Address participants by their first name or their chosen pseudonym.\n\n\nAvoid assumptions\nDo not assume that the concepts or ideas expressed by the participants are clear. Avoid taking for granted the meaning of any idea, concept, or expression. For example: In a focus group with Venezuelan migrants, the expression ‚Äúchamo‚Äù comes up. If you do not know this word, you should ask the participants directly what they are referring to with that word.\n\n\nGenerate empathy with participants\nGenerate an empathetic connection with focus group attendees. This helps them feel safe and valued when sharing emotions and experiences. A non-judgmental environment helps participants feel more comfortable speaking openly. Repeat or summarize participants‚Äô comments to show that you understand their perspectives. Be patient. Do not rush the answers. Allow participants to take their time to think and respond.\n\n\nBe a moderator, not a participant\nThe moderator‚Äôs role is to guide the conversation, not to be part of it or share personal views. Some moderators assume that sharing personal experiences will encourage a greater exchange of ideas among participants. However, this practice can induce bias and affect the conversation‚Äôs overall flow.\n\n\nBe prepared to listen to different opinions and control your reactions\nThe moderator is not impartial, no matter how hard they try. It is important to identify your own biases and seek to contain personal assessments of the various opinions that may come from the participants. Avoid making value judgments about the testimonies and responses of the attendees. Be mindful of your reactions. Even small responses like nodding or giving short positive feedback such as ‚ÄúI think so too‚Äù or ‚ÄúYou are right‚Äù can inadvertently influence the discussion. These reactions can induce social desirability bias, potentially altering the participants‚Äô responses and the overall dynamics of the discussion.\n\n\nUse your talents\nIdentify your talents and use those that build trust and encourage conversation among participants. Make a list of your skills and talents. Think about what you do well and how these talents can be useful in the context of a focus group.\n\n\nAsk follow-up questions\nAsking follow-up questions allows for a deeper exploration of participant‚Äôs responses. This is essential to understand not only what people think, but why they think the way they do.\n\n\n\n\n\n\n\n\nPotential challenges during a focus group\nThere are situations that could take place during a focus group that you should be prepared for. Here are some examples:\n\n\n\n\n\n\nEmotional overflow from participants\n\n\n\n\n\nManaging emotions can be one of the most complex challenges during a focus group, especially when discussions touch on intense or personal topics. Allow participants to express their emotions, but guide them constructively. Ask how they feel and why, and look for ways to channel those emotions into the discussion. If a person is emotionally affected by an event, ask them if they wish to continue the activity and remind them that participation is voluntary. If emotions disrupt the session, take a short break and show understanding and support. In accordance with research protocols, provide relevant mental health care resources that may be useful for the participants. If the focus group sessions address sensitive issues, the field team should have a course or training on psychological first aid.\n\n\n\n\n\n\n\n\n\nResistance to recording\n\n\n\n\n\nSome participants may not agree with the recording of their testimonies. If this happens, consider the following recommendations. If this is the case, emphasize confidentiality, anonymity, and privacy of the data. Remind participants that during the analysis of the information, no personal data of any participant will be added. If you do not obtain authorization to record the session, take notes on topics of conversation, interactions between participants, and other relevant data.\n\n\n\n\n\n\n\n\n\nConversation diversion\n\n\n\n\n\nWhile deviations from the central theme can sometimes provide valuable insights, steer the conversation back to the research objectives when it strays too far. During the participants‚Äô interventions, ask follow-up questions that allow you to close the ideas and/or redirect them to the research objectives.\n\n\n\n\n\n\n\n\n\nParticipants trying to dominate the conversation\n\n\n\n\n\nOccasionally, some participants may attempt to dominate the discussion, overshadowing more reserved participants. Set ground rules at the start of the session regarding the length of contributions. Identify strategies for asking participants to close their ideas when they are too long. For example, you can use an eye-catching object like a signaling paddle to get participants to finish their contributions.\n\n\n\n\n\n\n\n\n\nParticipants who are shy or reluctant to participate\n\n\n\n\n\nSome participants are too shy to participate or get pushed aside by those who seek to dominate the conversation. In these cases, you must control the situation so everyone can join the conversation. Use ‚Äúicebreaker‚Äù activities at the beginning of the session in which everyone participates. Directly address less active attendees with open-ended questions that encourage them to express their thoughts.\n\n\n\n\n\n\n\n\n\nPolarization of the conversation\n\n\n\n\n\nSome topics may have one side polarization on the group discussion, leading to an ‚Äúexcessive consensus‚Äù where some participants may hesitate to contradict the dominant view on certain topics. This can limit the diversity of perspectives and affect data quality. Emphasize that all viewpoints are valuable and that there are no right or wrong answers. Divide the group into smaller subgroups to discuss certain topics, then reconvene to share different perspectives. Limit speaking time to prevent any one participant from monopolizing the discussion. If the conversation becomes hostile, remind the group of the activity‚Äôs purpose and, if necessary, pause the session.\n\n\n\n\n\nFrequent mistakes during moderation\nWhen conducting focus groups, it is important to avoid common mistakes that can compromise the effectiveness of the session and the quality of the collected data. Some of the most critical mistakes are described here:\n\n\n\nFigure 1. Common mistakes made when conducting focus groups",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Focus Groups"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/focus-groups.html#after-the-focus-group",
    "href": "data-collection/qualitative-methods/focus-groups.html#after-the-focus-group",
    "title": "Conducting Focus Groups",
    "section": "After the focus group",
    "text": "After the focus group\nThe focus group session ends when all research topics have been covered and no new data or relevant information is obtained; the research objectives have been achieved, collecting the quantity and quality of data needed to answer the research questions, or when time and resources have reached their limits. To close the work session, it is recommended to (i) offer space for reflection in which participants can express comments, suggestions, or questions about what happened in the focus group; (ii) thank the participants for their time and willingness to participate, emphasizing that the opinions and information provided are of great importance for the study or research being conducted; and (iii) close the data flow, prioritizing the safe storage of the recording of the research activity. Some of the moderator‚Äôs responsibilities in this phase include:\n\nSupport data flow closure and secure storage of information.\nProvide the notes taken to the moderator.\nEnsure that all equipment used (tape recorders, computers, microphones, etc.) is complete.\n\nAt the end of the focus group, document the information obtained in detail. The lack of initial documentation products risks all information processing components and, thus, the quality of the obtained data. How this documentation is done is a decision linked to the research design and is determined before fieldwork begins. Record information as soon as the focus group concludes to prevent the loss of crucial details. This immediate action is vital to the research process. In addition, the products derived from the focus group must follow information storage protocols, which may include anonymization and encryption to avoid compromising the confidentiality of participants. Examples of products you can use for this purpose are:\n\nFull transcripts of the discussions\nDetailed notes taken during the research activity\nField notes\nField diaries",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Focus Groups"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/focus-groups.html#online-focus-groups",
    "href": "data-collection/qualitative-methods/focus-groups.html#online-focus-groups",
    "title": "Conducting Focus Groups",
    "section": "Online focus groups",
    "text": "Online focus groups\nVirtual focus groups are a viable alternative when in-person sessions are impossible due to factors like distance, time, or budget constraints. While the virtual format shares many characteristics with in-person focus groups, including preparation and moderation activities, it also presents unique challenges that require specific attention.\n\nPotential challenges of online focus groups\n\n\n\n\n\n\nLow interaction\n\n\n\n\n\nVirtual settings can reduce interaction due to participants‚Äô inability to read body language and challenges in responding in real time. Adapt icebreakers to virtual environments. Use activities that promote group dynamics and interaction. Encourage people to keep their cameras turned on. Try to schedule fewer participants to focus attention on the attendees. Manage short conversational scripts as virtual environments may limit direct communication, which could result in skipping sections and/or questions.\n\n\n\n\n\n\n\n\n\nConnectivity problems\n\n\n\n\n\nConnectivity issues may cause partial or complete interruptions in participants‚Äô involvement. Validate participants‚Äô internet connection before scheduling. Provide necessary support, such as data recharges, to ensure participation. Avoid imposing any costs on participants for participating.\n\n\n\n\n\n\n\n\n\nLack of knowledge of the video call platform\n\n\n\n\n\nSome participants may struggle with connecting to and navigating the chosen platform due to unfamiliarity with digital tools. During the scheduling, offer training on how to use the platform on which the activity will be carried out, for those who need assistance.\n\n\n\n\n\n\n\n\n\nLack of privacy\n\n\n\n\n\nSome participants may take the video call in shared spaces. This situation may place participants in uncomfortable or risky situations, depending on the sensitivity of the topics. During the scheduling, ask the participants directly where they will take the interview, who might listen to you, and if they feel safe having the interview under these conditions. Based on the above, assess whether certain people should participate in the study under these conditions. If people cannot find a private space and sensitive or confidential topics are handled, we recommend not including that participant in the focus group. Evaluate interviewing this person in a different setting.\n\n\n\n\n\n\n\n\n\nDistractions and lack of focus\n\n\n\n\n\nParticipants may become distracted by other duties or social media during the session, affecting their concentration and participation. Suggest that people have their cameras turned on. Emphasize the need for full attention and engagement during the session.\n\n\n\n\n\n\n\n\n\nFatigue due to constant use of screens\n\n\n\n\n\nProlonged screen time can lead to fatigue, affecting the focus and energy of both moderators and participants. Schedule breaks during long sessions. Limit the duration of each session to a reasonable time, approximately one hour. Plan multiple sessions if needed.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Focus Groups"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/focus-groups.html#a-practical-guide-for-conducting-focus-groups",
    "href": "data-collection/qualitative-methods/focus-groups.html#a-practical-guide-for-conducting-focus-groups",
    "title": "Conducting Focus Groups",
    "section": "A Practical Guide for Conducting Focus Groups",
    "text": "A Practical Guide for Conducting Focus Groups\nThe content in this resource comes from IPA Colombia‚Äôs ‚ÄúPractical Guide for Conducting Focus Groups‚Äù (see PDF document below). This practical guide provides an overview of how to conduct focus groups for qualitative data collection in the context of public policy design and evaluation of social programs. The guide includes a definition of this specific data collection technique, the purpose and benefits of using this technique, recommendations and steps-by-step instructions for implementation, and real-world applications.\n\n\nUnable to display PDF file. Download instead.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Focus Groups"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/focus-groups.html#footnotes",
    "href": "data-collection/qualitative-methods/focus-groups.html#footnotes",
    "title": "Conducting Focus Groups",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKitzinger, J. (1999). Qualitative Research: Introducing focus groups. BMJ, 311, 299-302. https://doi.org/10.1136/bmj.311.7000.299‚Ü©Ô∏é\nKitzinger, J. (1994). The methodology of focus groups: The importance of interaction between research participants. Sociology of Health and Illness, 16(1), 106.‚Ü©Ô∏é\nKitzinger, J. (1994). The methodology of focus groups: The importance of interaction between research participants. Sociology of Health and Illness, 16(1), 110.‚Ü©Ô∏é\nKitzinger, J. (1994). The methodology of focus groups: The importance of interaction between research participants. Sociology of Health and Illness, 16(1), 110.‚Ü©Ô∏é\nKrueger, R. A. (1998). Moderating focus groups. SAGE Publications.‚Ü©Ô∏é",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Focus Groups"
    ]
  },
  {
    "objectID": "data-collection/phone-surveys.html",
    "href": "data-collection/phone-surveys.html",
    "title": "Phone Surveys: Best Practices and Tools",
    "section": "",
    "text": "This guide provides best practices and tools for implementing phone surveys, a cornerstone of data collection for Innovations for Poverty Action. Voice and text-based phone surveys offer a cost-effective and adaptive method to gather high-quality data, especially in contexts where in-person surveys are impractical.",
    "crumbs": [
      "Data Collection",
      "Phone Surveys"
    ]
  },
  {
    "objectID": "data-collection/phone-surveys.html#methods-and-best-practices",
    "href": "data-collection/phone-surveys.html#methods-and-best-practices",
    "title": "Phone Surveys: Best Practices and Tools",
    "section": "1. Methods and Best Practices",
    "text": "1. Methods and Best Practices\n\nIntroduction\nPhone surveys are a cornerstone of IPA‚Äôs research methodology, offering cost-effective, scalable, and reliable data collection when in-person surveys face constraints. Phone surveys enable tailored data collection for real-time monitoring and impact evaluations, while maintaining ethical standards and operational continuity. Proven critical during COVID-19, they remain essential for hard-to-reach populations and time-sensitive research.\nThis page provides IPA teams and partners with:\n\nGeneral insights on the role and advantages of phone surveys in development research.\nAcademic research synthesizing evidence on best practices (e.g., optimizing response rates, minimizing attrition).\nPractical resources ‚Äì handbooks, SurveyCTO templates, and implementation guides ‚Äì to design and deploy effective phone surveys.\n\nWhether adapting an existing in-person survey to phone-based methods or launching a new remote data collection effort, the materials below offer evidence-backed guidance at every stage.\n\n\nWhat Are Phone Surveys?\nPhone surveys are a method of collecting quantitative or qualitative data by contacting respondents over the phone. Common approaches include:\n\nComputer-Assisted Telephone Interviewing (CATI): Live interviews conducted by trained enumerators.\nInteractive Voice Response (IVR): Automated surveys where respondents answer on a keypad or by voice.\nSMS/Text-based Surveys: Questionnaires delivered by text messaging.\n\n\n\nWhy Use Phone Surveys?\n\nAccessibility: Reach respondents in hard-to-access regions or during disruptions (e.g., pandemics).\nCost-Effectiveness: Lower logistical costs compared to in-person surveys.\nSpeed: Rapid deployment for time-sensitive data (e.g., crisis monitoring).\nSafety: Avoid physical contact during health emergencies.",
    "crumbs": [
      "Data Collection",
      "Phone Surveys"
    ]
  },
  {
    "objectID": "data-collection/phone-surveys.html#weighing-the-advantages-and-challenges",
    "href": "data-collection/phone-surveys.html#weighing-the-advantages-and-challenges",
    "title": "Phone Surveys: Best Practices and Tools",
    "section": "2. Weighing the Advantages and Challenges",
    "text": "2. Weighing the Advantages and Challenges\n\nAdvantages of Phone Surveys\n\n\n\n\n\n\nRandom Enumerator Assignment\n\n\n\n\n\nEnumerators can be assigned at random without geographic or travel constraints. Ensures fairness and reduces bias in data collection.\n\n\n\n\n\n\n\n\n\nReduced Logistical Constraints\n\n\n\n\n\nNo need for travel, reducing costs and time. Enumerators can work remotely, increasing flexibility.\n\n\n\n\n\n\n\n\n\nCost-Effectiveness\n\n\n\n\n\nLower operational costs compared to in-person surveys (e.g., no transportation or accommodation expenses). Scalable for large sample sizes.\n\n\n\n\n\n\n\n\n\nFaster Data Collection\n\n\n\n\n\nSurveys can be conducted quickly, especially with automated dialing systems. Real-time data entry reduces post-survey processing time.\n\n\n\n\n\n\n\n\n\nImproved Data Quality\n\n\n\n\n\nComputer-assisted systems minimize human errors in data entry. Built-in validation checks ensure accurate responses.\n\n\n\n\n\n\n\n\n\nWide Geographic Coverage\n\n\n\n\n\nAbility to reach respondents across large or remote areas without physical presence.\n\n\n\n\n\n\n\n\n\nFlexibility in Scheduling\n\n\n\n\n\nSurveys can be conducted at convenient times for respondents, improving response rates.\n\n\n\n\n\n\n\n\n\nEnhanced Monitoring\n\n\n\n\n\nSupervisors can monitor calls in real time, ensuring quality control. Automated systems track call outcomes (e.g., completed, refused, invalid).\n\n\n\n\n\nChallenges of Phone Surveys\n\n\n\n\n\n\nRespondent Tracking\n\n\n\n\n\nDifficulty maintaining contact with respondents over time. Higher attrition rates compared to in-person surveys.\n\n\n\n\n\n\n\n\n\nLanguage Barriers\n\n\n\n\n\nLimited availability of multilingual enumerators. Need for translation services increases costs.\n\n\n\n\n\n\n\n\n\nCall Refusals\n\n\n\n\n\nHigher refusal rates compared to in-person surveys. Respondent fatigue with repeated calls.\n\n\n\n\n\n\n\n\n\nTechnical Issues\n\n\n\n\n\nDependence on reliable phone networks and internet connectivity. Risk of dropped calls or poor audio quality.\n\n\n\n\n\n\n\n\n\nData Privacy Concerns\n\n\n\n\n\nChallenges in verifying respondent identity. Limited control over privacy during calls.\n\n\n\n\n\n\n\n\n\nLimited Non-Verbal Communication\n\n\n\n\n\nCannot observe body language or environmental context. More difficult to build rapport with respondents.\n\n\n\n\n\n\n\n\n\nCall Timing Constraints\n\n\n\n\n\nRestricted windows for successful contact. Need to balance persistence with respondent convenience.\n\n\n\n\n\n\n\n\n\nSurvey Design Limitations\n\n\n\n\n\nShorter attention spans for phone interviews. Complex questions may be harder to convey verbally.",
    "crumbs": [
      "Data Collection",
      "Phone Surveys"
    ]
  },
  {
    "objectID": "data-collection/phone-surveys.html#when-to-implement-phone-surveys",
    "href": "data-collection/phone-surveys.html#when-to-implement-phone-surveys",
    "title": "Phone Surveys: Best Practices and Tools",
    "section": "3. When to Implement Phone Surveys?",
    "text": "3. When to Implement Phone Surveys?\nAfter weighing the advantages and challenges, consider these key factors before implementing phone surveys:\n\nEssential Criteria\n\n\n\n\n\n\n\nCriterion\nKey Questions\n\n\n\n\nData Urgency\n- Is immediate data collection necessary?  - Can the research questions wait for in-person collection?\n\n\nSafety and Ethics\n- Can data be collected safely and ethically?  - Are there risks to respondents or enumerators?\n\n\nTechnical Feasibility\n- Do you have reliable phone numbers?  - Do respondents have consistent phone access?  - Is there adequate network coverage?\n\n\nResource Availability\n- Can you compensate respondents?  - Do you have budget for airtime and equipment?  - Are trained enumerators available?\n\n\nResearch Design Compatibility\n- Can your research questions be answered by phone?  - Is the target population reachable by phone?  - Are survey length and complexity suitable?\n\n\n\n\n\nChoosing an Appropriate Mode\nRecommended modes:\n\nCATI: Best for data quality but requires interviewers\nIVR: Lower cost but may frustrate respondents\nSMS: Limited by literacy and message length\nWeb: Less common in low-income countries\n\n\n\nResearch Goals by Mode\nSource: Remote Surveying in a Pandemic: Handbook\n\n\n\nGoal\nCATI\nIVR\nSMS\nWeb\n\n\n\n\nTracking respondents\n~\n+\n~\n-\n\n\nUpdating contact info\n+\n-\n~\n~\n\n\nDetermining language\n+\n+\n-\n+\n\n\nHigh-frequency data\n~\n+*\n+*\n+\n\n\nSensitive outcomes\n~\n~\n-\n~\n\n\nHigh response rates\n+\n-\n-\n-\n\n\nLarge samples\n-\n+\n~\n+",
    "crumbs": [
      "Data Collection",
      "Phone Surveys"
    ]
  },
  {
    "objectID": "data-collection/phone-surveys.html#resources",
    "href": "data-collection/phone-surveys.html#resources",
    "title": "Phone Surveys: Best Practices and Tools",
    "section": "4. Resources",
    "text": "4. Resources\n\nGuides and Templates\n\n\nRemote Surveying in a Pandemic: Handbook\nComprehensive guide to designing phone surveys during disruptions.\n\n\nUnable to display PDF file. Download instead.\n\n\n\n\nSurveyCTO Templates for Phone Surveys\nPre-built templates for phone surveys in SurveyCTO, including:\n\nCATI call tracking forms\nPhone number validation\nAppointment scheduling\nCall attempt logging\nMulti-language support\n\nView Templates on GitHub\n\n\nIPA RECOVR Phone Survey\nInsights on IPA‚Äôs phone survey initiative during COVID-19, including:\n\nSurvey implementation guide\nSample tracking protocols\nResponse rate optimization\nQuality control measures\nCross-country coordination\n\nView RECOVR Resources\n\n\nCATI SurveyCTO Plug-ins Webinar\nDiscover how J-PAL and IPA use SurveyCTO plug-ins for phone surveys, including:\n\nAutomated call scheduling\nMulti-language support\nCall tracking integration\nQuality monitoring tools\nCross-platform compatibility\n\nView Webinar Recording\n\n\nAcademic Research\n\n\nRemote Surveying in a Pandemic: Research Synthesis\nMeta-analysis of phone survey adaptations.\nDownload source data (Excel)\n\n\nPre-Survey SMS Contact\nEvidence on messaging to boost response rates.: Download PDF\n\n\nMonetary Incentives\nImpact of incentives on participation.: Download PDF\n\n\nMode Effects on Data Quality\nComparing phone vs.¬†in-person data accuracy.: Download PDF\n\n\nAttrition in Mobile Phone Panels\nStrategies to mitigate panel dropout.: Download PDF\n\n\nOptimal Timing for Random Digit Dialing\nMaximizing contact success rates.: Download PDF\n\n\nRepeated Attempts in Random Digit Dialing Surveys\nBest practices for rescheduling calls.: Download PDF\n\n\nBest Practices Summary\n\nPre-notification: Send SMS/WhatsApp alerts to improve response (WhatsApp Start Guide)\nIncentives: Small monetary rewards increase participation\nCall Scheduling: Prioritize evenings/weekends\nQuality Checks: Monitor for mode-related biases (Mode Effects)",
    "crumbs": [
      "Data Collection",
      "Phone Surveys"
    ]
  },
  {
    "objectID": "data-collection/phone-surveys.html#conclusion",
    "href": "data-collection/phone-surveys.html#conclusion",
    "title": "Phone Surveys: Best Practices and Tools",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nEffective phone surveys require careful planning, appropriate technology, and systematic quality control. Success depends on matching survey mode to research objectives, implementing robust tracking systems, and applying evidence-based practices for maximizing response rates and data quality.",
    "crumbs": [
      "Data Collection",
      "Phone Surveys"
    ]
  },
  {
    "objectID": "data-collection/in-person-surveys.html",
    "href": "data-collection/in-person-surveys.html",
    "title": "Research and Data Science Hub",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data-cleaning/variable-management.html",
    "href": "data-cleaning/variable-management.html",
    "title": "Variable Management",
    "section": "",
    "text": "Data cleaning practices to help you manage data and ensure data integrity and reproducibility. Covers handling missing values, categorical and dummy variables, specify variables, skip logic, storage types, date variables, and logic tests for survey data.\nRaw survey data is often relatively clean at the variable-level because survey software generates the data with various constraints on how the data are formatted. For example, multiple response variables only allow certain responses which the analyst knows from the survey instrument. These responses do not change in content unless the survey does. For other types of variables, most computer-assisted personal interviewing software such as SurveyCTO allow for constraints to be built into the survey instrument. IPA has several open-sourced tools to support data quality during data collection such as the data management system, which checks for data quality concerns and ipaclean, which provides a set of tools for data cleaning and validation.\nMuch of the cleaning work required for survey data includes standardizing variable formats to make them usable for analysis. This includes ensuring missing and non-response values appear as missing in statistical software, that categorical formats are useful, and variables correspond to the correct storage format, subjective orientation, and follow missingness patterns. These are substantive coding tasks and can benefit from consistent and automated approaches. This section is primarily concerned with doing those tasks safely and quickly with minimal manual input or transcription.",
    "crumbs": [
      "Data Cleaning",
      "Variable Management"
    ]
  },
  {
    "objectID": "data-cleaning/variable-management.html#handling-missing-values",
    "href": "data-cleaning/variable-management.html#handling-missing-values",
    "title": "Variable Management",
    "section": "Handling missing values",
    "text": "Handling missing values\nFor several reasons, variables will have missing values for certain observations. The data should reflect why these values are missing, particularly for numeric variables. For instance, you may want to know whether someone did not answer a question about business revenues because the question was skipped‚Äîthey were not business owners‚Äîor because they could not remember.\n\nMissing values on import\nMany statistical software will import variables and observations that only contain missing values when importing from other data formats. Often, clearing those values immediately after importing data is useful. This can be done manually, or through the missing command which can be installed through SSC. One approach is to do this is as follows:\n*Import data starting on the row of variable names: B1\nimport excel using \"${raw}/rawdata.xlsx\", cellrange(B1) first clear allstring\n\n*Remove rows and columns with only missing\nmissing dropobs, force\nmissing dropvars, force // usually don't use force, but fine in this case\n\n*Ensure non-missing IDs\nisid id\nSometimes missing values appear as non-empty values in other data management software (SQL uses NULL, R uses two types: NA with four subtypes and NaN, etc.). Check data to ensure those values are removed or properly converted to missing after importing.\n*Check variable names if anything equals \"NULL\"\nds, has(type string) // only search strings\nforeach var in `r(varlist)' {\n    replace `var' = \"\" if `var' == \"NULL\"\n}\nFor numeric variables, this can be done in one line using recode. For example, imagine all -99 values should be missing values:\n*Recode -99 to missing for all values\nrecode * (-99=.)\n\n\nExtended missing values in Stata\nStata has special codes for numeric missing values. For numeric variables, missing values are considered to be greater in value than all other numbers and themselves have an order of magnitude. The magnitude of missing values increases across the alphabet, with the standard missing value . coming before .a: . &lt; .a &lt; .b &lt; .c &lt; ‚Ä¶ &lt; .z The .a, .b, etc. are called ‚Äúextended missing values.‚Äù . Note that extended missing values cannot be stored in string variables. Instead, all string variables with a missing value are shown as \"\" (called ‚Äúblank‚Äù).\nEnsure extended missing values consistently represent missing values in your data. For instance, if -99 and -999 refer to ‚Äúdon‚Äôt know‚Äù in two different waves of the survey, they should be standardized. In that case, you will want to replace all -99 and -999 values with .d. It can be helpful to use the recode command to efficiently replace those values. The following standards for extended missing values are recommended:\n\n\n\nType of Missing\nExtended Missing\n\n\n\n\nDon‚Äôt know\n.d\n\n\nOther\n.o\n\n\nNot applicable\n.n\n\n\nRefusal\n.r\n\n\nSkip\n.s\n\n\nVersion differences\n.v\n\n\n\nSome responses may require the enumerator to switch from the standard missing values (for example if a response is restricted to be positive, -99 may not be allowed). Other times, enumerators may enter the wrong missing value by mistake, such as -999 instead of -99. As part of the code to relabel missing values by type, you can include code that searches for these changes. The following code replaces multiple values and looks for positive versions of the missing values in outliers.\n*Assign numerical codes\nloc idk     -99 99 999  // numerical code for \"Don't Know\"\nloc rf      -77 77 75   // numerical code for \"Refuse\"\nloc na      -88         // numerical code for \"Not Applicable\"\nloc oth     -66         // numerical code for \"Other\"\nloc skip    -70         // numerical code for  \"Skip\"\n\n* Replace values for each type of refusal across numerical variables\nqui ds, not(type string)\nlocal numvars `r(varlist)'\nforeach var of local numvars {\n\n    ** Replace missing values as negative for unlabeled numeric variable above 0\n    if mi(\"`: value label `var''\") { // no labels from SurveyCTO import code\n\n        *Skip if value takes less than 0 values\n        if `var' &lt; 0 continue\n\n        *now check if value has missing\n        qui sum `var' if inlist(`var', 99, 88, 77, 66)\n        if `r(N)' == 0 continue // move to next variable if no values have positive version\n\n        * only change if this is the outlier value conditional on previous being completed\n        foreach val of 99 88 77 66 {\n            di \"`var' has `r(N)' cases of `val'\"\n            qui sum `var'\n            qui replace `var' = -`val' if `var' == `val' & (`r(max)' == `val' | `r(min)' == `val')\n        }\n        // end foreach val of 99 88 77 66\n\n    }\n    // end if mi(\"`: value label `var''\")\n\n    ** Relabel based on missing patterns\n    foreach x of local idk {\n        replace `var' = .d if `var' == `x'      // Don't know\n    }\n    // end foreach x of local idk\n    foreach x of local na {\n        replace `var' = .n if `var' == `x'      // N/A\n    }\n    // end foreach x of local na\n    foreach x of local oth {\n        replace `var' = .o if `var' == `x'      // Other\n    }\n    // end foreach x of local oth\n    foreach x of local rf {\n        replace `var' = .r if `var' == `x'      // Refuse\n    }\n    // end foreach x of local rf\n    foreach x of local skip {\n        replace `var' = .s if `var' == `x'      // Skip\n    }\n    // end foreach x of local\n}\n// end foreach var of local numvars\nCheck outliers and replacements manually, and search for modal responses in numeric variables that will not have missing values as outliers. Accurate responses can overlap with missing response codes. Automated replacement code, such as the code above, should only be completed after confirming that all inverted values are correct. Even better, use of defensive coding such as assert or the pause command will resolve this. The best solution is often to make sure that the data generating process guards against this type of messiness. Programming in confirmation questions in the survey (‚ÄúDid the respondent really answer 99 or is this a missing value?‚Äù) can help accomplish this with more accuracy.",
    "crumbs": [
      "Data Cleaning",
      "Variable Management"
    ]
  },
  {
    "objectID": "data-cleaning/variable-management.html#categorical-and-dummy-variables",
    "href": "data-cleaning/variable-management.html#categorical-and-dummy-variables",
    "title": "Variable Management",
    "section": "Categorical and dummy variables",
    "text": "Categorical and dummy variables\nCategorical variables are ones that have no obvious ordering to the responses. For example, the question ‚ÄúWhat crop do you grow?‚Äù could have the following answers: soy bean, maize, cassava, ground nut, and yams. It can be helpful to have a numeric value attached to each response, however there is no clear ordering here. You can use the command encode to create a value for each in alphabetical order and it keeps the original response as the value label.\nDummy variables are one of the most common variable types you will use. These are also referred to as Boolean or indicator variables. These variables typically take on the values 0 and 1. However, it is important to note that your variable could and/or should have missing values if applicable. For example, an indicator about whether someone has a credit score could be defined as 1 for yes, 0 for no score and missing would mean there was no credit information available about that individual. Note that 0 and missing have different meanings and you should be careful around if and what values are missing. In Stata, you have several options for creating a dummy variable. Two examples of creating this ‚ÄúHas a credit score‚Äù dummy variable is below.\n\nThis first way recommended method is to start with an entirely missing variable and then replace with 1's and 0's for each condition.\n\ngen has_score = .\nreplace has_score = 1 if credit_score != .\nreplace has_score = 0 if (credit_score == . & info_available == 1)\nIt is important to note that 0 is defined for those missing a credit score but that credit information is available for, the observations that remain missing are those that are missing a credit score because there was no credit information available. You do not want to lose this information due to poor coding in which you say all these people have no score.\n\nThe second way is more concise but can be trickier.\n\ngen has_score = (credit_score != .) if (info_available == 1)\nThis method does the above in one step. The first half creates a 1/0 dummy by assigning 1 to any observation that meets the condition in the first parentheses and 0 if it does not. Then it uses the if and second parenthesis to assign missing values. If the second condition (info_available == 1) is false, then that observation will be missing. Either method is acceptable, just be careful to take the 0‚Äôs and missing values into account.",
    "crumbs": [
      "Data Cleaning",
      "Variable Management"
    ]
  },
  {
    "objectID": "data-cleaning/variable-management.html#specify-variables",
    "href": "data-cleaning/variable-management.html#specify-variables",
    "title": "Variable Management",
    "section": "‚ÄúSpecify‚Äù Variables",
    "text": "‚ÄúSpecify‚Äù Variables\nSurvey questions may have a ‚Äúspecify‚Äù option, in which the respondent explains their answer or gives an alternate answer not available on the list of choices for that question. These are often triggered by an ‚Äúother‚Äù response. In the raw dataset, alongside the original variable will be a ‚Äúspecify‚Äù variable that shows the comments for that question. You (or someone else familiar with the survey) will sometimes need to read through these specify answers and then recode the original variable accordingly. (‚ÄúSpecify‚Äù variables are also called other/specify variables, ‚Äúother‚Äù variables, and free-text variables.)\nFor example, if a question asked for someone‚Äôs favorite color, giving the options of blue, red, yellow, green, and other. If someone answered ‚Äúother‚Äù and then wrote ‚Äúsky blue‚Äù for their answer, you would want to recode the original variable for favorite color to say ‚Äúblue‚Äù instead of ‚Äúother.‚Äù However, if someone wrote ‚Äúpurple‚Äù you could leave their response as is (or, if enough people wrote purple, you could add another category to the variable).\nParticularly for large surveys, this can be a hassle. One helpful approach is to do the following:\n\nFirst clean each string variable so that similar answers will show the same value. Use string functions like lower(), trim(), and itrim() to convert answers like ‚ÄúPUR pLE‚Äù, ‚Äù Purple‚Äù and ‚Äúpurple‚Äù to all be ‚Äúpurple‚Äù.\nFor each specify variable, collapse the dataset into unique answers (for example, if three people wrote ‚Äúpurple‚Äù the collapsed dataset would only show ‚Äúpurple‚Äù once).\nStore those unique answers into a spreadsheet with another column that shows what variable the answer corresponds to. Then, leave one column blank, which you will eventually fill in with the value from the original variable that the response corresponds to (if it should be recoded). For instance, next to ‚Äúpurple‚Äù you would put nothing, but next to ‚Äúsky blue‚Äù you would put 1 if 1 corresponded to the ‚Äúblue‚Äù answer option.\nWrite code to merge the manual corrections from the excel file back into the do file, instead of running this as a series of replace or if `var' == \"sky blue\" | `var' == \"ocean blue\" | `var' == ... commands in the do file.\n\nMake sure you save do files and documents so that this process can be replicated and understood by someone else in the future.\nThis data flow would like the following. First, the specify responses are cleaned and saved so that the excel sheet can be modified.\n/* MR 10/25/2019:\n  The variable q_oth is the \"specify\" variable corresponding to\n  the variable q If q == 99, then q_oth has a string value input\n  by the enumerator.\n\n  First, I standardize strung responses of the q_other variable and\n  then output an excel document with each unique response.\n*/\n*String cleaning\ngen q_oth_cl = q_oth // create a clean copy to preserve the raw data\nreplace q_oth_cl = lower(q_oth_cl)\nreplace q_oth_cl = strtrim(q_oth_cl) // only trim external spaces so \"sky blue\" does not become \"skyblue\"\n\n*Save excel file\ntempvar map // create column header\ngen `map' = \"\"\nlab var `map'  \"Mapping\"\nlab var q_oth_cl \"Other values\"\n\n*Save to temporary file folder in the in the project folder\npreserve\n\n  keep q_oth_cl `map'\n  duplicates drop q_oth_cl, force\n  export excel q_oth_cl `map' using \"${temp}q_oth.xlsx\", firstrow(varl) replace\n\nrestore\nThis results in a table that looks like this:\n\n\n\nOther\nMapping\n\n\n\n\nsky blue\n1\n\n\nocean blue\n1\n\n\nnavy\n1\n\n\ndepends on the day\n-66\n\n\npurple\n\n\n\n\nThe RA would fill out the mapping column based on the allowed values in the survey. This means that the ‚ÄúMapping‚Äù column would take 1 for the ‚Äúsky blue‚Äù value if the data uses 1 for the survey. If the data uses string values at this point in the cleaning process the ‚ÄúMapping‚Äù column could be filled with ‚Äúblue.‚Äù Ensure that this process is reproducible and rule based. Inconsistent mapping of variables does not create clean data. After this table is completed, it can be merged back into the file to save the values.\nThe code to complete that looks like this:\n/* MR 10/25/19:\n  For question \"q\", specified other values were cleaned according to the\n  following rules:\n    -Any color response with more than 1% of the sample was\n     added as a category\n    -Specified colors that are a subset of the option (sample)\n    -Non-colors were replaced as missing\n      -Extended missing values were used if these should have been\n      an extended missing value captured by the survey.\n\n  These values were saved in a file in the Project Folder at:\n    ../08_Analysis&Results/01_Cleaning/05_Temp/q_oth_mapped.xlsx\n  They will then be merged in and replaced to the individual variables.\n*/\n*Load in data and save a tempfile\npreserve\n\n  import excel using \"${temp}q_oth_mapped.xlsx\", first clear\n  keep if !mi(Mapping) // only merge on mapped values\n  ren Othervalues q_oth_cl // change the file back to q_oth for the merge\n  tempfile q_oth_mapping\n  save `q_oth_mapping'\n\nrestore\n\n*Merge on file\nmmerge q_oth_cl using `q_oth_mapping', t(n:1) missing(nomatch)\n/*\n  Alternatively, merge to the subset of responses with non_missing values and\n  append these files afterwards using \"merge m:1 q_oth using `q_oth_mapping'\"\n*/\nassert _merge == -1 | _merge == 1 | _merge == 3 // missing, no coding, or coded\n\n*Replace values\nlevelsof q // first collect every level of q and replace\nloc levels `r(levels)'\nforeach level of local levels {\n  replace q = `level' if mapping == `level'\n}\n\n*Replace extended values\n// do this manually\n\n*replace missing values to IPA standard missing values\nreplace q = .d if mapping == -66 // don't know\nreplace q = .r if mapping == -77 // refusal\nreplace q = .n if mapping == -88 // not applicable\n\n*finally check that everything was captured\nassert q != 99 if _merge == 3 // this assumes 99 == other in the survey AND that the all values were coded\ndrop _merge q_oth_cl mapping // remove extraneous variable",
    "crumbs": [
      "Data Cleaning",
      "Variable Management"
    ]
  },
  {
    "objectID": "data-cleaning/variable-management.html#skip-logic",
    "href": "data-cleaning/variable-management.html#skip-logic",
    "title": "Variable Management",
    "section": "Skip Logic",
    "text": "Skip Logic\nSurveys will likely have skip logic. For instance, if a respondent says they have zero goats to a question, the survey may instruct the surveyor to (or the program may automatically) go to questions about sheep instead of questions about the quality of goat cheese the respondent makes from their goats. When a survey is bench tested and piloted, you should have tested that these skips worked by developing a series of checks based on a close reading of the survey instrument itself. However, skips may not be passed to the final dataset by some programs, such as SurveyCTO. This can make it hard to check if there were any errors in the survey coding.\nDefining additional missing values for skips and for questions that were not asked, such as those in long repeat groups can be helpful for two reasons:\n\nSkip values that take an extended missing values can be identified using ==.s without capturing other types of general missing values .s.\nExcluding skip values from the general missing value . can help to identify errors in cleaning later on, as Stata will not impute .s normally. Then, failures in skip logic can be identified as part of the cleaning process. If some of the skips did not work or allowed for some entry error among respondents, document the issues by outputting a list of the problematic observations into a spreadsheet and mention it to the PIs.\n\nThe following code assigns skip values and then confirms that the skips were successful during the survey implementation. It can be very helpful to use the assert command to check this. In addition, ensure that the observations who answered those questions are marked in the data by a dummy variable named in a consistent manner.\n/*\n    In this example, there is a module that asks about business profits\n    only if the respondent has a business. The question that starts\n    a set of questions, b_prof_s*, on business profits is b_prof_yn.\n    All questions should be skipped if  b_prof_yn == 0, but the\n    variables b_prof_s* exist if any respondent has a business.\n\n    First we assign the skip missing value to all observations if they\n    do not have a value. Then we run an assert to confirm skips worked\n    as intended. If they did not, the user is warned and\n    a dataset is saved.\n*/\n\n/* First identify if the respondent has a business and\nfill skip values */\nunab bus_items : b_prof_s* // save all business profits questions\nforeach var of local bus_items {\n    /* create skip patternm note that `var' == ., not mi(`var')\n    to ensure extended missing values are not overwritten */\n    replace `var' = .s if `var' == . & b_prof_yn == 0\n}\n\n\n** Now check to confirm that\nforeach var of local bus_items {\n\n    // don't use capture unless you control for every outcome\n    cap assert `var' == .s if b_prof_yn == 0\n\n    *Tag variables if this fails\n    if _rc == 9 gen `var'_nos = `var' != .s & b_prof_yn == 0\n\n    *Controlling for other options\n    else if !_rc di \"No errors in `var'\"\n\n    // exit with an error if a different error than the assert failing\n    else exit _rc\n}\n\n\n** Export a list of each variable and if it were skipped\n/* Formatting could be done differently here, the below\n   outputs an excel sheet that preserves all other answers\n   and is in the wide format.\n*/\npreserve\n\n    *Save ID and relevant variables\n    keep id key startdate b_prof*\n\n    *Keep relevant observations\n    qui ds b_prof_*_nos\n    egen tokeep = rowmax(`r(varlist)')\n    keep if tokeep == 1\n    drop tokeep\n\n    *Order by variable and missing\n    foreach var of local bus_items {\n        order `var' `var'_nos\n    }\n\n    *Save files\n    export excel using \"${temp}business_skip_errors.xlsx\", first(var) replace\n\nrestore",
    "crumbs": [
      "Data Cleaning",
      "Variable Management"
    ]
  },
  {
    "objectID": "data-cleaning/variable-management.html#storage-types-in-stata",
    "href": "data-cleaning/variable-management.html#storage-types-in-stata",
    "title": "Variable Management",
    "section": "Storage Types in Stata",
    "text": "Storage Types in Stata\nStatistical software requires a storage type to determine how and what data to store about each variable or value. This storage type determines if the software treats a variable as text or a number, and how much information is stored in each variable. This information could be how many digits of precision are required or if the variable is just 0 or 1s. Some data types such as dates have specific metadata attached ‚Äì January 1, 1960 was a Friday ‚Äì that relate to storage type.\nVariables are stored in two broad categories: string (text) or numeric. For tasks in analysis such as regression, Stata requires categorical variables to be stored as numeric variables, not string variables. This is also beneficial for storage size of variables. As a rule of thumb in Stata, ordinal and categorical variable should be stored as numeric variables with labeled values. Only text or IDs should ever be stored as a string variables. Labeling categorical variables is preferred and should be treated as a part of data management.\nStorage formats such string or numeric are the variable‚Äôs type, different from its format. Variable formats affect how Stata displays values of variables to the user and are loosely related to the storage type ‚Äì a string cannot be displayed with significant digits for example.\nNumeric variables are stored as byte, int, long, float or double. Float and double are the two that can hold non-integer numbers (decimals) and are the most common. More details on how numerical formats may affect datasets is available in this guide article. Numerical and string formats can be changed using the recast command, or by specifying a storage format using the generate command.\nString variables storage types are identified by their character length (str4 has 4 characters, str7 has 7 characters, etc.,). Variables are stored as string if they have any nonnumeric character in them (this includes commas and periods if they are imported as such). See help data_types for more information about variable types. Useful commands to go between string and numeric variables are destring and tostring. The command destring turns a variable from a string into a numeric (must contain all nonnumeric characters). The command tostring changes a numeric variable into a string variable. To convert strings to labeled numeric formats and vice versa see the encode and decode (or sencode and sdecode user-written commands).\nA variable‚Äôs format controls how the data is displayed. This is can be used to format numeric variables to display with commas or a specific number of decimal points. For details on the corresponding formats for each variable type and how to format variables, type help format. In short, it‚Äôs important to match the displayed format to the content, especially for outputs, so that content can be interpretable by humans.",
    "crumbs": [
      "Data Cleaning",
      "Variable Management"
    ]
  },
  {
    "objectID": "data-cleaning/variable-management.html#date-variables",
    "href": "data-cleaning/variable-management.html#date-variables",
    "title": "Variable Management",
    "section": "Date variables",
    "text": "Date variables\nDates are especially complex to work with in Stata. Stata stores dates as a numeric variable that captures either the number of days, months, quarters, or years since January 1, 1960. It is important to know that dates can also have a time component (datetimes), and are then stored as the number of milliseconds since January 1, 1960. These formats are numeric, but once they are stored as a date in Stata several special Stata functions can be applied to them to help with calculations that relate to dates and time. For more information see h datetime translation.\nSurvey CTO defaults to storing date variables as strings when importing them to Stata. It‚Äôs advantageous to convert these variables to a proper date format, as it allows for various logical and mathematical calculations. For example, you could count the number of days between a survey start date and submission date, or the number of minutes between the survey was started and completed. The SurveyCTO datetime metadata (starttime endtime submissiondate) are already stored as date variables.starttime and endtime are formatted as datetimes (%tc) and submissiondate is formatted as a date (%td)).\nTo convert imported dates to be stored as a date type in Stata, the functions date() and mdy() will convert non-date variables to date-formatted variables. The functions clock() and mdyhms() will convert non-date variables to datetime-formatted variables. The date() and clock() functions are useful if your dates are stored as a string variable whereas the mdy() and mdyhms() functions are useful if your dates are stored as numeric variables. These commands will create a variable that is the number of days, or the number of milliseconds, since January 1, 1960.\nFor a set of SurveyCTO datetimes using the same format, the following code will convert all of the variables to Stata date formats and check for data quality:\n*Define list of date variable\nds *_date // this should match your naming convention\nloc date_vars `r(varlist)'\n\n*save tempvar so indifferent to var length\ntempvar temp\n* create double to store datetime to avoid rounding\ngen double `temp' = .\n\n*Foreach date variable, convert to datetime and run checks\nforeach var of local date_vars {\n\n    *Display progress\n    di \"Working on `var'\"\n\n    *Skip if the date is already formatted as any date\n    /*\n        Note: This could also standardize formats using\n        the following code:\n\n        if regexm(\"`: format `v''\", \"%tc\") {\n             replace `v' = cofd(`v')\n             format `v' %tc_CCYY_NN_DD\n        }\n        else if regexm(\"`: format `var'', \"%t\") continue\n    */\n    if regexm(\"`: format `var''\", \"%t\") continue\n\n    *Convert from string to date\n    /*\n        Note: This specifies a format that all variables share.\n        See h datetime in Stata for an overview on how to define\n        datetime formats for the clock() and date() functions.\n    */\n    loc varl : var label `var' // save var label\n    replace `temp' = . // clear tempvar\n    replace `temp' = clock(`var', \"MDYhms\")\n    *drop the variable as cannot replace values and convert type\n    drop `var'\n    gen double `var' = `temp'\n    format `var' %tcCCYY_NN_DD__HH:MM:SS // choose a preferred format\n\n    *Check within range\n    loc survey_start = clock(\"Jan. 01 1960, 12:00:00 AM\", \"MDYhms\")\n    loc survey_end = clock(\"Feb. 29 2020, 11:59:59 PM\", \"MDYhms\")\n    assert inrange(`var', `survey_start', `survey_end')\n\n    *Check non-missing for calculate fields\n    /*\n        This may not be the case for all projects if some\n        calculate fields are conditional on being read\n    */\n    assert !mi(`var')\n\n}\n// end foreach var of local date_vars\nSome of these checks will duplicate checks conducted as part of IPA‚Äôs data management system. It doesn‚Äôt hurt to run these confirmations, but we recommend using the data management system to be aware of any problems as soon as possible.\nIf you would like to convert your daily variable into a monthly or yearly variable you can use the following functions: mofd() and yofd(). You can find descriptions of all the functions that convert between which count types for dates (daily, monthly, quarterly, or yearly) by typing help date() and clicking on ‚ÄúDate and time functions‚Äù to go to the Stata manual.\nOnce you have date in the right type, you can work with the formatting of the variable. The formatting is how you make the date look like a date rather than the underlying numeric value. You can read how to format your dates in any way imaginable by typing help datetime display formats. Note that changing the format of a date variable does not change its underlying value.",
    "crumbs": [
      "Data Cleaning",
      "Variable Management"
    ]
  },
  {
    "objectID": "data-cleaning/variable-management.html#logic-tests",
    "href": "data-cleaning/variable-management.html#logic-tests",
    "title": "Variable Management",
    "section": "Logic tests",
    "text": "Logic tests\nClean data should be free of mistakes such as out of range values. Yet, the thousands of lines of survey, database management, cleaning and analysis code that make up projects often contain a number of mistakes. These mistakes are a natural part of any large project. Instead of focusing on having mistake-free code in all cases, it‚Äôs often more effective to write code that tests for if the data matches expectations. This approach does not substitute for code review and being careful, but\nCommands like assert in Stata and stopifnot() in R allow the programmer to break the code if some logic test is not met. Logic tests check if the values observed in the data make sense given other values. For instance, can someone who says they were born in 2000 really have been a member of their current community for thirty years? These tests are used to ensure that the clean data is truly clean, not just that the programmer thinks the data are clean.\nThe list of logic tests to test is dependent on each individual dataset. Check for things that would be problematic for data analysis, such as missing values in variables you expect to always have a value. These checks should include other problems in the data that would clearly indicate that something is wrong with the data such as responses to a Likert scale survey question that wasn‚Äôt in the list of programmed responses. If any of those tests fail, flag the values in the data and bring them up with the PIs.\n\nDefensive Programming\nIn computer science, programming that ensures a program can function under unanticipated situations conditions is called defensive programming. It can be useful to bring this mentality into social science research. In many cases, defensive programming is natural with the type of data we work with. Many variables have characteristics that we expect from the data and can be easily checked. For example, for a survey of employed adults in the US, with a working age of 18, no respondents should have age below 18.\nWe can check that this is the case in Stata:\nassert age &gt;= 18 & !mi(age) // . is considered infinitely large by Stata, so &gt; 18 captures missing values as well.\nBuilding these tests into your data flow should be a natural test that you work into data cleaning. This can be expanded to other functions like merges. If every household but one received both an agricultural and household survey, each household survey should merge to the plot dataset except one observation. The merge can be checked to confirm that that is the case:\n*Check merge success rate\nmerge 1:1 id using \"plots.dta\" // note: merge has a built-in assert option\ncount if _merge != 3 // check how many merges did not have an observation in both\nassert `r(N)' == 1\nThese tests should be built into cleaning code, as they are computationally cheap unless data is large. They should be executed every time the code runs, and should test major modifications that affect important variables.\n\n\nLogic Tests for Survey Data\nSurvey data has the benefit of coming from a survey that you, as an analyst, have probably programmed. The survey software and import process often controls for many types of errors. However, data management errors can also occur due to the particular forms that survey data contains. These errors are common in survey data management:\n\nMissing values occur in variables that the programmer assumes always has real values\n\nMissing values are inconsistently defined in the survey and are not fully replaced (e.g.¬†‚ÄúNot Applicable‚Äù is -99, -98, and 77)\n\nVariable or value options change between survey versions and aren‚Äôt reconciled\n\nVariables with the same name are overwritten as part of a merge. See h mmerge for a user written command that stores this information.\n\nObservations are not unique and some actions such as sorting become irreproducible\n\nTesting for these errors as part of the cleaning process is strongly recommended.\n\n\nLogic Tests in Administrative Data\nThere are often broader concerns for administrative data since analysts do not directly control the data generation and management process. This means that data definitions and formats may change or be inconsistent over longitudinal data.\nSome additional concerns to focus on in administrative data are:\n\nEnsure that variables remain consistent over repeated deliveries\nData translation and missingness standards of the storage system may create values in statistical software (e.g.¬†SQL treats missing as ‚ÄúNULL‚Äù)\n\nVariable consistency can be handled using value labeling in Stata. For example, to ensure that data remains the same over the course of data collection, it can be useful to check that categorical variables map to expected values to a previously created value label. The noextend option of the encode commands allows you to confirm no values exist in the variable other than those in the supplied label. The following code accomplishes this:\n*Create a local list of variables to encode\nloc str_var var\n\n*Encode values and confirm expected\nforeach var of local str_var {\n\n    *Encode variables\n    sencode `var', label(`var'_label) replace noextend // type h sencode to see options\n}\n// end foreach v of local str_var",
    "crumbs": [
      "Data Cleaning",
      "Variable Management"
    ]
  },
  {
    "objectID": "data-cleaning/index.html",
    "href": "data-cleaning/index.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Best practices for data cleaning, focusing on manipulating raw data using Stata. Covers the essential steps from import to creating analysis-ready datasets.\nProper data cleaning is essential for analytical accuracy. Raw data requires modification before analysis. These modifications constitute cleaning. The cleaning process should always be reproducible, well documented, and defensive; the code should tell the user if the data is not as expected.\nThe IPA/GPRL Data Cleaning Guide presents a set of common tasks and provides information on ‚Äúwhat‚Äù each step is and ‚Äúhow‚Äù to complete each step in Stata. The guide also flags potential roadblocks during the data cleaning process. This guide assumes basic knowledge of Stata; introductory Stata training is available through IPA‚Äôs training repository.\nThis guide covers other parts of the data flow, including coding best practices, deidentification, and version control. In many ways these topics are distinct from data cleaning, but all interrelate to some extent. Effective data cleaning follows coding best practices, uses a version control system, and includes thorough documentation.",
    "crumbs": [
      "Data Cleaning",
      "About Data Cleaning"
    ]
  },
  {
    "objectID": "data-cleaning/index.html#data-flow",
    "href": "data-cleaning/index.html#data-flow",
    "title": "Data Cleaning",
    "section": "Data flow",
    "text": "Data flow\nAt a high level, data transitions from a format that reflects the collection structure to a format suitable for analysis. This transition occurs when data moves from generation‚Äîin surveys or automated banking systems‚Äîto analysis-ready formats. The contents of the data do not change during this process, but the storage, aggregation, and labeling formats do change.\nThis entire process constitutes a data flow. At GPRL and IPA, the data flow includes four key steps that take place in statistical software. Differences in the data may make it impossible to follow this order exactly. Deidentification should happen as soon as possible in the data flow if the data contains PII:\n\n\n\nData Flow Process\n\n\n\n\nImport data: Combine all collected data into a format readable by statistical software. This step imports raw data, applies corrections from enumerators, and removes duplicate observations.\nDeidentify data: Remove personally identifying information‚ÄîPII. This includes all individually identifying PII‚Äîgeographic information, names, addresses, enumerator comments, etc.‚Äîas well as group identifying information such as a combination of village and birthdate.\nClean data: Standardize data content, formats, and encoding. After this, verify data consistency and append similar datasets to create single datasets for outcome creation.\nCreate outcomes: Create individual outcome variables from the clean data. Merge and append data as part of this process to make a dataset at the level of analysis needed.",
    "crumbs": [
      "Data Cleaning",
      "About Data Cleaning"
    ]
  },
  {
    "objectID": "data-cleaning/index.html#data-cleaning",
    "href": "data-cleaning/index.html#data-cleaning",
    "title": "Data Cleaning",
    "section": "Data cleaning",
    "text": "Data cleaning\nAnalysts cannot use raw data directly for analysis. Individual survey items usually lack sufficient information on their own. Researchers must create outcome variables from standardized sets of variables. Additionally, analysts must add documentation so data users understand what each dataset contains.\nRaw data often needs corrections and deduplication that requires additional data from enumerators or respondents. Data collection for replacement forms part of the data collection process. Researchers often collect and make these replacements during monitoring. IPA and GPRL have produced many tools and resources to help this process. In particular, IPA‚Äôs Data Management System supports data quality monitoring, duplicate management, and corrections.\nAfter data reaches an importable format, the raw data will have its own idiosyncrasies. The cleaning process attempts to standardize these idiosyncrasies in a reproducible way. Consider three surveys, each with slightly different outputs. Cleaning makes the output from those datasets equivalent in format. The process applies standardized modifications to the content. The code that produces those data should run any number of times and should tell the user if something about the data has changed so that it cannot accomplish its function.\nThe cleaning process includes four rough stages:\n\nRaw Survey Data Management\nVariable Management\nDataset, Value, and Variable Documentation\nData Aggregation\n\nEach stage has a description in the guide, as well as a list of tasks with dedicated subpages. This guide also covers Stata coding practices relevant to this process and tasks related to outcome creation that require data management and are particularly prone to error in Stata.",
    "crumbs": [
      "Data Cleaning",
      "About Data Cleaning"
    ]
  },
  {
    "objectID": "data-cleaning/data-aggregation.html",
    "href": "data-cleaning/data-aggregation.html",
    "title": "Data Aggregation",
    "section": "",
    "text": "This guide explains two primary methods for combining datasets: appending datasets with the same variables but different observations, and merging datasets with the same observations but different variables. Learn best practices for exact matching and alternatives for fuzzy matching when precise identifiers are unavailable.",
    "crumbs": [
      "Data Cleaning",
      "Data Aggregation"
    ]
  },
  {
    "objectID": "data-cleaning/data-aggregation.html#when-to-use-each-method",
    "href": "data-cleaning/data-aggregation.html#when-to-use-each-method",
    "title": "Data Aggregation",
    "section": "When to Use Each Method",
    "text": "When to Use Each Method\nUse appending when: You have datasets with the same variables but different observations. Examples include combining adult and child survey data, or merging male and female participant files.\nUse merging when: You have datasets with the same observations but different variables. Examples include combining survey responses split across multiple files by the survey program.",
    "crumbs": [
      "Data Cleaning",
      "Data Aggregation"
    ]
  },
  {
    "objectID": "data-cleaning/data-aggregation.html#appending-data",
    "href": "data-cleaning/data-aggregation.html#appending-data",
    "title": "Data Aggregation",
    "section": "Appending Data",
    "text": "Appending Data\nAppending combines files that contain the same variables but different observations to create a complete dataset. When appending datasets, pay attention to variable names and types.\n\nKey Considerations for Appending\n\nVariable names must match exactly: only variables with identical names will combine successfully.\nMissing variables create gaps: if one dataset contains a variable that another lacks, the joint dataset will include that variable but observations from datasets without it will show as missing.\nVariable types must be compatible: variables should be the same general type, either numeric or string:\n\nMismatched types will generate errors and prompt you to use the force option\nThe force option is not recommended as it keeps the master data type and marks using observations as missing\nResolve type conflicts before appending\n\nString length handling: combining string variables of different lengths results in a string of the longer length.\nNumeric precision: when combining different numeric types, Stata keeps the more precise type and converts lower precision variables to higher precision.",
    "crumbs": [
      "Data Cleaning",
      "Data Aggregation"
    ]
  },
  {
    "objectID": "data-cleaning/data-aggregation.html#merging-data",
    "href": "data-cleaning/data-aggregation.html#merging-data",
    "title": "Data Aggregation",
    "section": "Merging Data",
    "text": "Merging Data\nMerging combines two or more datasets that contain the same observations but different variables. This situation often occurs when survey programs split variables across multiple datasets.\n\nPrerequisites for Merging\n\nEnsure both datasets have a unique ID\nSpecify the correct merge type: one-to-one or one-to-many\nCheck that datasets do not share variable names, except for ID variables\n\n\n\nHandling Variable Name Conflicts\nWhen datasets contain variables with identical names, Stata defaults to keeping the master data values. You can modify this behavior using update or replace options, but renaming one variable and keeping both is often preferable.\n\n\nMany-to-Many Merge Warning\nAvoid many-to-many merges as they represent poor practice. Many users expect this merge type to create all pair-wise combinations of matching observations, but it actually pairs datasets based on observation order within each ID. Use joinby if you need all pair-wise combinations.\n\n\nPost-Merge Validation\nAfter merging, validate results by checking the _merge variable. Each match type receives a numeric code. Use tab _merge to verify that results match expectations: number of matches, master-only observations, using-only observations, updated missing values, and conflicting nonmissing values.\nConsider adding assertions after merging to ensure correct execution. Alternative merge commands like safemerge and mmerge provide additional safety features.\n\n\nUseful Merge Options\nHelpful options include assert, keep, keepusing, gen, and nogen. Consult help merge for detailed explanations.\n\n\nAdditional Resources\nThe IPA Stata beginner‚Äôs training manual provides step-by-step merge guidance. The IPA high intermediate Stata training includes a module on merging with common pitfall discussions.",
    "crumbs": [
      "Data Cleaning",
      "Data Aggregation"
    ]
  },
  {
    "objectID": "data-cleaning/data-aggregation.html#fuzzy-matching",
    "href": "data-cleaning/data-aggregation.html#fuzzy-matching",
    "title": "Data Aggregation",
    "section": "Fuzzy Matching",
    "text": "Fuzzy Matching\nWhen datasets lack unique IDs, you must find alternative linking methods. String matching on names or other variables is common but often inaccurate due to misspellings. For machine-entered data, removing common mismatch sources like spaces and inconsistent capitalization may help. Manual matching works for small datasets, but large datasets often require fuzzy matching solutions.\nFuzzy matching finds approximately similar strings rather than exact matches. These algorithms provide match probabilities and only apply when exact matching is impossible. Consult with your manager before using fuzzy matching, as its match rate is typically lower than exact matching. Exact matching remains preferable when possible.\n\nFuzzy Matching Process\nFuzzy matching typically involves three steps:\n\nString cleaning: standardize spaces, capitalization, and remove special characters. This may include removing common phrases like titles in names.\nProbabilistic matching: algorithms estimate match probabilities between observations in different datasets.\nMatch review: review matches to determine the success threshold, then manually verify remaining matches.\n\n\n\nStata Commands for Fuzzy Matching\nSeveral user-written commands support fuzzy matching in Stata. Install them using ssc install [command]:\n\nreclink\nSpecify one or more variables for similarity assessment instead of a single unique ID. Numeric ages of twenty-five and twenty-six match more than thirty-five and sixty-five. String matching works even with imperfectly clean variables.\nKnown issues:\n\nThe idusing() variable should never exist in the master dataset\nreclink does not merge shared variable values from the using dataset without warning\nDatasets should not share variable names except for matching variables\nWhen idusing() and idmaster() use identical variable names, matches may fail\nConsider using temporary files when changing variable names for matching\n\nAlternative: reclink2 improves on reclink and adds many-to-one matching. Install with net install dm0082.\n\n\nmatchit\nMatches single variables to generate probability scores. Match multiple columns sequentially and average or weight probabilities for multi-column matching.\nFeatures multiple matching algorithms explained in the help file. All algorithms provide multiple likely matches for second-best guesses. Merge results using joinby or merge, but matchit does not function as a merge command.\n\n\nstrgroup\nInstall with net install strgroup. Calculates string differences and uses user-specified thresholds to create groups and matches.\nWorks within a single dataset. Data preparation typically involves joining a master variable to ID variables in the using dataset.\n\n\n\nAdditional Resources\nNumerous other commands and approaches exist for this problem. The economics literature has evaluated these methods for historical record linkage. Abramitzky et al. test multiple matching protocols and find that automated comparison techniques compare favorably to human matching in various circumstances. Stata resources accompany the linked paper.",
    "crumbs": [
      "Data Cleaning",
      "Data Aggregation"
    ]
  },
  {
    "objectID": "data-cleaning/dataset-documentation.html",
    "href": "data-cleaning/dataset-documentation.html",
    "title": "Dataset Documentation",
    "section": "",
    "text": "Standards for documenting datasets, focusing on Stata commands to name and label variables and attach notes to datasets in memory. Covers variable naming conventions, labeling systems, value labels, and metadata management for research data.",
    "crumbs": [
      "Data Cleaning",
      "Dataset Documentation"
    ]
  },
  {
    "objectID": "data-cleaning/dataset-documentation.html#features-of-well-documented-datasets",
    "href": "data-cleaning/dataset-documentation.html#features-of-well-documented-datasets",
    "title": "Dataset Documentation",
    "section": "Features of well-documented datasets",
    "text": "Features of well-documented datasets\nDocumented datasets in Stata should have the following characteristics:\n\nVariable names should follow patterns for both interpretability and ease in programming tasks.\nAll variables should have descriptive labels.\nAll values of categorical variables should have labels and be checked for consistency when you assign labels.\nAll datasets should only contain variables needed as part of the dataflow.\nDatasets should have internal notes describing additional information necessary to use the data, such as the name of the item in the questionnaire.\n\nWrite this documentation along with project manuals, codebooks, and readmes that describe other aspects of the data generating process. This includes why you made decisions to create variables, how datasets relate and change through the dataflow, and how you define variables used in the analysis.",
    "crumbs": [
      "Data Cleaning",
      "Dataset Documentation"
    ]
  },
  {
    "objectID": "data-cleaning/dataset-documentation.html#variable-names",
    "href": "data-cleaning/dataset-documentation.html#variable-names",
    "title": "Dataset Documentation",
    "section": "Variable names",
    "text": "Variable names\nVariable names often come from the prior method of data storage. These names may not work well for coding in statistical software due to length, formatting, or clarity. Standardizing these names to usable and interpretable formats is one of the first steps to ensure datasets are easy and intuitive to use.\nIf you use SurveyCTO data, the SurveyCTO import template automatically generates variable names and labels for you upon import. If you use other data or skip this template, you will need to rename your variables.\n\nNaming Standards\nVariable names should balance interpretability by users with the utilities that statistical software provides. For example, in Stata, several commands allow using wildcard commands such as * and ? to stand in for patterns in the data. This allows you to modify data systematically. If all income variables start with the inc_ prefix, you can modify every variable at once:\n*Call all income variables\nds inc_*\nBalance the following considerations while naming variables:\n\nGroup variable names that describe outcome categories. For example, prefix all variables that count yield with y_.\nGroup types of variables like comment fields with a unique substring such as _note.\nCreate names that have substantive meaning and are easy to type. For example, use inc_bus_ and inc_ag instead of section1b_ and section2a_.\nFor indicator variables, name the variable what the value ‚Äú1‚Äù indicates rather than the category. For example, if a variable takes 1 when a respondent is female and 0 when male, name the variable female not gender.\nCreate unique and consistent naming patterns across all datasets used in the project:\n\nTwo datasets with different units of analysis should not use the uninformative variable name id\nTwo datasets that describe income at various levels should use the same prefix to describe the same construct. For example, plot-level income could be inc_ag_plot1 and baseline household income could be inc_ag*_bl.\n\n\nIn wide data, it‚Äôs also important to consider how statistical software performs. Often times patterning variable names is necessary for tasks like reshaping to work smoothly. In wide-data stored in Stata, reshape uses a stub in the variable name to identify the value the long dataset would take for each group. Ensuring that variables are named consistently (e.g.¬†baseline variables are suffixed by _1, midline by _2, and endline by _3) can make it easier to reshape datasets.\n\n\nRenaming in Stata\nStata‚Äôs rename command is used to change variable names. While it is possible to rename multiple variables with these commands, it can often be easier to rename many variables from an external file such as an .xls. However, rename allows for some operators to rename multiple variables that share patterns. These commands can be very powerful and can sometimes capture variables that you do not intend to rename. See h rename group for a full description of renaming commands. Some options that are relevant for survey data follow:\n\n\n\n\n\n\n\n\nExtended command\nFunction\nExample\n\n\n\n\n*\nAny number of characters\nren year_* * removes the prefix year_ from all variable names that start with year_\n\n\n?\nExactly one character\nren monday_? day_?_1 would change monday_a to day_a_1\n\n\n#\nOne or more digit (numeric only)\nren age# age(##) renames all numeric variables to use a minimum of two digits for number suffixes (e.g.¬†age_1 becomes age_01)\n\n\n, renumber\nReorders names to increase by 1\nren survey_# survey_#, renumber reorders all variables prefixed with survey_ that end with a number so that they increase by 1\n\n\n\nAlso see renvars (net search renvars to install) a user written command that can help with complicated renaming tasks.\n\n\nRenaming from External Files\nIf you are renaming/labeling a lot of variables it can be cleaner to put them in an excel file and import from there, rather than writing it all in your do file. For an example of how to efficiently rename variables from a .xlsx file, see the following:\n**A. Import codebook file\n/*\nThis file contains the master name and labels, as well as the\nsurvey-wise name and labels\n*/\nimport excel \"${raw}/variable_codebook.xlsx\", firstrow clear\n\n\n** B. Make locals with common names and corresponding survey names\nsort common_varname // sort in unique order\n\n*Init project specific locals empty\nloc survey_names    // project-specific variable names\nloc common_names    // corresponding common variable names\n\n* Loop through all value of the excel file\nforvalues i = 1/`=_N' {\n\n    *Save name and labels in order from the excel sheet\n    loc survey_name = varname in `i'\n    loc common_name = common_varname in `i'\n    loc common_varl = common_varlab in `i'\n\n    *Fill locals to add information to project\n    loc proj_names `proj_names' `proj_name'\n    loc common_names `common_names' `common_name'\n    loc common_label \"`common_label' `\"`common_varl'\"'\"\n}\n// end forvalue i == 1/`N'\n\n\n**C. Run some checks\n*Check renaming lists are same length\nassert \"`:word count `proj_names''\" == \"`:word count `common_names''\"\n\n*Save list of locals for logc\nmacro list\n\n\n**D. IMPORT THE DATASETS IN A LOOP AND RENAME\n*Clean locals\nloc common_name // init empty\nloc common_varl // init empty\n\n*Load raw data\nuse \"``project'_directory'/`project'`input_dataset_suffix'.dta\", clear\n\n*Loop through variable names to remain\nforvalues i = 1(1)`: word count `proj_names'' {\n\n    *Collect names from list to rename variables\n    loc proj_name `:word `i' of `proj_names''\n    loc common_name `:word `i' of `common_names''\n    loc common_varl `:word `i' of `common_label''\n\n    *Rename and label from common names\n    rename `proj_name' `common_name'\n    lab var `common_name' \"`common_varl'\"\n}\n// end forvalues i = 1/`: word '",
    "crumbs": [
      "Data Cleaning",
      "Dataset Documentation"
    ]
  },
  {
    "objectID": "data-cleaning/dataset-documentation.html#variable-labels",
    "href": "data-cleaning/dataset-documentation.html#variable-labels",
    "title": "Dataset Documentation",
    "section": "Variable labels",
    "text": "Variable labels\nStata variables have both names and labels. Variable names are what Stata uses to define a column. Variable labels provide additional information that you can display to the analyst. Names should follow patterns that make programming easy. For example, you could code all consumption questions as cons_1 - cons_20 and call them with ds cons_? cons_??. Use variable labels as descriptors that say exactly what the variable is about. You can pull the exact question text from the survey, or use a paraphrased version if the text is lengthy.\n\nSystematizing labels\nVariable labels provide information about variable names that you often define for programmatic reasons.\n\nGive all variables labels, and give all multiple choice variables value labels.\nKeep the labeling system internally consistent.\nMake it easy to connect the variable in the dataset with the question on the questionnaire. Most analysis happens with the questionnaire in hand.\n\nOne format for defining variable labels for survey data includes both the question number in the questionnaire and a description of the contents in the variable. The basic format for that system is:\nVariable name: descriptive name that uses prefixes or suffixes to provide patterns\nVariable label: [question_number] descriptive label\nThis style is implemented below:\n*Define variable labels variables\nlabel var child_15      \"[QA.101] Has children under 15\"\nlabel var child_15B     \"[QA.102a] Number boys under 15\"\nlabel var child_15B_S       \"[QA.102b] Number boys in school\"\nlabel var child_15G     \"[QA.103a] Number girls under 15\"\nlabel var child_15G_S       \"[QA.103b] Number girls in school\"\nNote that this code aligns the variable names and the variable labels in the text. This makes it easy to read the labeling as a programmer.\n\n\nLabels from SurveyCTO\nSurveyCTO automatically labels variables using questions from the survey instrument. However, Stata allows labels a maximum of only 80 characters, which means SurveyCTO imports often truncate the labels. For tips on how to attach information longer than 80 characters see the variable notes guide.\n\n\nStata Storage of Variable Labels\nStata can use value label data using the extended macro functions (see h extended_fcn). The following code calls a variable label and assigns it to a local.\n*Call variable label of variable \"var\"\nlocal vlab : variable label var\nThis information can be searched conditionally. If, for example, you wanted to only apply a function to variables in the ‚ÄúQA‚Äù section of the survey defined in ‚ÄúSystematizing Labels‚Äù section in this article, you could check to see if the label starts with ‚Äú[QA.‚Äù:\n\nforeach var of varlist _all {\n    if regexm(\"`: variable label var'\", \"^\\[QA\") {\n        [do something]\n    }\n    // end if regexm(\"`: variable label var'\", \"^\\[QA\\.\") {\n}\n// end foreach var in `r(varlist)",
    "crumbs": [
      "Data Cleaning",
      "Dataset Documentation"
    ]
  },
  {
    "objectID": "data-cleaning/dataset-documentation.html#variable-metadata",
    "href": "data-cleaning/dataset-documentation.html#variable-metadata",
    "title": "Dataset Documentation",
    "section": "Variable metadata",
    "text": "Variable metadata\nSometimes you want to attach information or other labeling that is longer than Stata allows (labels are capped at 80 characters). If this is the case, you can store the full desired label into the variable notes or characteristics. Both notes and characteristics can describe variables or the dta file.\n\nNotes\nOne variable can have multiple notes. Add notes into variables by typing note VARIABLE : \"Note\". For example, for variable VARIABLE the note note was added. To display the notes stored in one variable just type ‚Äúnotes VARIABLE‚Äù. Stata also stores notes as locals and you can call them using `VARIABLE[note1]'. Stata numbers these notes based on the order in which it receives them. You can modify and delete note ordering with the note command (see help notes in Stata).\nSurvey CTO includes the full text of the question from the survey instrument as variable notes (as well as the truncated questions as variable labels) as part of the import do file. These notes will always be in the downloaded language. They will not contain filled values for the respondent that are produced as the result of calculate fields.\nIf you have changed or converted variable labels as part of a data transformation, you can convert notes into labels by looping through variables and using the stored local for notes:\n*Loop through each variable in the varlist VARIABLES\nforeach var of varlist VARIABLES {\n    label var `var' ``var'[note1]'\n}\n\n\nThe char command\nAdditional information can be added using characteristics, which function similarly to notes. The Stata manual describes characteristics as ‚Äúan arcane feature of Stata [that] are great use to Stata programmers.‚Äù Many commands use and define specific named characteristics to attach metadata. Characteristics (type help char in Stata) can describe variables and the dataset itself.\nThe main difference between characteristics and notes is that the char command requires a name for each characteristic. Whereas note VARIABLE : \"Note\" creates the next sequential note (1 if the first note, 2 if the second, etc.), char explicitly requires a character name, ‚Äúcharname‚Äù in the following code: char define VARIABLE[charname] \"Note\". This can be useful for saving labels in multiple languages. These characteristics can then be called by name, instead of an arbitrarily assigned number.\nFor example, a data flow could take labels in each language from a SurveyCTO form and assign them as characteristics to each variable produced by the survey in the following:\n*Import SurveyCTO\nimport excel using \"Baseline Household Survey.xlsx\", first clear\n\n*Keep variables with labels\nkeep type name label labelbangla relevance\nren label labelenglish\nren label* * // rename to remove \"label_\" prefix from all variables\n\n*Remove variables not exported to Stata\ndrop if inlist(type, \"begin group\",  ///\n  \"end group\", \"image\", \"begin repeat\", \"end repeat\")\ndrop type\n\n*Only keep variables with non_missing\nds name, not // get list of label variables\nloc languages `r(varlist)' relevance\negen has_lab = rownonmiss(`languages'), strok\n// keep only rows with has_lab\nkeep if has_lab &gt;= 1 & !mi(has_lab)\n\n*Save variables and language names\nloc varnames // init empty\nforval i = 1(1)`=_N' {\n  loc name = name[`i'] // save name of variable\n\n  *Save labels as locals\n  loc j = 1 // init counter at start\n  foreach language of local languages {\n     loc `name'_`j' = `language'[`i']\n     loc ++j\n  }\n  // end foreach language of local languages\n\n  *save local of names to add question text to\n  loc varnames `varnames' `name'\n}\n// end forval i = 1(1)_N\n\n*Load data\nuse survey.dta, clear\n\n*Loop through names to add characteristics\nforeach name of local varnames {\n  ds `name'* // collect names that are inclusive of repeat groups\n  loc varl `r(varlist)'\n\n  foreach var of local varl {\n\n  *Confirm only the variable or the repeat group\n  cap assert \"`var'\" == \"`name'\" | regexm(\"`var'\",\"^`name'[0-9][0-9]?$\")\n  if _rc continue // skip if \"`name'\" is a prefix\n\n     *add characteristic as a named language\n     loc j = 1\n     foreach language of local languages {\n        char define `var'[`language'] \"``name'_`j''\"\n        loc ++j\n     }\n     // end foreach language of local languages\n\n  }\n  // end foreach var of local varl\n\n}\n// end foreach name of local names",
    "crumbs": [
      "Data Cleaning",
      "Dataset Documentation"
    ]
  },
  {
    "objectID": "data-cleaning/dataset-documentation.html#value-labels",
    "href": "data-cleaning/dataset-documentation.html#value-labels",
    "title": "Dataset Documentation",
    "section": "Value labels",
    "text": "Value labels\nFor categorical variables the raw data often shows the string values for the selected response. For instance, you may see ‚Äúmale‚Äù and ‚Äúfemale‚Äù as possible responses to the variable gender. When doing calculations, however, you need these variables to be numeric (in the float or long format) if they are not already imported this way. Preserve the extra information the strings capture by using ‚Äúvalue labels.‚Äù A value label such as gender would assign ‚Äúfemale‚Äù to 0 and ‚Äúmale‚Äù to 1 and display female and male to the analyst. See help label for how to do this in Stata. Label values for two important reasons:\n\nit provides information to the analyst that reduces mistakes in coding or analyzing data\nmany programs use information on whether a variable has value labels to identify it as a categorical variable, as opposed to a continuous numeric variable.\n\n\nEncoding String Values in Stata\nThe quickest way to change string variables to numeric variables with value labels is the encode command. encode automatically converts the string variable into a numeric variable and assigns the numbers 1-x (where x is the number of unique answer choices) to the alphabetized list of the answer choices (ordered 0-9, followed by a-z). Because this happens automatically based on alphabetical order, you may need to recode or label them manually if you want value labels to match some existing assignment.\nStata stores value labels independently from the variables, so manage value labels separately from variables as they can contain PII. Deleting all variables that have a value label and saving the dataset removes the value label from the .dta file. To see which labels Stata currently defines and their content, use the label list command. The return values of label list and label dir also store helpful summary information. You can modify or delete these labels to combine using the options of label define function:\n*Drop old labels\nlabel drop ex1 ex2 ex3\n\n*Define label\nlabel define yesno 1 \"No\" 3 \"Yes\"\nlabel list yesno\n\n*Modify label to correct the error\nlabel define yesno 1 \"No\" 2 \"Yes\", modify\n\n*Add extended values to the label defined above\nlabel define yesno .n \"No response\" 3 \"Maybe\", add\n\n*Apply the label to all of the variables it should apply to\nloc dummy_vars ex1 ex2 ex3\nlabel values `dummy_vars' yesno\n\n\nFormatting Labeling in Stata\nIt can be useful to change the delimiter to a semicolon so that a single command can take up several rows in your text editor, making it easier to read labeling. This is especially useful when multiple values are labeled. See help delimit to learn about delimiters in Stata. An example would be:\n* Set delimiter for labeling\n#delimit ;\n\nlabel def female\n    0 \"[0] Male\"\n    1 \"[1] Female\"\n;\n\nlabel def region\n    1 \"[1] Northern\"\n    2 \"[2] Southern\"\n    3 \"[3] Western\"\n    4 \"[4] Eastern\"\n    5 \"[5] Central\"\n;\n\n#delimit cr\n\nlabel values female female\nlabel values region region\nNote how the labels have the corresponding value as well as the description in the value label. This is not strictly necessary, but can be useful if you want values to display alongside labels in outputs.\n\n\nDefensive Workflow for Encoding Values\nOne way to ensure that data is encoded in an expected way is to check that values are only encoded from a pre-specified list of value labels that you defined. The user written command sencode (install using ssc install sencode) can help support this. sencode labels the variable according to the values that you‚Äôve predefined and then adds additional values in order from the highest value if it encounters values that you haven‚Äôt defined. An example data flow follows:\n*Ensure sencode is installed\ncap which sencode\nif _rc ssc install sencode\n\n*Load data\n    sysuse auto, clear\nkeep if _n &lt;= 10 // Keep the first ten observations of the sample\n\n# del ;\n\n/*\nThis label is named by the variable name, an \"_\", and then \"label\"\nso that we can loop over the labels.\n*/\nlabel define make_label\n    1   \"AMC Concord\"\n    2   \"AMC Pacer\"\n    3   \"AMC Spirit\"\n    4   \"Buick Century\"\n    5   \"Buick Electra\"\n    6   \"Buick LeSabre\"\n    7   \"Buick Opel\"\n    8   \"Buick Regal\"\n    9   \"Buick Riviera\"\n    10  \"Buick Skylark\"\n;\n\n# del cr\n\n*Create a local list of variables to encode\nloc str_var make\n\n*Encode values and confirm expected\nforeach var of local str_var {\n\n    *Encode variables\n    /*\n    type h sencode to see options, the noextend option returns an error\n    if the existing label doesn't capture all values.\n    */\n    sencode `var', label(`var'_label) replace noextend\n}\n// end foreach v of local str_var\n\n*Display the labeled and unlabeled values\ntab make\ntab make, nol",
    "crumbs": [
      "Data Cleaning",
      "Dataset Documentation"
    ]
  },
  {
    "objectID": "data-cleaning/dataset-documentation.html#dataset-management",
    "href": "data-cleaning/dataset-documentation.html#dataset-management",
    "title": "Dataset Documentation",
    "section": "Dataset Management",
    "text": "Dataset Management\nKeep only necessary variables. Order those variables in an understandable way, and name and label them. Also put them in the correct storage format for analysis. The clearest way to do this may vary, especially with variable order. The order that questions appear in the survey works well. Always put unique identifiers first.\nAny script that saves data should have code that identifies the variables saved, orders them, and describes them for readers. This ensures that a reader can look at the code and understand what it produces without running a do file. An example codeblock for the end of a do file follows. Note that comments describe values and the file ends with some commented marker.\n**B. Sort and clean vars\n    isid hhid // confirm Household ID is unique\n    sort hhid // sort in a unique order\n\n    *Create a local of variables\n    loc vars                        ///\n    hhid        enum_id             /// ID Variables\n    cluster     survey_date form_id /// File source variables\n    treatment   scto_rand           /// Treatment assignment\n    bl_hhh_age  bl_hhh_female   bl_hhh_educ /// Baseline demos\n    bl_hh_size                      ///\n    bl_cons_veg_*   bl_cons_meat_*  bl_cons_purch_* /// Consumption\n    bl_cons_alc                     ///\n    bl_loan_size    bl_loan_exp_pay_m*  bl_loan_miss_m* /// Loan information\n    bl_msf      bl_otaf                 // Lender Fees\n\n    *Keep necessary values\n    qui ds `vars', not\n    assert `: word count `r(varlist)'' == 0 // check no variables dropped\n    keep `vars'\n\n    *Order ID first\n    order `vars'\n\n\n**C. Save and close\n    *Save data to the data folder\n    save \"${data}01a_baseline.dta\", replace\n\n    *Close the log\n    log c\n\n\n**EOF**",
    "crumbs": [
      "Data Cleaning",
      "Dataset Documentation"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html",
    "href": "data-cleaning/raw-data-management.html",
    "title": "Raw Data Management",
    "section": "",
    "text": "Best practices for importing and manipulating raw data, covering data structures, unique identifiers, PII handling, and file format conversions for both survey data and administrative data.\nWhen we refer to ‚ÄúRaw data‚Äù we mean data in it‚Äôs original form when initially received from the data source. Raw data come in the form of the data collection instrument used to generate the data, be it a survey form, administrative database, computer logs, a customer relationship management system, or other source. These formats usually result from the form best used to capture the data and not to process it. Format conversion from the source format to one usable by statistical software often requires changing file formats, changing data formats, and general error correction. This section of the cleaning guide covers that process, primarily focused on getting data from SurveyCTO to a preferred structure in Stata.",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html#survey-data",
    "href": "data-cleaning/raw-data-management.html#survey-data",
    "title": "Raw Data Management",
    "section": "Survey data",
    "text": "Survey data\nMost IPA research projects include survey data. Survey data has the advantage of a consistent format and a known structure. SurveyCTO data, especially, has tools that aid import into Stata. Deciding the data structure during survey programming provides important benefits for raw data management.\nSurvey data‚Äôs structure also carries unique challenges for data management: analysts must standardize formats of survey data and responses for analysis, add metadata to surveys, and complete corrections. Finally, since survey data often holds many sources of personally identifying information in the raw format, deidentification is often important and requires specific focus. Changes in survey version content may exacerbate this process and require multiple imports. This section provides information on these tasks. This section does not provide information on how to program surveys to make data management easy.\nIPA has developed tools to support survey implementation and data quality assurance. IPA‚Äôs data management system (DMS) provides Excel and Stata-based resources to handle data quality checks. All IPA projects require the DMS. It provides information on data quality and will resolve duplicates before data cleaning begins, among many other features.",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html#administrative-data",
    "href": "data-cleaning/raw-data-management.html#administrative-data",
    "title": "Raw Data Management",
    "section": "Administrative data",
    "text": "Administrative data\nAdministrative data often undergoes preprocessing for functional reasons. Working with SQL databases and other data management software provides additional challenges as data fields may change definitions over time and data structure may not be easily amenable to transfer or import into statistical software such as Stata or R.\nNo single process fits all administrative data management needs. Automating importing and restructuring, as well as checking data quality, are common analyst challenges before traditional cleaning begins. This section provides some information on how to handle these tasks in automatable fashions, as well as ways to think about data storage and unique identification. Other management tasks can be found in the data aggregation section.\nLarger datasets amplify these problems. Big data requires special data management techniques when data size reaches limitations in statistical software capabilities, processing power, and storage format. Excel can only store about 1 million observations in .xlsx and 65,000 using .xls. This guide does not cover these challenges in detail, but many resources exist for processing big data in Stata‚Äîsee NBER and Statalist for tips.",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html#importing-into-stata",
    "href": "data-cleaning/raw-data-management.html#importing-into-stata",
    "title": "Raw Data Management",
    "section": "Importing into Stata",
    "text": "Importing into Stata\nData comes in many forms, from raw text (.txt) files to multi-sheet Excel (.xls and .xlsx) files. Importing data into Stata is necessary if the data is not already in Stata format (.dta file). In general, you should be able to use Stata‚Äôs functions and loops to efficiently import data. Stata can import most data formats. If your project‚Äôs data was collected using a platform like SurveyCTO, the raw data will come in .csv format and the SurveyCTO server will provide a do file that imports the raw .csv data into the Stata .dta format.\n\nImporting different file types\nIn Stata 13 and beyond, the import command can import CSV files, excel files, and more depending on the option used (delimited for CSV, excel for excel), and export does the same for exporting. The import excel and import delimited commands provide a number of options that allow for a large amount of control of importing including from where in the workbook data should be imported from and how data should be saved. See help import for details on these options.\nIf you are new to using import are importing a file type you have not seen before, it can be helpful to use the drop-down menu by clicking ‚ÄúFile&gt;Import‚Äù and then selecting the appropriate file type. Once you do this, you will be able to copy the specific command syntax directly from the command prompt or review window in Stata to your do-file.\nThe insheet command remains an alternative for .csv, .tsv, and .txt files. It performs differently than import delimited and can be useful for some forms of data, but import delimited should be used preferentially. One thing to note is that it is often a good idea when using the insheet command, to use the option names and have your dataset have the same variable names at those at the top of the raw dataset.\n\n\nImporting multiple files at once\nA useful function for importing multiple files within a folder is the dir extended macro function. You can find documentation on this by typing help extended_fcn in Stata. This function allows you to store all the names of the files in a folder in a local so you can loop through them for importing. See example code of this process below.\n/* This stores all files with the extension .xlsx in the\n\"$raw\" data folder into a local \"files\" */\nlocal files: dir \"$raw\" files \"*.xlsx\", respectcase\n\n/* Loop through the files to import, clean the file name,\nand save as a dta */\nforeach file in `files' {\n\n    *Show your progress of which file you are working on\n    di in red \"working on `file'\"\n\n    *Import each file\n    import excel using \"$raw/`file'\", clear firstrow\n\n    **Quality Checks (Optional)\n    *Assert you have the correct number of observations.\n    qui count\n    // If you know the amount of expected observations\n    assert `r(N)' == [number_of_expected_observations]\n\n    /*Check that what variables you think should be unique\n    identifiers are indeed unique. */\n    isid unique_id_var\n\n    *Check for expected/necessary variables\n    confirm var expected_var_names\n\n    * Edit filename\n    /*The filenames in the local \"files\" includes the\n    extension (in this case .xlsx). So, I remove these and\n    make new clean file name to save the files as.\n    You can edit the filenames however you see fit. */\n    local cleanfilename = subinstr(\"`file'\", \".xlsx\",\"\",.)\n\n    *Save the file with the new clean file name as a dta file\n    save \"filepath/dtafiles/`cleanfilename'_raw.dta\", replace\n}\nNote that the new .dta files are no longer saved in the same folder that your raw excel, csv, or any other type of files were saved in. As the imported data is no longer raw, they should be saved in either a temporary or data folder.\nIt can be helpful set up a ‚Äúdta‚Äù or ‚Äútemp‚Äù folder for you to save these intermediate data files before you start. To do so, you can create a folder to save your files in directly in your script by using mkdir \"filepath\". If the directory already exists, this will create an error. One solution is to use the capture command and type cap mkdir \"filepath\" which will suppress the error. We recommend avoiding capture in most situations.",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html#data-structure-and-reshaping",
    "href": "data-cleaning/raw-data-management.html#data-structure-and-reshaping",
    "title": "Raw Data Management",
    "section": "Data structure and reshaping",
    "text": "Data structure and reshaping\n\nWide and Long Data\nOne way to describe data is if the dataset is stored in a ‚Äúlong‚Äù or ‚Äúwide‚Äù format. Long data keeps repeated values as an observation (row):\n\n\n\nHousehold ID\nMember ID\nTreatment\n\n\n\n\nHH001\n01\nT\n\n\nHH001\n02\nC\n\n\n\nWide data describes data that has similar values as variables (columns). The same example could be displayed wide by expanding the Member ID column to be a suffix of the Treatment column, like the following:\n\n\n\nHousehold ID\nTreatment_01\nTreatment_02\n\n\n\n\nHH001\nT\nC\n\n\n\nThere is no correct choice for how data should be stored. You should decided the format to make the data most usable for the analysis being conducted. As a rule of thumb, in a clean dataset each observation is a row, each variable is a column, and each type of observation is a separate dataset (Wickham, 2014). However, the unit of analysis may change for different analyses. An analysis could be conducted at the person-year-level in one analysis, but at the person-level in another. The context of the data being analyzed are required to determine what format the data should be stored in. As long as an ID variable exists for each level of the data (e.g.¬†a long dataset may have a household ID and a household member ID). For more guidance on how Stata treats ‚Äúwide‚Äù and ‚Äúlong‚Äù datasets as well as how to change between them (reshape the data) type help reshape in Stata.\nWhen cleaning data, it‚Äôs generally a good idea to decide how the data will be stored based on how it will be cleaned. For example, if one goal of cleaning is to produce an income variable at the household-level, but income is collected at the respondent-day-level, it may be a good idea to clean the income module separately and keep that dataset long. Then, the income data can be collapsed to the household-level and merged onto the household-level dataset later in the variable construction process. How, or if, that happens is up to you as the analyst.\nThere is one point at which the data has a known unit of analysis: during high frequency checks and backchecks. Data is always at the unit of the survey submission when doing quality control. One observation is one survey submission. For multi-day surveys, a survey submission may be a different level of data than the survey.\n\n\nSurveyCTO Data\nWhen downloading from SurveyCTO, you can choose whether to export the data in wide or long format. SurveyCTO also produces a file dataset that links long formatted datasets on their unique IDs if the data is exported long.\nKeeping the data in long format means that SurveyCTO will automatically organize observations at the largest unit level (e.g.¬†household-level) and will save all sub-unit level data into separate datasets for each repeat group (e.g.¬†person-level, plot-level, etc.). This means you will have a larger number of datasets to work with (and perhaps a more involved merging process if you want to compile all the data), but each dataset will be at the unit of the question, so may be more intuitive to work with.\nWide data saves you the process of merging manually. If any repeat groups exist, sub-unit level data will automatically be reshaped in order to fit with the main dataset. For example, names of individual household members collected by the variable name will be reshaped as name_1 (for the first member), name_2 (for the second member), etc. It is easier to run quality checks on wide data, as these data will contain all data collected by the survey. However, these datasets can also grow very large very quickly and become unwieldy to work with. To load wide data into Stata it may be necessary to increase the number of variables Stata can read in a single dataset (up to 32,767 for Stata-SE) by typing set maxvar 32767\n\n\nAlternative to reshape\nTo convert between wide and long data, Stata uses the reshape command. Reshaping is a very computationally intensive command. If you are dealing with a large data set you will quickly find that using reshape can take an excessively long time or even break the current Stata session. There is an alternative way to manually code a reshape using expand and replace, that has the benefits of running much faster. It also provides an understanding of how a reshape transforms your data structure. In addition, variable labels can be modified with more control if done manually.\nThe following code reshapes a wide dataset by person to a long dataset by person-month. The variable that we want to reshape is income.\n/*sCount all of the income variables and create\na variable for each observations*/\nds income*\nlocal copies : word count `r(varlist)'\nexpand `copies'\n\n/*Then create a list of all of the vars with each stub\n and manually expand*/\ngen yearmonth = . // create an empty var that will hold the sub-group identifier (this is the j var in reshape)\nforeach var in income {\n\n    *Save all of the income variables\n    ds `var'_*,\n    local reshapevarlist `r(varlist)'\n\n    /*remove the stub so that we can have the yearmonth\n    or j identifier foreach var alone while maintaining their order*/\n    local monthyear = subinstr(\"`reshapevarlist'\", \"`var'_\", \"\", .)\n\n    *create an empty version of the stub, this will become the long var\n    gen `var' = .\n\n    *Loop through each value\n    forvalues x = 1/`copies' {\n\n        * Replace the variable from the list we defined earlier in the loop\n        /* See \"h mod\" to understand how mod works,\n        but in short \"if mod(n, `copies') == `x'\"\"\n        will only replace the variable for the nth observation\n        in each group defined by\n        an ID variable (e.g. the nth or last row created in the expand)\n        */\n        local currvar : word `x' of `reshapevarlist'\n        replace `var' = `currvar' if mod(_n, `copies') == `x'\n\n        * Generate the identifier variable\n        local yearmonth : word `x' of `monthyear'\n        replace yearmonth = `yearmonth' if mod(_n, `copies') == `x'\n\n        *Drop the wide variable\n        drop `currvar'\n    }\n    // end forval x= 1/`copies'\n}\n// end foreach var in income",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html#adding-or-replacing-data",
    "href": "data-cleaning/raw-data-management.html#adding-or-replacing-data",
    "title": "Raw Data Management",
    "section": "Adding or replacing data",
    "text": "Adding or replacing data\nRaw data may not contain the full or correct set of data. There are many reasons why this may be the case: survey responses may need to be corrected by enumerators, translations for responses need to be added, admin data may have entry errors, etc. It is relatively simple to make changes in data using statistical software, but it is important to make changes in a systematic way to ensure all modifications are reproducible.\nThere are at least two common situations in IPA projects where having a standardized data flow for modifying or adding data will increase data quality: replacements and translations. This article suggests best practices to add data collected outside of a survey form.\n\nReplacements\nAs you are collecting data, there will inevitably be errors in your data that need to be manually corrected. It is important to always maintain the raw dataset with the original collected data. Once you have confirmed that a value in your dataset is incorrect and needs to be changed, this replacement should be made and saved in a new dataset, before you have done any other necessary cleaning.\nWhen making a replacement, confirm that you are using a truly unique value for your observation. For example, the key variable should be used if you are making replacements in SurveyCTO data, since there can be duplicates in your ID variable, or you may need to make a replacement in the ID variable.\nFor every replacement you make to your dataset, you must record:\n\nWho made the original error\nWho confirmed it was an error\nThe original value\nThe new value\nThe reason for the change\n\nOne way to make replacements is using the replace command if the key variable matches the observation.\n*enum confirmed they added an extra zero to income\nreplace income = 1000 if key == \"uuid:2b2763e1-71b6-4e1e-8023-c15cdf7fa39d\"\nIf you are making multiple replacements, this method can create long datasets and make it difficult to keep track of which replacements have been made. It can also lead to PII appearing in your do files if you are making replacements on PII data or sensitive data. To avoid encrypting your datasets, consider using user-written commands readreplace or ipacheckreadreplace (ipacheckreadreplace is a wrapper for readreplace). Both commands use an Excel file as an input sheet, where all replacements, notes, original values, and replacements are logged. ipacheckreadreplace has a template replacements Excel file that you can download when you run the ipacheck command.\nIf you are using IPA‚Äôs Data Management System, this code snippet is included in the master_check do file. You can also use the ipacheckreadreplace command in your own code with this format:\nipacheckreadreplace using \"hfc_replacements.xlsm\", ///\n    id(\"key\") ///\n    variable(\"variable\") ///\n    value(\"value\") ///\n    newvalue(\"newvalue\") ///\n    action(\"action\") ///\n    comments(\"comments\") ///\n    sheet(\"sheetname\") ///\n    logusing(\"replacements_log.xlsx\")\nThe Excel template already uses these column names, so you must change the options/column names if you are using your own file or column names. ipacheckreadreplace also creates a replacements log, another Excel file that lists all the replacements, notes, and values, as well as a note that specifies if the replacement was successful. A replacement will only be successful in ipacheckreadreplace if the unique ID variable and the original value match what was entered in hfc_replacements.xlsx.\n\n\nTranslation\nSometimes open survey responses need to be translated for deliverables or to support an analyst who isn‚Äôt fluent in the survey language. Translating these data within statistical software can result in long scripts with large potential for error rates and a potential to contain PII. This can be avoided by using an excel-based workflow with encrypted translation file:\n\nFor each variable that needs to be translated, save the values that need translation to an excel sheet with an empty column (variable in the Stata .dta) for the language the responses need to be translated into.\nTranslate the responses using a standardized procedure, e.g.¬†double-entry with another person breaking ties and rules on when to drop comments with PII.\nWrite code to merge the translation from the excel file back into the do file, instead of running this as a series of replace commands that are prone to error.\n\nThis workflow is shown below:\n/* MR 10/25/2019:\n  Create a file to store translations for question q.\n*/\n\n*Generate translated variable name\ngen q_en = \"\"\nlocal vlab : variable label q // extract variable label to copy\nlabel variable q_en \"`vlab', English\" // label with translation info\n\n*Save excel file for coding with only those questions that need translating\nexport excel id q q_en using \"${temp}/q_en.xlsx\" if !mi(q), firstrow(variables) replace\nThis results in a table that looks like this:\n\n\n\nid\nq\nq_en\n\n\n\n\n01-0001\nLe dio sus herramientas agr√≠colas a su primo\n\n\n\n18-0007\nEl ID de esta respuesta debe set 18-0008, no 18-0007\n\n\n\n\nThe responses would be translated and then the file is then saved with a different name. We recommend saving the file as ‚Äú[filename]_translated_[date]_[initials]‚Äù. We recommend doing double entry for all manual additions and comparing differences between any added responses. The completed file would have a value for every question below.\n\n\n\n\n\n\n\n\nid\nq\nq_en\n\n\n\n\n01-0001\nLe dio sus herramientas agr√≠colas a su primo.\nThis respondent gave their farming tools to their cousin\n\n\n18-0007\nEl ID de esta respuesta debe set 18-0008, no 18-0007\nThe ID of this response should be 18-0008, not 18-0007\n\n\n\nOnce these translations are completed, they would be merged on to the do file. The code to complete that looks like this:\n/* MR 01/24/20:\n    Merge on the translated data from the saved excel file.\n\n    MR translated these data on January 24, 2020 and double entered them.\n    MR's RM double checked conflicting translated entries.\n\n    If PII was removed as part of the translation process, this will be\n    marked with a dummy* and corrected in the response of both languages.\n\n    *Note: no responses needed PII removal, so this code does not\n    create the dummy.\n*/\n*Load in data and save a tempfile\npreserve\n\nimport excel using \"${temp}/q_en_20200124_MR.xlsx\", first clear\n\n*Do some cleaning to ensure excel files matches expected\nmissing dropvars, force // remove extravars\nmissing dropobs, force // remove empty observations\nconfirm variable id q q_en // check have variables\n\n*Save tempfile to merge\ntempfile q_en\nsave `q_en'\n\nrestore\n\n*Merge on file\nmmerge id using `q_en', t(1:1) uname(check_)\n\n*Check file\nassert _merge == 1 | _merge == 3 // in master only or translated\nassert q == check_q if _merge == 3 // ensure no changes to comment field\nassert _merge == 3 if !mi(q) // ensure all are translated\n\n*Manually replace translation after checks\nreplace q_en = check_q_en\n\n*Housecleaning\ndrop _merge check_q check_q_en\nThis process can be repeated for any number of variables. It can also be extended to remove PII as part of the translation process. In that case, make sure to maintain a raw version of the dataset that is encrypted, and mark which values were changed to remove PII in the translated dataset.",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html#surveycto-split",
    "href": "data-cleaning/raw-data-management.html#surveycto-split",
    "title": "Raw Data Management",
    "section": "SurveyCTO split",
    "text": "SurveyCTO split\nSome surveys questions have the option of ‚Äúcheck all that apply,‚Äù meaning that they allow for multiple responses. If you uncheck the ‚Äúexport select_multiple responses as a series of 1/0 columns‚Äù in SurveyCTO desktop, SurveyCTO will export these variables as a string containing all possible answers in the form of a space-separated list (e.g., a variable may have the string ‚ÄúA C E‚Äù to indicate responses of A, C, and E).\nFor analysis purposes, it‚Äôs usually best to split these variables into binary variables, one for each possible choice (A, B, C, D, or E), as well as one for each combination of choices. The IPA-written stringdum function can help converting these variables to a more useful form, including separate dummies for each option, variables that count the number of times each option is selected, and several options for naming conventions. The split command can also convert these variables, by parsing the variable on spaces. This splits the variable into a new variable after each space (see help split). If this is not sufficient, Stata has a number of string functions (help string functions). You can also use the regexm function to accomplish this, which uses regular expressions. Below is an example script using both split and regexm to accomplish cleaning a select multiple question manually.\n* Create a local of all string variables\nqui ds _all, has(type string)\nforeach var in `r(varlist)' {\n    local stringvars `stringvars' `var'\n}\n\n\n/* Exclude from stringvars vars with names that will be too long if we append\n  four characters (\"_num\") to them first check using tab that they are not\n  select_mult. If the length of the variable name is greater than 27 characters,\n    1. manually verify that it is not a select_mult variable\n    2. add it to a special local\n*/\nforeach var of varlist _all {\n    if strlen(\"`var'\")&gt;27 {\n        tab `var'\n        local too_long_vars `too_long_vars' `var'\n    }\n}\n\n* Remove the too_long_vars from the string variable list\nlocal stringvars: list stringvars - too_long_vars\n\n/* For each string variable,\n   1.  create a tempvar,\n   2.  an indicator for which have a number-space-number combo and\n   3. add them to a select multiple local\n*/\nforeach var in `stringvars' {\n    tempvar `var'_num\n    gen ``var'_num' = (regexm(`var', \"[0-9] [0-9]\"))\n    qui sum ``var'_num'\n    if r(max) ==1 {\n        *add the variable to a list of select_multiple variables\n        local select_mult `select_mult' `var'\n    }\n}\n\n/* Exclude from the `select_mult' local variables which have been\nerroneously captured by the process above irregular responses to\nthe \"survey_note\" variable are captured by this process */\nlocal exclude_notes \"survey_notes\"\nlocal select_mult: list select_mult - exclude_notes\n\n/*For several otherwise numeric variables,\nthe option \"Not asked in this version\" remains; remove it*/\nforeach var in `select_mult' {\n\n     *no observations; safe to use this as a numeric indicator for .v\n    tab `var' if `var'==\"-444\"\n    replace `var'=\"-444\" if `var'==\"Not asked in this version\"\n\n    /* split and de-string vars in the selec_mult list,\n    which now have only numeric characters */\n    split `var', gen(`var'_) destring\n}\n\n*Re-code missing values in the now-numeric variables\nqui ds _all, has(type numeric)\nforeach var in `r(varlist)' {\n    replace `var'=.o if `var'==-777\n    replace `var'=.d if `var'==-888\n    replace `var'=.r if `var'==-999\n    replace `var'=.v if `var'==-444\n}",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html#unique-identifiers",
    "href": "data-cleaning/raw-data-management.html#unique-identifiers",
    "title": "Raw Data Management",
    "section": "Unique identifiers",
    "text": "Unique identifiers\nUnique IDs are critical to a well-managed dataset, particularly when it comes time to merge across datasets or sort values in a consistent manner. You should use the isid command in Stata to check that the ID is indeed unique before you begin any sort of data management. Uniqueness is important not just to describe data, but because it affects how Stata manages data. Data will take a random order within a non-unique value if data are sorted on a shared identifier. This randomness is controlled by a separate random seed in Stata and can be made reproducible using set sortseed or sort, stable. Those approaches do not substitute for not having a unique ID; reproducibility is not a substitute for uniqueness.\nTo illustrate this, you can run the following code in Stata:\n*Use system dataset\nset sortseed 1\nsysuse auto, clear\n\n*sort by foreign and save order\nsort foreign\ngen sort_order = _n\n\n*sort randomly so Stata will sort by foreign again\n/*  sort first checks if it's in order, and doesn't do anything if\n    the data are in the expected order\n*/\ngen rand = runiform()\nsort rand\ndrop rand\n\n*Check if foreign has a unique order\nsort foreign\nassert _n == sort_order // check if the current order == saved order\n\n73 contradictions in 74 observations\nassertion is false\nr(9);\n\nDuplicates\nID variable should describe data at the unit of analysis. The unit of analysis is dependent on what the dataset is describing. For example, a survey module that describes yield by plot should not be unique at the household-level, but should be at the household-plot-level. That is equivalent to saying that isid householdid plotid should not return an error in Stata. If you receive an error, use the duplicates command to investigate as to why you have duplicate observations (help duplicates). Although duplicate surveys should be resolved in IPA‚Äôs data management system, duplicate observations may remain in the raw data for a variety of reasons.\nIf they are true duplicates across all variables and it is clear why these duplicates were created, you can proceed to remove duplicate copies. If they are not duplicates across all variables, you will need to find why there are multiple observations and develop a rule for choosing which one to keep. Selected values should be reproducible and follow a consistent rule. In general, observations that are more recent and more informative (i.e., have fewer missing values) are better. However, for some projects, it is better to keep the earlier observation. You should check with your PI, the project manager and other staff involved in the survey wave about how to create a rules or rules. Once you come up with a decision, be sure to document both what you decide to do and why it was decided, as well as when and who made the decision for future reference.\nDuplicates may not just exist for an ID variable. Some respondents may take a survey twice and receive two different IDs. See help duplicates for guidance on commands that can be useful for this. Apart from dropping duplicates in terms of all variables using duplicates drop, check for duplicates in terms of things you suspect may identify the same person (e.g., name, address, phone number, birthday and combinations of the above) using either duplicates or isid.\nAfter cleaning each dataset individually, check that the unique IDs are assigned correctly across datasets. For instance, if you have a baseline survey and an endline survey, after merging them based on unique ID, check that the names and birthdate variables from each survey match to the other survey.\n\n\nID formats\nIDs may not be unique due to how Stata manages numerical formats. If the ID variable is longer than 17 digits, numeric IDs will no longer be unique as Stata will begin to round the trailing digits. This is due to how Stata stores when they have more digits than their storage type can hold (16 for doubles). No numeric IDs will be unique if they have more than 17 digits, especially if the last digits are changing for individuals that should be unique. Store IDs as strings and include a letter to ensure that the ID will not be turned into a numeric variable and subsequently rounded by compress or destring. Alternatively, keep ID values at the minimum length needed, as it is not necessary to have an ID value over 17 digits.",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-cleaning/raw-data-management.html#de-identifying-data",
    "href": "data-cleaning/raw-data-management.html#de-identifying-data",
    "title": "Raw Data Management",
    "section": "De-identifying data",
    "text": "De-identifying data\nPersonally Identifiable Information ‚Äì also known as PII ‚Äì is any datapoint or combination of datapoints that allows someone to identify an individual or household with a reasonable degree of certainty.\nExamples of individual data points of PII include names, GPS coordinates, national identification numbers and addresses. Depending on the context, certain combinations of demographic data points qualify as PII so long as they can identify an individual or household with a reasonable degree of certainty. For example, the combination of village name, birth date, gender might be identifiable in small communities. Requirements and recommendations around PII apply equally to PII that consist of singular data points and those that consist of combinations of data points.\nAll PII must remain encrypted in storage and securely transmitted between devices. The only people who can access PII data must be both on the IRB-approved research protocol and referenced in the informed consent. For more information, see IPA‚Äôs protocols surrounding PII\n\nWhen to remove PII\nAfter you have data, it‚Äôs best practice to remove PII as soon as possible. The earlier you can de-identify the better. Unless necessary, avoid working with data containing PII as you move forward with analysis.\nSome aspects of cleaning and analysis will require including PII. For instance, if you are trying to remove duplicate observations and need to determine if two respondents have the same name, you‚Äôll want that in there. As always, you will need to keep the data encrypted with Boxcryptor at that point. Make sure do-files/scripts are encrypted as well if they include PII.\nAfter you no longer need PII, strip it from the main dataset. The best practice for de-identifying data is to:\n\nCreate a first .do file that imports the data, standardizes duplicate observations, completes corrections, and removes PII.\nCreate a second .do file that performs the data cleaning on the limited or fully deidentified dataset. If need be, you can save this version in an unencrypted folder so that other project members who can access the data.\n\nThis results in a data flow where all data modification happens after the data is deidentified.",
    "crumbs": [
      "Data Cleaning",
      "Raw Data Management"
    ]
  },
  {
    "objectID": "data-collection/admin-data.html",
    "href": "data-collection/admin-data.html",
    "title": "Administrative Data",
    "section": "",
    "text": "A guide to obtaining, using, and managing administrative data for research, covering access processes, ethical considerations, and common challenges.",
    "crumbs": [
      "Data Collection",
      "Administrative Data"
    ]
  },
  {
    "objectID": "data-collection/admin-data.html#what-is-administrative-data",
    "href": "data-collection/admin-data.html#what-is-administrative-data",
    "title": "Administrative Data",
    "section": "What is Administrative Data?",
    "text": "What is Administrative Data?\nAdministrative data is typically collected by government agencies and organizations for registration, transaction, and record-keeping purposes. Examples of administrative data can include credit card transactions, electronic medical records, insurance claims, educational records, arrest records, and mortality records.\n\nReal-World Example: IPA‚Äôs Experience with Administrative Data\nIPA‚Äôs research on using administrative data for Monitoring and Evaluation (2016) highlights several key advantages:\n\nCost-Effectiveness: Administrative data reduces or eliminates the need for additional monitoring activities or surveys.\nTimely Response: Regular updates in management information systems enable faster analysis of key indicators.\nLarge Sample Size: Coverage of entire beneficiary populations provides robust sample sizes.\nImproved Accuracy: Reduces social desirability bias and recall issues common in self-reported data.\n\nThis research emphasizes that data accuracy and reliability should take precedence over cost savings. Organizations must balance data quality, actionability, and resource allocation when incorporating administrative data into their research design.\n\n\nUnable to display PDF file. Download instead.",
    "crumbs": [
      "Data Collection",
      "Administrative Data"
    ]
  },
  {
    "objectID": "data-collection/admin-data.html#standard-processes-for-accessing-administrative-data",
    "href": "data-collection/admin-data.html#standard-processes-for-accessing-administrative-data",
    "title": "Administrative Data",
    "section": "Standard Processes for Accessing Administrative Data",
    "text": "Standard Processes for Accessing Administrative Data\nAccessing administrative data involves four main steps:\n\n\n\n\n\n\nFinding Administrative Data\n\n\n\n\n\nImplementing partners and government agencies are valuable sources of administrative data. Common sources include:\n\nHealth Data: Regional/national health departments, hospitals, health insurance records\nFinancial Data: Banks, credit unions, credit reporting agencies\nEducation Data: Schools, ministries of education, standardized testing agencies\n\n\n\n\n\n\n\n\n\n\nFormulating a Data Request\n\n\n\n\n\nWhen requesting administrative data, researchers should:\n\nDefine the time frame, format, and structure needed: ‚ÄúPrimary school records from January 2022 to December 2023 in CSV format‚Äù\nList specific variables of interest: Student ID, school level, Teacher ID, attendance, test scores\nSpecify whether you need identified or de-identified data: Request de-identified data when possible to reduce ethical complexity\nAvoid broad requests: Instead of ‚Äúall student data,‚Äù specify exact variables, time frames needed, and frequency of updates\n\n\n\n\n\n\n\n\n\n\nData Flow Strategies\n\n\n\n\n\nA well-planned data flow strategy ensures smooth integration of administrative data:\n\nGather Identifying Information: Determine what identifiers are available in the study sample such as national ID numbers, phone numbers, email addresses\nLink Datasets: Use pre-existing identifiers to match study data with administrative data. For example, match student IDs with test records using national ID numbers\nChoose a Matching Strategy:\n\nExact matching: When identifiers are identical such as national ID numbers\nProbabilistic matching: When using combinations of name, date of birth, and location information\n\nDetermine Who Performs the Link: Clarify whether the data provider, researcher, or a third party will conduct the linkage and deindentification to maintain data security and privacy",
    "crumbs": [
      "Data Collection",
      "Administrative Data"
    ]
  },
  {
    "objectID": "data-collection/admin-data.html#ethical-considerations-for-using-administrative-data-in-rcts",
    "href": "data-collection/admin-data.html#ethical-considerations-for-using-administrative-data-in-rcts",
    "title": "Administrative Data",
    "section": "Ethical Considerations for Using Administrative Data in RCTs",
    "text": "Ethical Considerations for Using Administrative Data in RCTs\nUsing administrative data in research requires adherence to ethical and legal standards. Key considerations include:\n\nInstitutional Review Board (IRB) Approval\nMost research using administrative data qualifies as human subjects research and requires IRB approval. This includes ensuring:\n\nProper handling of personally identifiable information (PII)\nJustification for using identified vs.¬†de-identified data\nSecurity measures for protecting sensitive information\n\n\n\nInformed Consent\nIn some cases, researchers may need to obtain informed consent from participants before accessing administrative data. This depends on:\n\nThe type of data you access\nWhether it is possible to de-identify the data\nLegal requirements set by the data provider\n\n\n\nData Security and Compliance\nResearchers must implement robust security measures, including:\n\nEncryption for storing and transferring sensitive data\nAccess controls to limit data exposure\nCompliance with legal regulations such as the General Data Protection Regulation (GDPR) or Health Insurance Portability and Accountability Act (HIPAA), if applicable\n\nFor further clarification on IRB-related issues, any project with concerns should email humansubjects@poverty-action.org.",
    "crumbs": [
      "Data Collection",
      "Administrative Data"
    ]
  },
  {
    "objectID": "data-collection/admin-data.html#challenges-when-using-administrative-data",
    "href": "data-collection/admin-data.html#challenges-when-using-administrative-data",
    "title": "Administrative Data",
    "section": "Challenges When Using Administrative Data",
    "text": "Challenges When Using Administrative Data\nWhile administrative data offers many advantages, researchers often face challenges such as:\n\nDifferential Coverage\nTreatment and control groups may appear differently in administrative records, leading to bias. Examples include:\n\nIdentifiers Obtained After Enrollment: In a financial literacy program evaluation, treatment group participants may be more willing to provide bank account numbers for linking with administrative data, creating selection bias.\nProgram-Generated Data: If the intervention encourages healthcare visits, treatment group participants will appear more frequently in health administrative records, inflating apparent impact.\n\n\n\nReporting Bias\nSome administrative data relies on self-reporting, which can introduce inaccuracies. Examples include:\n\nIncentives for Misreporting: Agencies or individuals may have reasons to over- or under-report data.\nHuman Error in Data Entry: Manual data entry can introduce inconsistencies.\n\n\n\nCost of Administrative Data\nAdministrative datasets vary in cost, depending on:\n\nThe number of records requested\nFile-years needed\nData preparation time required by the provider",
    "crumbs": [
      "Data Collection",
      "Administrative Data"
    ]
  },
  {
    "objectID": "data-collection/admin-data.html#conclusion",
    "href": "data-collection/admin-data.html#conclusion",
    "title": "Administrative Data",
    "section": "Conclusion",
    "text": "Conclusion\nAdministrative data is a powerful tool for randomized evaluations, offering cost-effective, accurate, and comprehensive insights. However, researchers must navigate ethical, legal, and logistical challenges to ensure data quality and validity. By following standardized processes, addressing ethical concerns, and mitigating common challenges, researchers can leverage administrative data for impactful research.",
    "crumbs": [
      "Data Collection",
      "Administrative Data"
    ]
  },
  {
    "objectID": "data-collection/index.html",
    "href": "data-collection/index.html",
    "title": "Data Collection at IPA",
    "section": "",
    "text": "IPA employs various data collection methods to ensure comprehensive and high-quality research. This section provides guidance on the main data collection approaches and implementation protocols including in-person surveys, phone surveys, WhatsApp data collection, administrative data, and qualitative methods.",
    "crumbs": [
      "Data Collection",
      "About Data Collection"
    ]
  },
  {
    "objectID": "data-collection/index.html#data-collection-methods",
    "href": "data-collection/index.html#data-collection-methods",
    "title": "Data Collection at IPA",
    "section": "Data Collection Methods",
    "text": "Data Collection Methods\n\n\n\n\n\n\nIn-Person Surveys\n\n\n\n\nIn-Person Survey Guidelines\n\nIn-person surveys remain the gold standard for detailed data collection. Through face-to-face interviews, enumerators can build rapport with respondents, observe behavioral cues, and collect complex information that may be difficult to obtain through other methods. In-person surveys remain the gold standard for detailed data collection. Through face-to-face interviews, enumerators can build rapport with respondents, observe behavioral cues, and collect complex information that may be difficult to obtain through other methods.\n\n\n\n\n\n\n\n\nPhone Surveys\n\n\n\n\nPhone Survey Guidelines\n\nPhone surveys are a cornerstone of data collection at IPA. Voice and text-based phone surveys offer a cost-effective and adaptive method to gather high-quality data, especially in contexts where in-person surveys are impractical. Phone surveys are a cornerstone of data collection at IPA. Voice and text-based phone surveys offer a cost-effective and adaptive method to gather high-quality data, especially in contexts where in-person surveys are impractical.\n\n\n\n\n\n\n\n\nWhatsApp Data Collection\n\n\n\n\nWhatsApp Surveys Guidelines\n\nWhatsApp has become a powerful tool for data collection, offering a cost-effective, scalable, and user-friendly alternative to traditional survey methods.\n\n\n\n\n\n\n\n\nAdministrative Data\n\n\n\n\nAdmin Data Guidelines\n\nThis resource provides a quick guide on obtaining and using nonpublic administrative data for randomized evaluations.\n\n\n\n\n\n\n\n\nQualitative Methods\n\n\n\n\nQualitative Methods Guidelines\n\nQualitative research methods provide invaluable insights into human experiences, behaviors, and systems that drive change. These approaches help bridge the gap between quantitative data and the realities of individuals and communities participating in research studies.",
    "crumbs": [
      "Data Collection",
      "About Data Collection"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/ethnographies.html",
    "href": "data-collection/qualitative-methods/ethnographies.html",
    "title": "Rapid Ethnographic Assessments",
    "section": "",
    "text": "Practical guidance for researchers and practitioners on how to plan and conduct Rapid Ethnographic Assessment to gather insights on specific communities, issues, or environments within short time frames through flexible, participatory approaches.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Rapid Ethnographic Assessments"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/ethnographies.html#what-is-a-rapid-ethnographic-assessment",
    "href": "data-collection/qualitative-methods/ethnographies.html#what-is-a-rapid-ethnographic-assessment",
    "title": "Rapid Ethnographic Assessments",
    "section": "What is a Rapid Ethnographic Assessment?",
    "text": "What is a Rapid Ethnographic Assessment?\nRapid Ethnographic Assessments (REA) are a qualitative research method that allows researchers to collect information efficiently for decision-making. This methodology has several key features1:\n\n\nConduct participant observations: To approach the internal perspective of individuals in specific situations, from everyday life and the researcher‚Äôs own experiences during events.\nUse of field notes: To condense the preliminary results from various data collection techniques such as focus groups, interviews, and observations. These field notes include dense descriptions and reflective statements documenting the fieldwork, along with personal reflections.\nEstablish relationships with the community or participants: REA builds on the premise that researchers can obtain extensive and in-depth information through short, intensive and participatory fieldwork with the community. This participatory aspect ensures active engagement and involvement.\nSeek social and cultural relationships within a group of people: REA emphasizes on understanding individual daily activities and group‚Äôs culture, norms, activities, and social relations. In contrast to classic approaches, it offers a more specific focus, a shorter duration, and limited scope.\nHolistic approach to the topics studied: Conducting an REA allows researchers to connect qualitative data collection techniques and other available data sources to generate insights efficiently. Researchers obtain these insights through their interpretations. REA focuses not on objectivity but on identifying high-quality relationships among people, stories, and events mapped in the field.\n\n\nThese principles have their roots in classical ethnography, which aims to understand a problem or situation from the ‚Äúinsider‚Äôs‚Äù or ‚Äúsubject‚Äôs perspective.‚Äù Unlike classical ethnographies that involve extensive time‚Äîmonths or years‚Äîin the field, researchers execute Rapid Ethnographic Assessments in a short period and aim to tell decision-makers timely. Some authors suggest conducting REA over four to six weeks.2\n\n\n\n\n\n\nCombining multiple data-collection techniques\n\n\n\nWith rapid ethnographies, researchers collect information mainly through qualitative techniques, such as interviews, focus groups, and observations. However, researchers can also use quantitative techniques, such as available data and short surveys, to support answers to research questions.3 To achieve relevant insights, REAs feature triangulation of results, consolidation of multidisciplinary teams, and iterative learning processes.4 Combining multiple techniques positions rapid ethnography as a tool that provides findings and recommendations based on detailed, culturally relevant information reflecting local realities.\n\n\n\nWhen to consider doing a REA?\nREAs are particularly useful when practical and actionable recommendations are needed within short time frames. While this methodology has great potential, the rapid collection and processing mean that REA findings are exploratory. Their scope focuses on identifying findings from particular contexts rather than generating generalizable knowledge. REA is especially useful when:\n\nMore information is needed about an emerging problem or situation with little existing knowledge: Rapid ethnographies can provide descriptive information about complex problems, helping to identify causes and design appropriate interventions.\nEngagement with ‚Äúhidden‚Äù or vulnerable populations is necessary: Some populations may be particularly closed and difficult to reach. Accessing these groups requires the support of trusted individuals within the community. Rapid ethnographies often seek to identify and involve these community leaders to facilitate contact and build trust during the research.\nCommunity involvement is essential: Understanding how to address a problem requires the involvement of local community members. Involving the people who are directly affected by a problem often results in more practical and achievable recommendations for the program or intervention.\n\nBelow are three real-world applications of rapid ethnographic assessments:\n\n\n\n\n\n\nPublic Health\n\n\n\n\n\nAfter Hurricane Katrina, New Orleans experienced a demographic increase in the Latino population, particularly among single, undocumented men. Researchers suspected an emerging pattern of crack use among this population but needed more information to formulate responses. The REA results revealed how contextual factors such as a growing drug market, coupled with social isolation and victimization of the undocumented Latino population, led to the initiation and increase in crack use among a group that previously had relatively low drug use.\n\n\n\n\n\n\n\n\n\nMigration\n\n\n\n\n\nAccessing vulnerable migrant and refugee populations presents challenges due to stigmatization from irregular migratory status, lack of documentation, and distrust toward state institutions. Rapid ethnographies enabled strategic approaches with community leaders and migrant organizations within local communities. Their involvement proved fundamental in building community trust and accessing the target population, making participants feel included and valued in the process.5\n\n\n\n\n\n\n\n\n\nEmergency Settings\n\n\n\n\n\nBetween November 2020 and January 2021, during the COVID-19 pandemic, researchers conducted a REA in Dhaka, Bangladesh. They used in-depth interviews and participant observations to explore health-related beliefs and practices. The REA revealed a gap between scientific explanations of COVID-19 and the community‚Äôs cultural and spiritual beliefs, such as the perception of the virus as a disease of the rich and sinners, leading many to reject biosecurity measures. These findings highlighted the need for deeper understanding of community perceptions to improve public health policy effectiveness.6",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Rapid Ethnographic Assessments"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/ethnographies.html#planning-a-rea",
    "href": "data-collection/qualitative-methods/ethnographies.html#planning-a-rea",
    "title": "Rapid Ethnographic Assessments",
    "section": "Planning a REA",
    "text": "Planning a REA\n\n\nAssess the relevance of the methodology\n\n\nBefore starting a rapid ethnographic assessment, it is crucial to determine if this methodology is suitable for your research objectives. Consider the following questions:\n\nDoes the research require in-depth, qualitative insights within a short timeframe? Rapid ethnographies are designed to provide timely, actionable data.\nAre there specific cultural or social dynamics that need to be understood quickly? This method is particularly useful for exploring complex social interactions and cultural contexts.\nIs there a need for participatory engagement with the community? Rapid ethnographies emphasize active involvement and collaboration with community members.\nAre the necessary resources and expertise available? Ensure that your team has the skills and tools required to conduct qualitative research effectively.\n\nIf the answers to these questions align with your research needs, rapid ethnography may be an appropriate choice. The data produced during REA is gathered through a combination of various data collection techniques. The information and resources obtained from these techniques can be presented in a variety of formats. This interactive process involves the use of:\n\nAudio recordings from interviews and focus groups\nPhotographs and videos taken during observation activities\nTexts resulting from note-taking during activities\nOther resources such as maps or drawings provided by the community\n\nThe mechanism of interaction between resources and the process on how that information contributes to generate insights depends on the scope of research questions and the strategy designed by the researcher to address rich information with time constraints.\n\n\nAdditional considerations for conducting a REA\n\n\n\nDuration: Although there is no set rule on how much time is sufficient to conduct it, most experts agree that the time spent in the field collecting data should be between two and six weeks.7 The speed of data collection will depend on research objectives, funding, and team technical expertise. Remember that a key aspect of rapid ethnographies is to produce actionable and timely data for decision-makers. If the timeline is not feasible and available funding is limited, another method may be more appropriate. Key questions to consider include: When are the findings needed? Can the timeline be met? Are the necessary technical and financial resources available?\nEthical considerations: Ensure that appropriate IRB permits and certifications are in place if required. This measure ensures that the research will not harm people and communities due to the project‚Äôs processes or findings.\nData collection techniques: Identify data collection techniques that will be prioritized Here, consider the information you want to collect and consult the practical guidelines for focus groups, interviews, and observations, as these are the most common collection techniques in REA. However, depending on the research needs and the research team‚Äôs experience, other collection techniques such as ethnographic mapping or short surveys can be implemented (For detailed data collection techniques used in rapid ethnographies, see Annex 1).\n\n\n\nStructure your team\n\n\nWhen structuring your team, carefully consider who will make up an effective and productive team. Take into account the experience required for each role and ensure that team members are well-suited to the tasks they will perform in the field. To achieve this, define who will be in the field and the characteristics that each team member should have. The best multidisciplinary teams comprise people with complementary skills and technical or subject matter expertise. In addition, teamwork has been identified as beneficial, as it facilitates the development of ethnographic research from different points of view, often integrating knowledge from various fields, areas, or topics.8 Three essential elements of a strong team are (i) expertise in qualitative methods, (ii) expertise in content or topics, and (iii) expertise in the local context.9\nThere should be at least one person on the project team with experience in qualitative research methods. This person should be responsible for helping to keep the project on track by monitoring the quality of the data collected and training team members when necessary. This role will facilitate learning from participants due to their ethnographic orientation and sensitivity to the project.10\nIt is also highly recommended to have people with thematic expertise and actively involve stakeholders. Thematic experts could guide the research team in understanding the findings nuances and connecting it with literature, as well as provide institutional context, depending on the study topic. In turn, stakeholders can act as advisors throughout the study, ensuring relevance and facilitating the incorporation of findings into practical changes.11\nIn addition, the team should include people who are closely involved on a daily basis with the problem or phenomenon under study. These are usually people from the local community or target population. These people are sometimes called ‚Äúcultural experts.‚Äù They should be a trusted person who can help facilitate access and interaction with hard-to-reach populations and provide information on how things are organized within their community.12\nFinally, assign roles and functions to each team member. Before starting the collection, define the roles and functions of the field team. Some key roles and functions must be assigned to achieve the proposed objectives. To do this, make a role assignment matrix to organize the tasks of contacting or scheduling participants, logistics of access to the territory, information collection, data flow closure, and analysis.\n\n\nRecognize the skills of the field team13\n\n\nThe quality of data produced during qualitative research activities is closely related to the skills of the moderator and facilitator. These competencies, outlined in Table¬†1, enable teams to recognize and adequately represent the diversity within qualitative data.\nWhile these skills are typically developed through years of study and practice, field teams often consist of individuals from multidisciplinary backgrounds with varying levels of experience. Regardless of experience level, it is important to review and discuss these skills with your team to identify possible gaps and opportunities for improvement.\nCurrently, there are no standardized metrics to measure the prevalence of these skills among qualitative fieldwork moderators. Therefore, the skills in Table¬†1 should be viewed as a resource for reference and reflection for teams.\nEnsuring that everyone on the team understands how these competencies contribute to effectively engaging with the population is essential for collecting high-quality information.\n\n\n\nTable¬†1: Skills of field staff conducting a REA\n\n\n\n\n\n\n\n\n\n\nAbility\nDescription\nWhy is this skill important?\n\n\n\n\nCognitive empathy\nThe field team‚Äôs ability to understand and communicate participants‚Äô situations from their perspectives, understanding how they see the world and their roles within\nAllows researchers to connect more deeply with participant‚Äôs realities and experiences. Helps to create a relationship of trust and respect with the participants. Seeks to avoid generalizations and stereotypes that may arise from preconceptions or external influences such as previous studies. Enhances understanding of participants‚Äô situations without resorting to pity.\n\n\nFollow-up\nThe field team‚Äôs ability to recognize when additional information is needed to answer the questions initially posed and those that arise during the research process. This ability implies curiosity and a willingness to explore new issues or doubts that emerge as data collection progresses.\nIncreases the quality and robustness of data by allowing a more detailed exploration of the studied phenomenon. Contributes to obtaining deeper responses from participants. Enables exploration of emerging themes during data collection. Helps in detecting and validating patterns observed in the field.\n\n\nSelf-awareness and reflexivity\nThe field team‚Äôs ability to continuously reflect on how their presence, background, and assumptions influence data collection, interpretation, and analysis. This ongoing self-reflection ensures that the qualitative field team is mindful of its impact on the research process and the participants.\nHelps maintain ethics in the researcher-participant relationship. Facilitates understanding of personal limitations in connecting with participants. Aids in developing strategies to overcome communication barriers and create an environment where participants feel comfortable sharing sensitive information.\n\n\nHeterogeneity\nThe field team‚Äôs ability to represent and reflect the diversity within the group being studied. This skill involves recognizing and documenting the differences and variations among individuals or subgroups during qualitative research, typically applied during the data analysis phase.\nContributes to challenging generalized and simplistic patterns. Ensures that data reflect both common and atypical experiences. Demonstrates the field team‚Äôs ability to identify, recognize, and document heterogeneity in the population studied.\n\n\nPalpability\nThe field team‚Äôs ability to provide detailed descriptions in their field notes or diaries, making the data tangible and clear. This involves avoiding abstract descriptions and, instead, offering vivid accounts that allow the research team to visualize and understand participants‚Äô experiences and contexts.\nThe palpable field notes and diaries are accompanied by textual quotations, images, or other audiovisual resources that show events, situations, and actors that support the research findings. Reliable findings are supported by specific details that clearly depict the events and situations studied. Helps to avoid abstraction in the data, grounding conclusions in concrete evidence.\n\n\n\n\n\n\n\n\nPlan logistics activities\n\n\n\nContact allies, leaders, and key actors: Engage with local partners to understand contextual factors, convene participants, and coordinate logistics. Local partners can facilitate data collection.\nMake a schedule defining the field collection activities:: Define all field collection activities, the total number of tasks to be completed, and the time available; Share the schedule with allies and key stakeholders to; Confirm participant availability on the selected dates; Identify locations where activities will take place.; Ensure there are no conflicting events in the community; Use visual tools such as a Gantt chart for developing the schedule.\nVerify the location and conditions of the places where the collection activities will take place: Identify safe locations that facilitate interaction with the community. Work with local community organizations and leaders if possible. Gather general field information and visit the community with someone locally trusted to establish rapport and familiarize yourself with the area.\nEnsure the safety of the field team: Conduct a thorough risk assessment during the planning phase. Prioritize the safety of both the field team and the community to minimize risks.\nEnsure that you have all the necessary materials to carry out the activities: Prepare incentives for participants (if applicable). Arrange refreshments, stationery, attendance lists, informed consents (if applicable), recording equipment, photographic equipment, batteries, etc.\n\n\n\n\n\n\n\nScheduling REA activities\n\n\n\nA field collection activity lasts, on average, two to three hours. Therefore, a maximum of two activities per day is recommended. This ensures data quality, does not exhaust the field team or the community, and avoids losing focus due to fatigue or saturation. In addition, the schedule of activities should include the time allocated by the team to:\n\nTravel or transport to the collection sites\nCollect data\nTake notes\nRefine the field notes and integrate other resources generated\n\n\n\n\n\nPlan to ensure effective fieldwork and secure storage of gathered information\n\n\nIn addition to logistical planning, the team must decide how to handle the collection and secure management of data to ensure its quality and facilitate its subsequent systematization. To this end:\n\nDefine who will lead data collection and reception within the team: One person should lead the instrument design, data collection, resources reception (based on fieldwork results), and analysis. Ideally, this person should have experience applying qualitative research methods and be involved in fieldwork and data collection. This person‚Äôs involvement in the project‚Äôs different stages will make it easier to manage the information collected and ensure the integration of the various sources and resources.\nDefine a note-taking strategy for the different activities: Note-taking is essential across all data collection activities in REA, even if recordings are available. The team leader should agree with the team on things like the format that everyone will use for their note-taking, critical aspects that require more or less attention from the team to address in the notes, types of formats that could complement the notes (photographs, videos, maps, etc.), and the dates for intermediate and final results. Overall, Having a common understanding of these processes will ensure the smooth flow of data collection and a clear scope of each role within the team.\nStudy the instruments, scripts, and practical guides provided to ensure data quality during data collection: Before fieldwork, all team members should familiarize themselves with the characteristics of the instruments and the type of notes and supplementary resources to be generated in each case. Use the practical guides provided to prepare accordingly for the prioritized techniques.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Rapid Ethnographic Assessments"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/ethnographies.html#conducting-a-rea",
    "href": "data-collection/qualitative-methods/ethnographies.html#conducting-a-rea",
    "title": "Rapid Ethnographic Assessments",
    "section": "Conducting a REA",
    "text": "Conducting a REA\nStart by establishing a periodic check-in meetings with the team. During fieldwork, the team should meet periodically to share insights and determine if adjustments are needed for upcoming activities. These debriefing sessions are a key component of rapid ethnography and should be scheduled to ensure they are not overlooked. Some guiding Questions for Check-Ins include:\n\nWere there any unexpected or stressful events?\nHow well did you perform as an interviewer, moderator, observer, or note-taker?\nWhat kind of data did you collect?\nAre there any gaps that need follow-up?\nWhat adjustments would you make to the data collection guides or techniques?\nAre there new sites or contexts that need observation?\nTake notes during these meetings to track follow-up actions. These sessions are also a chance to discuss the project‚Äôs progress, address logistical challenges, and provide team members with training or advice on collection techniques.\n\n\n\n\n\n\n\nThe importance of check-ins\n\n\n\nDue to time constraints, it‚Äôs tempting to skip check-ins or leave discussions for after fieldwork is complete‚Äîthis is a mistake!\nSharing information in a group setting improves the quality of data collection and subsequent analysis. Make sure blocks of time are scheduled in advance for these meetings.\n\n\n\nPotential challenges during check-in meetings and fieldwork development\nThere are situations that could take place while checking-in with your field team that you should be prepared for. Here are some examples:\n\n\n\nTable¬†2: Challenges (and solutions) for REA check-ins\n\n\n\n\n\n\n\n\n\nChallenge\nPossible solutions\n\n\n\n\nOne team member usually dominates the conversation during debriefing meetings\nEmphasize the importance of teamwork during training. Rotate facilitators for check-in meetings to give everyone a chance to lead and ensure balanced participation. Foster an environment where each team member shares their insights.\n\n\nField teams may encounter challenges in implementing reflexivity practices when conducting REAs\nEncourage ongoing communication about biases that may influence data interpretation. Adapt meeting formats to fit fieldwork dynamics. Use flexible field diaries to document biases while accommodating the pace and nature of the research.\n\n\nThere is an evident lack of consistency in collecting information among the field team14\nImplement standardized data collection tools and provide clear instructions on using them. During training, discuss the context of the REA and emphasize shared best practices for approaching unstructured situations.\n\n\nSome team members need to pay more attention to the ethical aspects of the research\nAccelerated timelines can lead to ethical lapses. Emphasize the importance of ethical practices during training and ensure that all team members understand how to uphold ethical standards to protect participants and maintain data integrity.15\n\n\nTeam members say they are uncomfortable with the activities they witness\nDiscuss potential uncomfortable situations during training so that team members are prepared. If discomfort arises in the field, reassign less sensitive tasks to the team member and ensure they can perform their role without bias. Some team members may be unsuitable for certain projects due to personal beliefs or values.\n\n\nSome team members are trained and oriented quantitatively and unfamiliar with REAs and qualitative methods\nProvide thorough training on qualitative methods and the specific REA protocols. Ensure that all team members understand their roles in data collection, including note-taking and techniques. Rotate tasks to allow everyone to gain hands-on experience in different aspects of the REA process.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Rapid Ethnographic Assessments"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/ethnographies.html#after-a-rea",
    "href": "data-collection/qualitative-methods/ethnographies.html#after-a-rea",
    "title": "Rapid Ethnographic Assessments",
    "section": "After a REA",
    "text": "After a REA\nAfter data collection is complete, the team will have multiple sources of information, resources, and data from the research activities conducted in the field. To ensure proper management of these data, the following steps are suggested:\n\nAfter each data collection journey, each team member must ensure secure data collection storage according to the protocols for developing human subject research defined for the project. This includes: (i) transfer all recordings, photographs, and audiovisual material to the designated folders or storage platforms; (ii) Name files according to protocol; (iii) Anonymize and encrypt sensitive material to protect participants‚Äô privacy.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe lack of initial documentation products jeopardizes all the information processing components and, therefore, the data quality obtained.\n\n\n\nField notes are the primary data source for rapid ethnographies, so it is crucial to write and expand them as soon as possible, ideally within 24 hours of collection. To do this, integrate complementary material and resources, such as photographs, videos, maps, drawings, field diaries, etc., into the field notes, according to the protocols and quality criteria agreed upon with the team in the planning phase. Standardizing this process will save time during the systematization phases, prevent data loss, and facilitate the project leader‚Äôs review of the information.\n\n\n\n\n\n\n\nTip\n\n\n\nUse of photographic material Photographs are excellent complements to field notes. While they don‚Äôt need to be technically perfect, their primary function is to document key details and provide contextual information. However, always follow ethical guidelines to ensure participants‚Äô safety and confidentiality before taking photographs.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Rapid Ethnographic Assessments"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/ethnographies.html#digital-ethnographies",
    "href": "data-collection/qualitative-methods/ethnographies.html#digital-ethnographies",
    "title": "Rapid Ethnographic Assessments",
    "section": "Digital Ethnographies",
    "text": "Digital Ethnographies\nThe internet and digital technologies have accelerated the way people live, work, and relate to each other. In response to this social phenomenon, digital ethnography has emerged, a method that studies how people behave and interact in virtual environments, creating communities in digital spaces.16\nDigital ethnography adapts traditional ethnographic research techniques to studying online cultures and communities formed through communications mediated by a computer or mobile device. Instead of physically being in a community, researchers immerse themselves in online communities, forums, and social networking platforms. This way, the digital ethnographer observes and analyzes how individuals interact in these virtual spaces.17\nTechniques commonly used in digital ethnography are participant and non-participant observation, interviews, focus groups, and short surveys. To conduct a digital ethnography, follow the steps described above for performing a rapid ethnography. In addition, these considerations will be useful when adapting this method to a digital environment.18\n\n\n\nTable¬†3: Challenges (and solutions) for digital REAs\n\n\n\n\n\n\n\n\n\n\nChallenges\nDescription\nSolutions and recommendations\n\n\n\n\nEthical concerns: Informed consent\nObtaining informed consent from online participants can be challenging.\nWhenever possible, contact participants and share and socialize the informed consent. Some platforms may have specific terms of service, but it is essential to ensure that participants understand the research and agree to participate voluntarily.\n\n\nEthical concerns: anonymity and privacy\nProtecting the privacy and anonymity of participants in virtual environments is becoming more complex.\nSome platforms may have specific terms of service and data processing. Please review these terms. Researchers should tell participants if they intend to share information so that appropriate measures can be taken and the risk of participant identification minimized if the participant does not wish to be identified.\n\n\nDigital divide: Disparities in access\nNot everyone can access the internet or digital devices equally.\nBe mindful of access gaps that may lead to a biased sample, excluding certain demographic groups and limiting the generalizability of the findings.\n\n\nDigital divide: Low digital literacy\nParticipants may have no or low skills in handling technological devices, which may affect their online behaviors.\nThese differences should be considered when presenting findings during the data analysis and interpretation phases. When using WhatsApp, use the application‚Äôs various resources, such as voice notes, images, videos, and stickers, to convey information accurately, directly, and responsively to users with these difficulties.\n\n\nData quality and authenticity: Incorrect representation\nParticipants may present themselves differently online, creating a potential gap between their online persona and offline identity.\nNote that automated accounts, trolls, and fake profiles can complicate data interpretation and introduce biased information into the analysis. Always be aware of the potential for misrepresentation in digital environments.\n\n\nLow participation: Interaction at problematic times\nParticipants may need help participating in the activities due to schedule conflicts or lack of time availability.\nConduct baseline surveys to identify the schedules that best meet the population‚Äôs needs. From the beginning, underline clear rules about desirable times to receive messages.\n\n\nLow participation: Lack of trust\nParticipants may be distrustful of being artificially linked to virtual communities.\nMake prior contact with participants before linking them to virtual spaces to introduce them to the organization. Present again the purpose of the research. Sometimes, participants do not complete the informed consent form.\n\n\nInterpretation: Difficulty in interpreting content\nMessages shared by participants may present an interpretive challenge for the researcher, as the tone of the message is attributed by the reader.\nActively moderate to ask counter-questions to clarify the message. Promote clear communication in written messages. Record questions arising from participation in virtual scenarios from participant interactions in your field diary or field note.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Rapid Ethnographic Assessments"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/ethnographies.html#a-practical-guide-for-conducting-ethnographies",
    "href": "data-collection/qualitative-methods/ethnographies.html#a-practical-guide-for-conducting-ethnographies",
    "title": "Rapid Ethnographic Assessments",
    "section": "A Practical Guide for Conducting Ethnographies",
    "text": "A Practical Guide for Conducting Ethnographies\n\n\nUnable to display PDF file. Download instead.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Rapid Ethnographic Assessments"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/ethnographies.html#footnotes",
    "href": "data-collection/qualitative-methods/ethnographies.html#footnotes",
    "title": "Rapid Ethnographic Assessments",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPalazzo, L., Figueroa Gray, M., Pullmann, M., & Lewis, C. C. (2023). Rapid ethnographic assessment: A toolkit for essential partner perspectives on short timelines.‚Ü©Ô∏é\nSangaramoorthy, T., & Kroeger, K. (2020). Rapid ethnographic assessments: A practical approach and toolkit for collaborative community research.‚Ü©Ô∏é\nPalazzo, L., Figueroa Gray, M., Pullmann, M., & Lewis, C. C. (2023). Rapid ethnographic assessment: A toolkit for essential partner perspectives on short timelines; Sangaramoorthy, T., & Kroeger, K. (2020). Rapid ethnographic assessments: A practical approach and toolkit for collaborative community research.‚Ü©Ô∏é\nPalazzo, L., Figueroa Gray, M., Pullmann, M., & Lewis, C. C. (2023). Rapid ethnographic assessment: A toolkit for essential partner perspectives on short timelines.‚Ü©Ô∏é\nLesmes Guerrero, N., & Rojas, A. (2022). The role of leaders in the regularization process of Venezuelan migrants in Colombia.‚Ü©Ô∏é\nAkhter, S., Bashar, F., & Kamruzzaman, M. (2022). A rapid ethnographic assessment of cultural and social perceptions and practices about COVID-19 in Bangladesh: What the policymakers and program planners should know. Qualitative Health Research.‚Ü©Ô∏é\nSangaramoorthy, T., & Kroeger, K. (2020). Rapid ethnographic assessments: A practical approach and toolkit for collaborative community research.‚Ü©Ô∏é\nVindrola-Padros, C. (2021). Rapid ethnographies: A practical guide. Cambridge University Press.‚Ü©Ô∏é\nSangaramoorthy, T., & Kroeger, K. (2020). Rapid ethnographic assessments: A practical approach and toolkit for collaborative community research.‚Ü©Ô∏é\nPalazzo, L., Figueroa Gray, M., Pullmann, M., & Lewis, C. C. (2023). Rapid ethnographic assessment: A toolkit for essential partner perspectives on short timelines.‚Ü©Ô∏é\nVindrola-Padros, C. (2021). Rapid ethnographies: A practical guide. Cambridge University Press.‚Ü©Ô∏é\nTrotter, R. T., Needle, R. H., Goosby, E., Bates, C., & Singer, M. (2001). A methodological model for rapid assessment, response, and evaluation: The RARE program in public health. Field Methods, 13(2), 137-159.‚Ü©Ô∏é\nSmall, M. L., & McCrory, J. (2022). Qualitative literacy: A guide to evaluating ethnography and interview research.‚Ü©Ô∏é\nVindrola-Padros, C. (2021). Rapid ethnographies: A practical guide. Cambridge University Press.‚Ü©Ô∏é\nSangaramoorthy, T., & Kroeger, K. (2020). Rapid ethnographic assessments: A practical approach and toolkit for collaborative community research.‚Ü©Ô∏é\nKozinets, R. (2015). Netnography: Seeking understanding in a networked communication society. https://doi.org/10.1002/9781118767771.wbiedcs067‚Ü©Ô∏é\nQuestionPro. (2024, July 9). Digital ethnography: What it is, advantages and tools.‚Ü©Ô∏é\nEstella, A. (2024). Digital ethnography: A methodological monograph; QuestionPro. (2024, July 9). Digital ethnography: What it is, advantages and tools; Mosquera, M. A. (2008). From anthropological ethnography to virtual ethnography: Study of social relations mediated by the Internet. Fermentum: Revista Venezolana de Sociolog√≠a y Antropolog√≠a, 18(53), 532-549.‚Ü©Ô∏é",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Rapid Ethnographic Assessments"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/index.html",
    "href": "data-collection/qualitative-methods/index.html",
    "title": "About Qualitative Methods",
    "section": "",
    "text": "IPA‚Äôs mission to alleviate poverty requires more than just numbers; it requires a deep understanding of human experiences, behaviors, and systems that drive change. Qualitative methods help bridge the gap between data and the realities of individuals and communities that participate in studies.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "About Qualitative Methods"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/index.html#about-qualitative-research",
    "href": "data-collection/qualitative-methods/index.html#about-qualitative-research",
    "title": "About Qualitative Methods",
    "section": "About qualitative research",
    "text": "About qualitative research\nQualitative research methods are tools for exploring the how and why behind human actions, decisions, and experiences. Unlike quantitative approaches, which focus on numbers and statistical trends, qualitative methods delve into words, narratives, and visual data to uncover rich, contextual insights. These methods are particularly valuable for understanding the complexities of poverty and the effectiveness of interventions.\nAddressing poverty is about more than measuring income or access to resources; it is about understanding the underlying social, cultural, and psychological factors that perpetuate it. Qualitative methods help researchers:\n\nUnderstand Context: Explore how local norms, values, and practices influence the impact of programs.\nAmplify Voices: Capture the perspectives of marginalized groups who are often overlooked in traditional studies.\nDesign Better Interventions: Use insights to tailor programs that meet the specific needs of target populations.\nEvaluate Impact: Unpack the mechanisms behind why interventions succeed or fail.\n\n\n\n\n\n\n\nQualitative research at IPA\n\n\n\nAt IPA, qualitative research plays a vital role in designing, implementing, and evaluating programs aimed at reducing poverty. Here are some examples:\n\nFinancial Inclusion Studies: Interviews and focus groups with low-income individuals to understand barriers to saving, borrowing, and using financial products.\nEducation Interventions: Ethnographic studies in schools to explore teacher-student dynamics and identify strategies to improve learning outcomes.\nHealth Programs: In-depth interviews with caregivers and healthcare workers to assess the cultural acceptability of new health interventions.\nGender Dynamics: Case studies to explore how social norms and power structures affect women‚Äôs participation in economic programs.\n\n\n\nQualitative research does not stand alone‚Äîit complements quantitative methods to create a more holistic understanding. For example, qualitative data can explain anomalies in survey results, uncover mechanisms of change in randomized controlled trials or RCTs, and provide actionable insights for scaling successful programs.\n\n\n\n\nQualitative data collection in Tumaco, Colombia, photo by German Acosta",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "About Qualitative Methods"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/index.html#a-toolkit-of-qualitative-methods",
    "href": "data-collection/qualitative-methods/index.html#a-toolkit-of-qualitative-methods",
    "title": "About Qualitative Methods",
    "section": "A Toolkit of Qualitative Methods",
    "text": "A Toolkit of Qualitative Methods\nBy incorporating qualitative methods in research, IPA ensures that programs are not only evidence-based but also rooted in the realities of the people served. These methods help design innovative solutions, evaluate their effectiveness, and contribute to a world free of poverty.\n\n\n\n\n\n\nFocus Groups\n\n\n\nFocus groups involve guided discussions with a small group of participants to explore their perceptions, opinions, and attitudes towards a specific topic. This method is useful for generating diverse perspectives and understanding group dynamics. ‚Üí Learn more about focus groups\n\n\n\n\n\n\n\n\nQualitative Interviews\n\n\n\nInterviews involve one-on-one conversations between a researcher and a participant to explore individual experiences, beliefs, and motivations. This method allows for deep, personal insights and is flexible in terms of structure and depth. ‚Üí Learn more about qualitative interviews.\n\n\n\n\n\n\n\n\nObservations\n\n\n\nObservational research involves systematically watching and recording behaviors and interactions in their natural settings. This method provides insights into real-world practices and social interactions that other methods might not capture. ‚Üí Learn more about observations.\n\n\n\n\n\n\n\n\nRapid Ethnographies\n\n\n\nRapid ethnographies are short-term, intensive studies that use ethnographic techniques to gather in-depth information about a community or organization. This method is particularly useful for understanding cultural contexts and identifying key issues in a limited time. ‚Üí Learn more about rapid ethnographies.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "About Qualitative Methods"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html",
    "href": "data-collection/qualitative-methods/observations.html",
    "title": "Qualitative Observations",
    "section": "",
    "text": "Practical guidance for researchers and practitioners on planning and conducting qualitative observations in instances where researcher is an active participant and instances where the researcher is an observer of research subjects.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html#what-is-an-observation",
    "href": "data-collection/qualitative-methods/observations.html#what-is-an-observation",
    "title": "Qualitative Observations",
    "section": "What is an observation?",
    "text": "What is an observation?\nObservations are a qualitative research data collection technique that identifies traits, behaviors, and interactions of individuals and their contexts through the senses and interpretations of the researcher.1 The results of observations typically include detailed descriptions of what the field researcher observed, heard, and interpreted during research activities. Observation is therefore a subjective data collection technique that relies on observation, note-taking, and interpretation skills.2 Researchers can classify observations as either participant or non-participant, depending on how they conduct the fieldwork.\n\nParticipant Observation\nIn participant observation, researchers actively participate and interact in daily activities or specific contexts of the participants.3 This approach allows researchers to gain first-hand understanding of participants‚Äô daily practices, perspectives, and relationships. For example, a researcher might join community leaders on journeys to identify transport needs for accessing a social program.\n\n\nNon-Participant Observation\nIn non-participant observation, the researcher analyzes the behavior and relationships of participants without intervening, or intervening as little as possible, in the context or activity. Observers position themselves in a suitable place to listen and observe interactions without making comments or participating. For example, a researcher might conduct asynchronous remote observations through video recordings to identify how public school teachers reinforce gender stereotypes among students.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html#unstructured-vs.-structured-observations",
    "href": "data-collection/qualitative-methods/observations.html#unstructured-vs.-structured-observations",
    "title": "Qualitative Observations",
    "section": "Unstructured vs.¬†structured observations",
    "text": "Unstructured vs.¬†structured observations\nIn both participant and non-participant observations, the level of data structuring varies based on the fieldwork approach and the expected research outcomes:\n\nUnstructured Observations\nUnstructured observations use open-ended questions in data collection instruments to obtain highly descriptive and narrative information from the researcher. For example, researchers mapping the challenges of caring for children might attend a guided tour with facilitators who share their experiences and suggest improvements to support children‚Äôs well-being. This approach is suitable in certain context described below in Table¬†1.\n\n\n\nTable¬†1: When to consider unstructured observations 4\n\n\n\n\n\n\n\n\n\nContext\nDescription\n\n\n\n\nExploratory Research\nWhen the research topic is relatively unknown or when exploring a phenomenon without preconceived notions or rigid frameworks.\n\n\nComplex or Dynamic Settings\nIn environments where behaviors or interactions are complex and unpredictable, allowing the observer to capture a wide range of activities and interactions.\n\n\nContextual Understanding\nWhen aiming for a deep, holistic understanding of a setting, context, or culture without predefined categories.\n\n\nFlexibility Required\nWhen the researcher needs flexibility to follow interesting leads or unexpected occurrences during observation.\n\n\nDeep Immersion\nWhen deeply understanding the culture, practices, and perspectives of a group by experiencing them firsthand.\n\n\n*Hidden Behaviors\nWhen certain behaviors or interactions are not visible to outsiders, requiring the researcher to become part of the group.\n\n\nBuilding Trust\nWhen establishing trust and rapport with participants is essential for obtaining accurate data, best achieved through involvement.\n\n\nSensitive Topics\nWhen studying sensitive or private behaviors that participants may not openly share with an outsider.\n\n\n\n\n\n\n\n\nStructured Observations\nStructured observations use closed-ended questions to obtain specific information, resulting in short responses, counts, or responses on scales. For example, researchers might conduct asynchronous observations of recordings to identify how often teachers in public educational institutions reinforce gender stereotypes among young children. As with unstructured observations, structured ones are suitable in certain context described below in Table¬†2.\n\n\n\nTable¬†2: When to consider structured observations 5\n\n\n\n\n\n\n\n\n\nContext\nDescription\n\n\n\n\nFocused Research\nWhen the research aims to collect specific, consistent, and comparable data across different settings or subjects.\n\n\nQuantifiable Data Needed\nWhen the researcher needs to quantify behaviors or interactions, making it easier to analyze and compare results.\n\n\nPredefined Variables\nWhen the study has clear research questions or hypotheses, and the researcher knows exactly what behaviors or events to observe and record.\n\n\nConsistency Across Observers\nWhen multiple observers are involved, ensuring consistency in data collection to maintain reliability.\n\n\nObjectivity\nWhen maintaining a level of detachment is important to avoid influencing participants‚Äô behavior.\n\n\nEthical Concerns\nWhen participation might create ethical issues, such as situations where the researcher‚Äôs involvement could affect the outcome.\n\n\nNatural Behavior\nWhen observing natural, unaltered behavior is critical, and the presence of a participant observer might alter how people act.\n\n\nLarge Groups\nWhen studying large groups or public behaviors where direct participation isn‚Äôt feasible or necessary.\n\n\nComparative Studies\nWhen comparing behaviors across different groups or settings, requiring a consistent, detached observational stance.\n\n\n\n\n\n\n\n\nChoosing Between Structured and Unstructured Approaches\nParticipant observations often align with an unstructured approach, as the richness of these activities typically relies on the events, topics, and places that emerge during fieldwork. In contrast, structured observations often align with a non-participant approach, allowing research teams to standardize observations and minimize interference with the context. The choice between structured, unstructured, or mixed approaches depends on the specific needs of the study and its research design. In practice, these approaches are not mutually exclusive, and research teams can combine both during fieldwork.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html#planning-an-observation",
    "href": "data-collection/qualitative-methods/observations.html#planning-an-observation",
    "title": "Qualitative Observations",
    "section": "Planning an observation",
    "text": "Planning an observation\nPreparing for an observation involves several key steps: planning logistical aspects, structuring the team, understanding research objectives and observation instruments, and considering recommendations for implementation. Consider the following logistical elements and resources when planning:\n\n\n\n\n\n\nUnderstand What You Will Observe\n\n\n\n\n\nThe first step is to review the observation matrix or other relevant study materials, paying attention to the following aspects:\n\nObjectives of the observation.\nType of observation to be conducted.\nActivities to be carried out.\nPeople, places, and territories involved in the observation context.\nTiming and extent of the fieldwork.\n\n\n\n\n\n\n\n\n\n\nContact Allies, Leaders, and Key Actors\n\n\n\n\n\nFor participant observations, we recommend contacting local people who will accompany the observations, provide context, and facilitate the activities. For non-participant observations, allies or leaders can assist with logistics and offer valuable recommendations to the research team.\n\n\n\n\n\n\n\n\n\nReview Research Ethics and Safe Data Handling Protocols\n\n\n\n\n\nTwo aspects must be taken into account when approaching an observation ethically:\n\nEnsure that all necessary permissions have been obtained from communities and partners involved to conduct observations.\nObtain informed consent from participants, particularly when collecting audio, image, or video records.\n\n\n\n\n\n\n\n\n\n\nEnsure You Have an Observation Guide and Note-Taking Format\n\n\n\n\n\nDuring the planning phase, you should make sure that:\n\nYou understand how the observation will be conducted.\nYou have adapted the note-taking strategy to the context of the observation‚Äîdigital or paper.\nYou are clear on the expected outputs at the end of the field activities.\n\n\n\n\n\n\n\n\n\n\nVerify the Location and Conditions of the Observation Site\n\n\n\n\n\nSince observations often take place in participants‚Äô everyday environments, it‚Äôs essential to ensure that the site is comfortable and suitable for the observation.\n\n\n\n\nStructuring your team\nThe team members who develop the observations vary considerably according to the scope of the research and the resources available to achieve these objectives.\nObservation teams should ideally consist of at least two members who conduct observations together. Having multiple observers broadens the perspectives on the observed contexts and enhances the collection of resources, such as audiovisual material. However, if budget constraints limit the number of observers, this may affect the quality of the activity and the data collected. Additional participants might be necessary, especially in sensitive or high-security contexts. When structuring your team, consider how individual observer characteristics might influence data collection. The team should reflect on which aspects might introduce bias or cause resistance from the population being studied. Understanding the context and the population is crucial for this exercise.\n\n\nUnderstanding the Desirable Skills of the Field Team\nThe quality of data produced during qualitative research activities relates to the skills of the moderator and facilitator.6 These competencies, outlined in Table¬†3, enable teams to recognize and adequately represent the diversity within qualitative data. While these skills are typically developed through years of study and practice, field teams often consist of individuals from multidisciplinary backgrounds with varying levels of experience. Regardless of experience level, it is important to review and discuss these skills with your team to identify possible gaps and opportunities for improvement.\nCurrently, no standardized metrics exist to measure the prevalence of these skills among qualitative fieldwork moderators. Therefore, teams should view the skills below as a resource for reference and reflection. Ensuring that everyone on the team understands how these competencies contribute to engaging with the population is essential for collecting high-quality information.\n\n\n\nTable¬†3: Desirable skills in the field team\n\n\n\n\n\n\n\n\n\n\nAbility\nDescription\nWhy is this skill important?\n\n\n\n\nCognitive empathy\nThe field team‚Äôs ability to understand and communicate participants‚Äô situations from their perspectives, understanding how they see the world and their roles within it.\nAllows researchers to connect more deeply with participant‚Äôs realities and experiences. Helps to create a relationship of trust and respect with the participants. Seeks to avoid generalizations and stereotypes that may arise from preconceptions or external influences such as previous studies. Enhances understanding of participants‚Äô situations without resorting to pity.\n\n\nFollow-up\nThe field team‚Äôs ability to recognize when they need additional information to answer the questions initially posed and those that arise during the research process. This ability implies curiosity and a willingness to explore new issues or doubts that emerge as data collection progresses.\nIncreases the quality and robustness of data by allowing a more detailed exploration of the studied phenomenon. Contributes to obtaining deeper responses from participants. Enables exploration of emerging themes during data collection. Helps detect and validate patterns observed in the field.\n\n\nSelf-awareness and reflexivity\nThe field team‚Äôs ability to continuously reflect on how their presence, background, and assumptions influence data collection, interpretation, and analysis. This ongoing self-reflection ensures that the qualitative field team is mindful of its impact on the research process and the participants.\nHelps maintain ethics in the researcher-participant relationship. Facilitates understanding of personal limitations in connecting with participants. Aids in developing strategies to overcome communication barriers and create an environment where participants feel comfortable sharing sensitive information.\n\n\nHeterogeneity\nThe field team‚Äôs ability to represent and reflect the diversity within the group being studied. This skill involves recognizing and documenting the differences and variations among individuals or subgroups during qualitative research, typically applied during the data analysis phase.\nContributes to challenging generalized and simplistic patterns. Ensures that data reflect both common and atypical experiences. Demonstrates the field team‚Äôs ability to identify, recognize, and document heterogeneity in the population studied.\n\n\nPalpability\nThe field team‚Äôs ability to provide detailed descriptions in their field notes or diaries, making the data tangible and clear. This involves avoiding abstract descriptions and, instead, offering vivid accounts that allow the research team to visualize and understand participants‚Äô experiences and contexts.\nReliable findings are supported by specific details that clearly depict the events and situations studied. Helps to avoid abstraction in the data, grounding conclusions in concrete evidence.\n\n\n\n\n\n\n\n\nStudy the observation guide\nThe resources designed for observation help guide the observer, with varying degrees of structure depending on the research design. Observation guides are characterized by:\n\nAlignment with Research Objectives: The guide should directly relate to the goals of the research.\nIdentification of Key Characteristics: It should clearly identify the characteristics that need to be observed.\nAppropriate Scope: The guide should contain a manageable number of statements for the characteristics being observed, ensuring it is comprehensive but not overwhelming.\n\n\n\n\n\n\n\nTip for note-taking\n\n\n\nYou can use narrative resources, photographs, drawings, or examples to describe participants‚Äô interactions and reactions, as appropriate.\n\n\nFamiliarize yourself with the characteristics of the instruments and the type of information you expect to generate ‚Äì such as field notes, diaries, or activity logs. If you conduct the observation in a group, study the instruments together and agree on the general criteria you wish to observe.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html#conducting-an-observation",
    "href": "data-collection/qualitative-methods/observations.html#conducting-an-observation",
    "title": "Qualitative Observations",
    "section": "Conducting an observation",
    "text": "Conducting an observation\nBegin by introducing yourself and clearly explaining each team member‚Äôs role and the session‚Äôs general dynamics. If appropriate, read the informed consent, ensuring that all participants clearly understand the purpose of the activity and the potential risks and benefits of their involvement. Given the variety of formats collected ‚Äì audio, video, images ‚Äì informed consent must cover the full scope of the activity. Beyond the formal process of informed consent, make sure that the people, group, or community being observed know about the study and its aims.\nWhether conducting participant or non-participant observation, creating a comfortable and calm atmosphere is crucial to avoid discomfort and encourage spontaneity. Show interest, friendliness, and respect for all contributions, and maintain a curious and open-minded attitude, even when discussions are lengthy or opinions differ from your own. Finally, take notes on your observations or make audiovisual records according to your study‚Äôs requirements.\n\n\n\n\n\n\nTip for long-term observations\n\n\n\nIn prolonged observations, it is strongly recommended to supplement the notes of the research activities with field diaries.\n\n\n\nPotential challenges in a qualitative observation\nThere are situations that could take place during a qualitative interview that you should be prepared for. Here are some examples:\n\n\n\n\n\n\nDistrust on the part of the participants\n\n\n\n\n\nWhile the presence of observers inherently alters the participants‚Äô daily routine, it can also lead to increased distrust among some individuals. Provide a detailed introduction, clearly explaining the purpose of your visit and the activities involved. Reassure participants that they are not being evaluated or judged. Use electronic devices like a computer or cell phone to record key ideas, which may make participants feel less directly observed. If the observation was planned as non-participant, consider engaging in the activities to create a more comfortable environment.\n\n\n\n\n\n\n\n\n\nDifficulty in documenting all data\n\n\n\n\n\nBalancing observation with participation can make it challenging to document all relevant information that arises during the activity. Prioritize information that is most relevant to your research. Focus on the research objectives and follow the observation guide to maintain attention on key aspects. Take quick notes during the observation, using keywords or key concepts to capture essential details.\n\n\n\n\n\n\n\n\n\nObjectivity vs.¬†subjectivity\n\n\n\n\n\nSince observation is a subjective technique, it is crucial to differentiate between your interpretations and the descriptive elements of the environment. Write your diary or field note in three stages: Describe the environment and the participants‚Äô interactions. Detail your own participation in the space. Record reflections and interpretations that emerge from the observation. Remember that it is essential to keep a distance from the observation activities. You should participate in the spaces while maintaining an objective view from a distance.7\n\n\n\n\n\n\n\n\n\nGenerating agreement on assigned values\n\n\n\n\n\nUsing quantitative scales in observations can be challenging, as it requires consensus on what each observer should record in each response box. Establish clear observation agreements or use standardized rating rubrics to ensure consistency across observers. Start with an unstructured observation to identify any biases or concerns, then proceed to a structured observation based on those insights. When assigning values on a scale, provide a rationale in additional comments to clarify why a particular category was selected.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html#after-the-observation",
    "href": "data-collection/qualitative-methods/observations.html#after-the-observation",
    "title": "Qualitative Observations",
    "section": "After the observation",
    "text": "After the observation\nThe observation exercise concludes when you leave the participants‚Äô environment and return to a private space for reflection. Upon returning:\n\nReview Notes: Revisit your notes and elaborate on key observations.\nWrite Observation Notes: Include descriptions of the environment, participant interactions, your participation, and any reflections or interpretations that arise.\nSupplement Your Notes: Add photographs, drawings, or maps as necessary.\nDocument Preliminary Findings: In cases of extensive fieldwork, summarize preliminary findings in field diaries based on the various collection techniques used.\n\n\nPotential errors to avoid during data closure\nThe quality of note-taking is critical to the success of the collection technique; errors to avoid are:\n\nNeglecting Note-Taking: Avoid delaying note-taking; complete it during or immediately after the research activity.\nInadequate Refinement: Ensure that you refine and complete notes within 24 hours of the observation to capture accurate details and reflections.\nEthical Practices: Always adhere to ethical practices in data closure, particularly in protecting research subjects.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html#online-observations",
    "href": "data-collection/qualitative-methods/observations.html#online-observations",
    "title": "Qualitative Observations",
    "section": "Online observations",
    "text": "Online observations\nYou can conduct digitally mediated observations in two primary ways:\n\nVideo Conferencing: An observer connects remotely via a video conferencing platform, supported by an in-situ facilitator.\nAsynchronous Review: Observers analyze recordings of activities at a later time.\n\nBoth methods involve non-participant observation with a significant physical distance from the context being studied. This distance can lead to a disconnect between the observer and the participants‚Äô reality, which may limit the depth of reflections and interpretations. Despite these limitations, virtual observations can provide valuable insights, particularly in evaluating specific aspects of social programs. For instance, quantitative data can be collected by observing recorded class sessions, contributing to assessments of program impact.\nVirtual observations can also serve as an alternative when constraints arise in the following resources:\n\nTime: When scheduling conflicts or time zone differences make in-person observations impractical.\nBudget: When financial limitations restrict travel and accommodation expenses.\nAccessibility: When physical access to the observation site is challenging due to geographical or logistical barriers.\nHealth and Safety: When health risks, such as during a pandemic, prevent in-person interactions.\nEthical Concerns: When the presence of an observer might influence participants‚Äô behavior or when anonymity is crucial.\n\nBy leveraging digital tools and platforms, researchers can continue to gather valuable data while adapting to various constraints and ensuring the safety and well-being of both observers and participants.\n\n\n\nTable¬†4: Considerations and challenges in virtual observations\n\n\n\n\n\n\n\n\n\n\nChallenges\nDescription\nPossible solutions\n\n\n\n\nVolume of observations\nConducting around 500 observations posed significant logistical challenges, requiring large teams and extensive resources.\nUse audiovisual recordings to capture information, which the designated team can then review.\n\n\nObserver profile\nIf the project‚Äôs focus on socio-emotional skills necessitates observers with specific professional expertise.\nIdentify the appropriate observer profile, both professionally and personally. For example, select psychologists with experience in emotional intelligence and observation.\n\n\nObserver out of context\nRemote observations can detach observers from the context, particularly when others collect the data, leading to a lack of crucial contextual understanding.\nDevelop critical questions for the collection team, focusing on specific aspects like site characteristics, time of observation, participant attitudes, and unrecorded comments to help the observer understand the context. Promote dialog between the collection team and the observers to bridge contextual gaps.\n\n\nMultiple observers\nAsynchronous observation on a large scale often involves multiple observers with varying personal and professional backgrounds, potentially influencing the observation outcomes.\nCreate a structured observation guide that includes item descriptions and relevant examples to ensure consistency. For instance, provide examples of teacher actions related to emotional recognition, regulation, and validation. Conduct group observation exercises to align and verify observation standards.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html#a-practical-guide-for-conducting-observations",
    "href": "data-collection/qualitative-methods/observations.html#a-practical-guide-for-conducting-observations",
    "title": "Qualitative Observations",
    "section": "A Practical Guide for Conducting Observations",
    "text": "A Practical Guide for Conducting Observations\nWe adapted the content in this resource from IPA Colombia‚Äôs ‚ÄúPractical Guide for Conducting Observations.‚Äù This practical guide provides guidance to conduct observations to gather qualitative information in the context of public policy design and evaluation of social programs. The guide includes i) a definition of the technique and types of the collection, ii) the purpose for collecting information, iii) recommendations and steps to follow to apply the collection technique, and iv) applications of the technique in virtual context.\n\n\nUnable to display PDF file. Download instead.",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/qualitative-methods/observations.html#footnotes",
    "href": "data-collection/qualitative-methods/observations.html#footnotes",
    "title": "Qualitative Observations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDiaz, L. (2010). The observation. Faculty of Psychology, National Autonomous University of Mexico.‚Ü©Ô∏é\nDiaz, L. (2010). The observation (p.¬†9). Faculty of Psychology, National Autonomous University of Mexico.‚Ü©Ô∏é\nKawulich, B. B. (2005). Participant observation as a data collection method. Forum Qualitative Social Research, 6(2), Article 43.‚Ü©Ô∏é\nCreswell, J. W., and Poth, C. N. (2018). Qualitative inquiry and research design: Choosing among five approaches (4th ed.). SAGE Publications.‚Ü©Ô∏é\nCreswell, J. W., and Poth, C. N. (2018). Qualitative inquiry and research design: Choosing among five approaches (4th ed.). SAGE Publications.‚Ü©Ô∏é\nCreswell, J. W., and Poth, C. N. (2018). Qualitative inquiry and research design: Choosing among five approaches (4th ed.). SAGE Publications.‚Ü©Ô∏é\nRoller, M. R. (2022). Ethnography and the observation method: 15 articles on design, implementation, and uses. Research Design Review.‚Ü©Ô∏é",
    "crumbs": [
      "Data Collection",
      "Qualitative Methods",
      "Observations"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-data-security.html",
    "href": "data-collection/whatsapp/whatsapp-data-security.html",
    "title": "WhatsApp and Data Security",
    "section": "",
    "text": "This resource guides you through securing your data collected through Twilio by implementing robust encryption measures, ensuring your data‚Äôs confidentiality and integrity.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "WhatsApp and Data Security"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-data-security.html#encrypting-data-through-whatsapp-surveys",
    "href": "data-collection/whatsapp/whatsapp-data-security.html#encrypting-data-through-whatsapp-surveys",
    "title": "WhatsApp and Data Security",
    "section": "Encrypting Data through WhatsApp Surveys",
    "text": "Encrypting Data through WhatsApp Surveys\nEnsuring the security of sensitive data is crucial, especially when dealing with Personally Identifiable Information (PII). If your survey or workflow involves PII, follow these steps to implement encryption before publishing into Google Sheets:\n\nSet Up Encryption in Twilio\n\nNavigate to ‚ÄúFunctions/Services/Create Service‚Äù within your Twilio console\nCreate a service called ‚Äúencrypt‚Äù\n\n\n\n\nEncrypt Service\n\n\n\nClick on ‚ÄúAdd Function‚Äù and paste the following code:\n\n\n Twilio Encrypt Function\n\n\n\n\n\nEncrypt configuration snapshot\n\n\n\nModify the secret_key and store it securely on your computer. Modify line 50 as shown in the next image.\n\n\n\n\nChange encryption key\n\n\n\nAdjust the amount and names of the variables you intend to encrypt in lines 53 and 54 as shown in the previous image.\n\n\n\nCreate an Encryption Widget in Twilio Studio Flow\n\nJust before the function widget that publishes to Google Sheets, create a function widget for encryption.\n\n\n\n\nEncryption Widget Configuration\n\n\n\nSelect the service you created and set the environment as ‚ÄúUI‚Äù as shown in the previous image.\nChoose the name of the function you created and input the variables to be encrypted as function parameters.\n\n\n\n\nEncryption Widget Configuration Example\n\n\n\nUtilize the keys you want as variable names in the function code, using {widgets.widget_name.inbound.Body} as the values, as shown in the previous image.\n\n\n\nUpdate Google Sheets Publishing Function\n\nIn the function responsible for publishing to Google Sheets, replace {widgets.variable_name.inbound.Body} with {widgets.name_of_your_encryption_widget.parsed.name_of_the_variable}\n\n\n\n\nExample of the configuration of the publish function\n\n\nBy implementing these steps, you will protect all the PII within your survey or workflow, ensuring that sensitive information remains confidential throughout the data collection.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "WhatsApp and Data Security"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-data-security.html#decrypting-data-through-whatsapp-surveys",
    "href": "data-collection/whatsapp/whatsapp-data-security.html#decrypting-data-through-whatsapp-surveys",
    "title": "WhatsApp and Data Security",
    "section": "Decrypting Data through WhatsApp Surveys",
    "text": "Decrypting Data through WhatsApp Surveys\nAfter your encrypted data is sent to Google Sheets, you‚Äôll receive strings that may seem unintelligible. Follow these steps for decryption:\n\nDownload and Save Encrypted File\n\nDownload the encrypted file provided.\nSave it as a .csv file into a Boxcryptor/Cryptomator location for added security. It needs to be an ‚ÄúX:/‚Äù route.\n\n\n\nRun Decryption Code\n\nExecute the following Python code in the command prompt:\n\npython .\\csv_decryptor.py --encrypted_csv_path X:\\path\\to\\your\\csv\\demo.csv --list_of_columns_to_decrypt col1,col2 --secret_key your_secret_key1\n\nEnsure you replace the placeholders; here is where you can find the information needed:\n\n\nX:\\path\\to\\your\\csv\\demo.csv: The location of the encrypted dataset. Ensure this is in an ‚ÄúX:\" route for proper functionality.\ncol1,col2: The columns that contain encrypted data.\nyour_secret_key1: The secret key specified when creating the encryption function.\n\nThis process will generate a decrypted file version in the same path as the encrypted one. Following these clear steps ensures a smooth and secure decryption of your data.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "WhatsApp and Data Security"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-design.html",
    "href": "data-collection/whatsapp/whatsapp-design.html",
    "title": "Designing a WhatsApp Survey",
    "section": "",
    "text": "Learn how to design WhatsApp surveys using Twilio‚Äôs Content Template Builder, widgets, and functions. This guide covers creating message templates, building interactive flows, and implementing custom functionality for survey design.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Designing a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-design.html#content-template-builder-in-twilio",
    "href": "data-collection/whatsapp/whatsapp-design.html#content-template-builder-in-twilio",
    "title": "Designing a WhatsApp Survey",
    "section": "Content Template Builder in Twilio",
    "text": "Content Template Builder in Twilio\nThe Twilio Content Template Builder allows you to create and manage message templates across multiple Twilio-supported messaging channels, including WhatsApp. This tool provides a unified framework for designing structured messages, ensuring a consistent user experience across platforms while simplifying implementation. You can find this tool in the Twilio Console under the Messaging dropdown, then Senders, and finally Content Template Builder.\n\n\n\nCreate content template\n\n\n\nCreating a Content Template\nTo create a template, go to the Twilio Console and open the Content Template Builder Click **‚ÄúCreate New‚Äù‚Äù\nTemplates consist of fixed content and dynamic variables, allowing you to personalize messages with user-specific details such as names, order confirmations, or appointment times. When creating a template, you need to provide:\n\nA template name\nThe template language\nThe Content type\n\n\n\nMessage Content Types in Twilio\n\ntext: A standard text message containing only plain text.\nmedia: A message that includes images, videos, or audio files.\nquick-reply: A message with predefined response buttons for users to select from.\ncall-to-action: A message with buttons that direct users to a link or initiate a phone call.\nlist-picker: A structured message that presents users with a selectable list of options.\ncard: A message format that includes an image, title, description, and optional buttons for interaction.\ncard (WhatsApp Card): A WhatsApp-specific card message with rich media and interactive elements.\nauthentication: A message type designed for sending authentication codes.\ncatalog: A message format for showcasing multiple items in a structured catalog layout.\n\n\n\n\nNew content template\n\n\nWhatsApp requires all templates to go through a review process before use. Manage your templates using the Content Template Builder. Submit templates for WhatsApp approval, monitor approval statuses, delete, or duplicate your templates.\n\n\nAdding Interactive Buttons\nThe Content Template Builder makes it easy to add interactive buttons to your templates. Buttons can be configured to:\n\nSend quick replies with predefined responses\nDial a phone number\nRedirect users to a specific link\n\nThis example shows a quick-reply template that can have buttons for an intro message.\n\n\n\nCreate buttons for a content template\n\n\n\n\nUsing Approved Templates\nAfter approval, content templates can be used to send messages through the Twilio API. The API dynamically replaces template variables with actual values before sending the message. This example shows an approved WhatsApp message with interactive buttons:",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Designing a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-design.html#widgets",
    "href": "data-collection/whatsapp/whatsapp-design.html#widgets",
    "title": "Designing a WhatsApp Survey",
    "section": "Widgets",
    "text": "Widgets\nTo create a Twilio flow, follow these steps. First, navigate to the ‚ÄúStudio‚Äù section in the main navigation menu. Click on the ‚ÄúCreate a Flow‚Äù button or the ‚Äú+‚Äù icon to start creating a flow. Provide a name and an optional description for your flow. You will then be taken to the Flow Builder, where you can design your flow visually.\n\n\n\nCreate Twilio flow\n\n\nAfter the flow is created, you can drag widgets from the sidebar onto the canvas to build your flow, connecting them with connectors to define the conversation or logic flow. Customize each widget‚Äôs settings according to your requirements. In Twilio messaging, widgets are interactive elements or components that can be used to enhance the user experience and gather information from users. They provide a way to incorporate dynamic and engaging elements within the messaging flow. Widgets are typically used to collect user input, display options, or present interactive content.\nThe widget library is on the right side of the Flow Builder. Here you can see the four main categories:\n\nFlow control: These widgets allow you to implement conditional logic and control the flow of a conversation or process, enabling dynamic decision-making based on specific conditions or user inputs.\nVoice: These widgets facilitate the implementation of voice-based interactions, enabling functionalities such as making and receiving phone calls, playing audio prompts, gathering user input through speech recognition, and routing calls to different destinations based on conditions or logic.\nMessaging: These widgets enable the implementation of interactive messaging experiences, allowing functionalities like sending and receiving SMS, WhatsApp, and multimedia messages (MMS), collecting user input, displaying options, and handling conversations in a dynamic and engaging manner.\nTools and execute code: These widgets provide developers with tools to perform actions like making API requests, executing JavaScript code, accessing external resources, and integrating custom logic into their Twilio applications, allowing for enhanced customization and flexibility in the application‚Äôs behavior.\nConnect other products: These widgets facilitate seamless integration with external services and products, enabling developers to incorporate functionalities from other platforms, such as customer relationship management (CRM) systems, databases, or third-party APIs, into their Twilio applications, expanding the capabilities and possibilities of their communication workflows.\n\n\n\n\nTwilio Flow Builder Interface\n\n\n\nTrigger Widget\nIn Twilio Studio, the ‚ÄúTrigger‚Äù widget is a crucial component with specific functionality. The Trigger widget initiates the execution of a Twilio Studio flow. This is how it works:\n\nEntry Point: The Trigger widget serves as the entry point of your Studio flow. When an event or condition occurs that should start the flow, the Trigger widget is activated.\nEvent-Based Start: The Trigger widget is often configured to start the flow based on a specific event, such as an incoming call, message, or any other trigger you define.\nConnectivity to External Systems: The Trigger widget can also be connected to external systems or services using Twilio Functions or other Twilio products. For example, you might use a Trigger to initiate a flow when receiving a webhook.\nCustom Triggers: Additionally, you can use custom triggers to initiate a flow. This involves setting up conditions within your application or system that, when met, send a request to Twilio to trigger the associated Studio flow.\nHandling Incoming Communication: Common use cases involve using the Trigger widget to handle incoming communication, such as an incoming call or message, and then guiding the flow through subsequent widgets to provide responses or perform specific actions.\n\n\n\n\nTrigger Widget Example\n\n\n\n\nMessaging Widgets\nThere are two widgets in this section: the ‚ÄúSend Message‚Äù and ‚ÄúSend and Wait for Reply‚Äù widgets. These are used for displaying content in the survey.\n\nSend Message widget: This feature allows you to send outbound messages to recipients using various channels, including SMS, WhatsApp, or other messaging services. By specifying the content, it enables efficient and personalized communication with users. The Send Message widget in Twilio establishes a unidirectional connection, where the system sends a message to a recipient without waiting for or expecting a response. It is primarily used for one-way notifications, alerts, or information dissemination.\n\n\n\n\nSend Message Widget Example\n\n\n\nSend and Wait for Reply widget: This feature allows you to send a message to a recipient and pause the flow, waiting for their response. You can specify the message content, and the flow will wait for the recipient to reply before proceeding. This facilitates interactive and dynamic conversations involving back-and-forth communication with users. The Send and Wait for Reply widget establishes a bidirectional connection, enabling interactive conversations and allowing for dynamic communication between the system and the user.\n\n\n\n\nSend and Wait Widget Example\n\n\nIn summary, the key distinction lies in the type of connection established by these widgets. The Send Message widget operates with a unidirectional connection for one-way messaging, while the Send and Wait for Reply widget establishes a bidirectional connection to enable interactive conversations by awaiting and processing user responses.\n\n\nFlow Control Widgets\nThere are three widgets in this section: the ‚ÄúSplit Based On,‚Äù ‚ÄúSet Variable,‚Äù and ‚ÄúRun Subflow‚Äù widgets. These widgets allow you to implement conditional logic and control the flow of a conversation or process, enabling dynamic decision-making based on specific conditions or user inputs.\n\nSplit Based On: This widget enables you to dynamically split the flow of your application based on specific conditions or logic. It allows you to define multiple paths or branches within your flow, each with its own set of conditions and actions. When a message or event reaches the Split Based On widget, you can configure it to evaluate certain criteria, such as user input, variables, or data from previous steps in the flow. Based on the evaluation, the widget routes the flow to the appropriate branch that matches the condition. This widget is useful for implementing decision-making logic within your application. It enables you to create different paths or outcomes based on specific conditions, providing a way to customize the user experience, handle different scenarios, and route conversations accordingly. In this example, the system evaluates if the participant replied with a Yes or a No.\n\n\n\n\nSplit Based On Widget Example\n\n\n\nSet Variable: This widget enables the creation and assignment of variables within an application‚Äôs flow. By defining variable names and assigning values to them, developers can store and manipulate data throughout the conversation or process. These values can be constants, dynamic inputs from users or system data, or the result of calculations. The variables can then be referenced and utilized in subsequent steps of the flow, allowing for personalized and dynamic interactions. The Set Variable widget empowers developers to manage and track information, perform calculations, and make conditional decisions based on the stored values, enhancing the flexibility and customization of Twilio applications. In this case, you create a dummy variable that will take a value of 1 when the user replies with a Yes.\n\n\n\n\nSet Variable Widget Example\n\n\n\nRun Subflow: This widget enables the execution of separate and reusable subflows within the main flow of an application. By invoking a specific subflow and providing necessary input parameters, developers can modularize complex logic or functionality. This promotes code reusability, simplifies maintenance, and enhances flow organization. Upon executing the subflow, the control returns to the main flow, allowing for seamless integration and continuation of the application‚Äôs logic. The Run Subflow widget empowers developers to create more efficient and manageable Twilio applications by encapsulating and invoking reusable subflows as needed. In this case, you tell Twilio to send a user to a different flow (or survey) based on their answer.\n\n\n\n\nRun Subflow Widget Example\n\n\nIn summary, the Split Based On widget enables branching based on conditions, the Set Variable widget transitions linearly with variable assignments, and the Run Subflow widget transitions back to the main flow after executing the separate subflow. You can see how they look in the following image.\n\n\nFunction Widgets\n\nRun Function widget: This feature provides a powerful capability to execute custom code or serverless functions within your Twilio application. By utilizing this widget, you can perform specific actions, implement custom logic, and seamlessly integrate external functionality into your Twilio Studio flow. Whether you need to make API calls, perform calculations, access external resources, or implement complex business logic, the ‚ÄúRun Function‚Äù widget enhances the capabilities of your Twilio application, allowing you to extend its functionality and customize its behavior according to your unique requirements. This example uses a function that waits a few seconds between questions or messages. This is useful when you want to make the survey feel more natural and have a more human-like interaction with the user.\n\n\n\n\nRun Function Widget Example",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Designing a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-design.html#functions",
    "href": "data-collection/whatsapp/whatsapp-design.html#functions",
    "title": "Designing a WhatsApp Survey",
    "section": "Functions",
    "text": "Functions\nThere are three widgets in this section: the ‚ÄúRun Function,‚Äù ‚ÄúMake HTTP Request,‚Äù and ‚ÄúAdd TwiML Redirect‚Äù widgets. This section focuses only on the ‚ÄúRun Function‚Äù widget.\nTo leverage functions in Twilio, you can write them in JavaScript or any supported language of your choice. These functions can be triggered within a Twilio Studio flow using either the ‚ÄúHTTP Request‚Äù or ‚ÄúRun Function‚Äù widgets. By incorporating the ‚ÄúRun Function‚Äù widget, you can execute custom code or serverless functions, enabling you to perform specific actions and implement customized logic tailored to your Twilio application. This integration of functions seamlessly merges external functionality, enhancing the capabilities of your Twilio Studio flow and empowering you to create dynamic and powerful communication experiences.\n\nWait Function\nThe Wait function creates a pause in the flow execution, allowing for a more natural experience for the respondent. By introducing pauses between questions or messages, the Wait function gives respondents sufficient time to read and comprehend lengthy text in the survey. This helps to ensure a smooth and user-friendly survey interaction, allowing respondents to fully engage with the content and provide thoughtful responses. Let‚Äôs create this function in Twilio.\n\nNavigate to Functions and Assets: Go to the Twilio Console and locate the ‚ÄúFunctions and Assets‚Äù section.\nAccess the Services Tab: Within ‚ÄúFunctions and Assets,‚Äù click on the ‚ÄúServices‚Äù tab to manage your services.\n\n\n\n\nAccess the Services Tab\n\n\n\nCreate a Service: Click on the option to create a service. Name this service ‚Äúfunctions.‚Äù\n\n\n\n\nCreate Service\n\n\n\nCreate a Function: Inside the ‚Äúfunctions‚Äù service, select the option to create a function.\nName the Function: When prompted to name the function, enter ‚Äúwait.‚Äù\n\n\n\n\nName the Function\n\n\n\nPaste the Code: Within the ‚Äúwait‚Äù function, paste the following lines of code.\n\nexports.handler = function(context, event, callback) {\n  setTimeout(function() {\n    return callback(null, null);\n  }, 1000);\n};\n\nSave the Function: Save the function after pasting the code to preserve your changes.\nDependencies: Click on the dependency configuration and select the ‚ÄúNode.js v16‚Äù option.\nDeploy the Service: After the function is saved, deploy the entire service to make the changes effective.\n\nBy following these steps, you‚Äôll successfully create a Wait function inside the Twilio platform.\n\n\nGSheet Publish Function\nThe GSheet Publish function in Twilio facilitates the transmission of participant responses to a Google Sheet, enabling you to track and manage the collected data. By utilizing this function, the responses provided by participants are seamlessly sent to a designated Google Sheet, ensuring easy access to the data for analysis and tracking purposes. This functionality streamlines the process of gathering and organizing responses, allowing you to efficiently monitor and manage the collected information within a familiar spreadsheet format.\nThis function is covered in detail in the WhatsApp Survey Deploy guide.\n\n\nEncrypt Function\nThe Encrypt function is designed to encrypt variables, ensuring that sensitive information, such as personally identifiable information (PII), remains secure when data is stored or published. By encrypting the variables, the values are transformed into a secure format, protecting them from unauthorized access or exposure. This functionality is crucial for safeguarding sensitive information and maintaining the privacy and confidentiality of user data within the server. The Encrypt function provides a robust layer of security to prevent the public disclosure of sensitive data.\nThis function is covered in detail in the WhatsApp and Data Security guide.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Designing a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-collection/whatsapp/whatsapp-design.html#next-steps",
    "href": "data-collection/whatsapp/whatsapp-design.html#next-steps",
    "title": "Designing a WhatsApp Survey",
    "section": "Next Steps",
    "text": "Next Steps\nAfter designing your WhatsApp survey, you‚Äôll want to:\n\nDeploy your survey: Follow the WhatsApp Survey Deploy guide to launch your survey\nImplement security measures: Review the WhatsApp and Data Security guide for data protection\nMonitor responses: Set up data collection and monitoring systems\n\nFor more information about WhatsApp surveys, see the About WhatsApp Surveys overview.",
    "crumbs": [
      "Data Collection",
      "WhatsApp Surveys",
      "Designing a WhatsApp Survey"
    ]
  },
  {
    "objectID": "data-quality/accompany-surveyors.html",
    "href": "data-quality/accompany-surveyors.html",
    "title": "Accompany Surveyors",
    "section": "",
    "text": "This guide provides comprehensive guidelines on why, when, and how to accompany surveyors during data collection. It covers the importance of monitoring data quality, ensuring accuracy, understanding respondent reactions, and building accountability through structured observation and feedback.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Accompany Surveyors"
    ]
  },
  {
    "objectID": "data-quality/accompany-surveyors.html#why-accompany-surveyors",
    "href": "data-quality/accompany-surveyors.html#why-accompany-surveyors",
    "title": "Accompany Surveyors",
    "section": "Why Accompany Surveyors",
    "text": "Why Accompany Surveyors\nAccompanying surveyors is a key step in ensuring high-quality data collection. It allows researchers to monitor data quality, provide feedback, and detect any issues early in the survey process. By demonstrating commitment to accuracy, this practice sets expectations for the team and reinforces the importance of ethical and accurate data collection.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Accompany Surveyors"
    ]
  },
  {
    "objectID": "data-quality/accompany-surveyors.html#motivations-for-accompanying-surveyors",
    "href": "data-quality/accompany-surveyors.html#motivations-for-accompanying-surveyors",
    "title": "Accompany Surveyors",
    "section": "Motivations for Accompanying Surveyors",
    "text": "Motivations for Accompanying Surveyors\n\nEnsuring Accuracy: Observing surveyors helps verify that they ask questions correctly and consistently.\nUnderstanding Respondent Reactions: Researchers can identify whether respondents interpret questions as intended.\nDetecting Issues Early: Researchers can catch and correct mistakes or inconsistencies before they affect the dataset.\nImproving Surveyor Performance: Providing targeted feedback helps surveyors improve their skills.\nBuilding Accountability: Regular monitoring discourages falsification and reinforces ethical data collection.\nStrengthening Relationships: Engaging with surveyors demonstrates appreciation for their work and provides firsthand experience of their challenges.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Accompany Surveyors"
    ]
  },
  {
    "objectID": "data-quality/accompany-surveyors.html#how-to-accompany-surveyors",
    "href": "data-quality/accompany-surveyors.html#how-to-accompany-surveyors",
    "title": "Accompany Surveyors",
    "section": "How to Accompany Surveyors",
    "text": "How to Accompany Surveyors\n\nFull Accompaniment at the Start:\n\nDuring the initial days of the survey, observe each surveyor for an entire interview to ensure they understand and apply the questionnaire correctly.\n\nOngoing Partial Accompaniment:\n\nAs the survey progresses, limit accompanying to key sections of the interview such as two to three sections per visit.\n\nStructured Observation and Feedback:\n\nUse standardized forms to document observations and track surveyor performance.\nProvide immediate feedback and follow up on recurring mistakes.\n\nRandom and Scheduled Observations:\n\nMix planned and unannounced observations to get a realistic picture of surveyor performance.\n\nEscalation for Persistent Issues:\n\nAddress repeated mistakes with training or corrective actions.\nIf necessary, implement penalties or contract termination for continued poor performance.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Accompany Surveyors"
    ]
  },
  {
    "objectID": "data-quality/accompany-surveyors.html#monitoring-and-reporting",
    "href": "data-quality/accompany-surveyors.html#monitoring-and-reporting",
    "title": "Accompany Surveyors",
    "section": "Monitoring and Reporting",
    "text": "Monitoring and Reporting\n\nResearch Protocol: Accompany at least 10% of surveys to comply with best practices.\nSurvey Diary: Maintain a log of observations and feedback for continuous improvement.\nExternal Resources: See the paper version and SurveyCTO XLSForm of IPA‚Äôs accompaniment template for documentation and reporting.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Accompany Surveyors"
    ]
  },
  {
    "objectID": "data-quality/aea-registry.html",
    "href": "data-quality/aea-registry.html",
    "title": "AEA RCT Registry",
    "section": "",
    "text": "This guide explains the importance of the AEA RCT Registry for combating publication bias and ensuring transparency in social science research. It provides step-by-step instructions for creating accounts, registering trials, managing collaborators, and includes email templates for coordinating with Principal Investigators.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "AEA Registry"
    ]
  },
  {
    "objectID": "data-quality/aea-registry.html#what-is-the-aea-rct-registry-and-why-is-it-important",
    "href": "data-quality/aea-registry.html#what-is-the-aea-rct-registry-and-why-is-it-important",
    "title": "AEA RCT Registry",
    "section": "What is the AEA RCT Registry and why is it important?",
    "text": "What is the AEA RCT Registry and why is it important?\nThe AEA RCT Registry is a website for registering RCTs in the social sciences, established by the American Economics Association. It is important because it combats publication bias by providing a public record of the study. Pre-registration is a long-standing requirement in the medical community (e.g., for FDA‚ÄîFood and Drug Administration‚Äîapproval, for publication in medical journals), and it is increasingly becoming a focus in the social sciences as well.\n\nHow to create an account on the AEA RCT Registry website\n\nCreate an account at: AEA RCT Registry Signup.\nFill in your first and last name, organization name, contact number, email, and location.\nFollow the automated prompt to confirm your email.\nAfter confirmation, log in and register trials.\n\n\n\nHow to create a trial on behalf of someone else (i.e., you are not the ‚Äòlead‚Äô investigator or Primary Investigator)\n\nLog in to your AEA Registry account.\nCreate a trial from the main page by clicking on ‚ÄúRegister a Trial.‚Äù\nFill out the required fields (highlighted in red) and any other available study information.\nUpload a pre-analysis plan (optional, can be added later).\nWhen ready for the PI to review the trial, add them as a collaborator:\n\nFrom the draft trial main page, click the ‚ÄúManage Collaborator‚Äù button (orange, upper right corner).\nAdd collaborators using their registered email address.\n\n\n\n\nManaging Collaborators\n\nClick the ‚ÄúAdd Collaborator‚Äù field and enter each person‚Äôs registered email.\nScroll down and press ‚ÄúSave.‚Äù\nChange the email in the ‚ÄòPrimary Investigator‚Äô field from your email to the PI‚Äôs email using the dropdown menu.\nThe ‚ÄúTrial Information‚Äù page will update automatically with the assigned PI.\n\n\n\n\n\nWhat to do before clicking the ‚ÄúRegister Trial‚Äù button\n\nHave the PI review the draft registration for accuracy and completeness.\nMake any necessary modifications.\nClick the ‚ÄúRegister Trial‚Äù button to finalize registration.\n\n\n\nWhat happens once a trial reaches its ‚ÄúTrial End Date‚Äù in the registry?\n\nThe registry system uses the end date to remind PIs to update the trial and to publish hidden fields through an automated email.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "AEA Registry"
    ]
  },
  {
    "objectID": "data-quality/aea-registry.html#draft-email-templates-to-pis",
    "href": "data-quality/aea-registry.html#draft-email-templates-to-pis",
    "title": "AEA RCT Registry",
    "section": "Draft Email Templates to PIs",
    "text": "Draft Email Templates to PIs\n\n\n\n\n\n\nDraft email for a PI with an Existing Account\n\n\n\n\n\nDear [-PI name-], A draft registration has been created for your trial ‚Äú[-trial name-]‚Äù in the AEA RCT Registry (https://www.socialscienceregistry.org). For the draft, most of the required fields have been completed by reviewing your [-name of documents-], but there is still some information that is missing. Before the trial can be officially registered, you must add the following information: [-list what is missing for mandatory fields-]\nTo complete registration, log in and:\n\nReview the draft registration at [-direct link here-], checking for accuracy, and any additional information you may want to include.\nMake any necessary changes or modifications.\nClick the ‚ÄúRegister Trial‚Äù button to complete the registration.\n\nContact the research team if you have any questions. You are being added to this trial and should receive an automated email from the registry system.\nBest regards, [-Your name-]\n\n\n\n\n\n\n\n\n\nDraft email for a new PI\n\n\n\n\n\nDear [-PI name-], In an effort to register IPA projects in the American Economic Association‚Äôs registry for randomized controlled trials (https://www.socialscienceregistry.org), A registration has been drafted for your trial ‚Äú[-trial name-].‚Äù Before the trial can be officially registered and first reviewed by you, create an account at: https://www.socialscienceregistry.org/users/sign_up and then follow the prompt to confirm your email. After creating an account, you will be added as a collaborator to review and publish this trial.\nContact the research team if you have any questions.\nBest regards, [-Your name-]",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "AEA Registry"
    ]
  },
  {
    "objectID": "data-quality/bench-testing.html",
    "href": "data-quality/bench-testing.html",
    "title": "Bench Testing",
    "section": "",
    "text": "A guide to systematically test surveys before data collection to identify and fix skip logic issues, unclear wording, and missing response options.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Bench Testing"
    ]
  },
  {
    "objectID": "data-quality/bench-testing.html#what-is-bench-testing",
    "href": "data-quality/bench-testing.html#what-is-bench-testing",
    "title": "Bench Testing",
    "section": "What is Bench testing?",
    "text": "What is Bench testing?\nBench testing is the process of testing a survey to ensure that it functions correctly and as intended. This involves checking for skip logic issues, verifying that all questions display appropriately, and ensuring that the survey flows from start to finish. Bench testing helps identify and fix any potential problems before you administer the survey to respondents, thereby improving data quality and reliability.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Bench Testing"
    ]
  },
  {
    "objectID": "data-quality/bench-testing.html#bench-testing-priorities",
    "href": "data-quality/bench-testing.html#bench-testing-priorities",
    "title": "Bench Testing",
    "section": "Bench Testing Priorities",
    "text": "Bench Testing Priorities\nBench testing simulates different response scenarios to verify that your programmed survey works as intended. A thorough bench test involves systematically reviewing each question while considering these key aspects:\n\nSurvey Flow: Does the survey run from start to finish in all scenarios?\nResponse Options: Are all possible answers covered, including ‚Äúdon‚Äôt know‚Äù and ‚Äúrefuse‚Äù?\nNumeric Constraints: Are appropriate limits set for numeric inputs?\nSkip Logic: Does the branching logic work correctly? Are questions showing or hiding as intended?\nEnumerator Guidance: Are hints provided where needed to ensure consistent question delivery?\n\nThe technical review of SurveyCTO xlsform code should focus on:\n\nChoice Lists: Check for duplicate or overlapping options\nVariable Names: Ensure they are under 32 characters‚Äîshorter in repeat groups\nBest Practices: Verify compliance with SurveyCTO standards‚Äîrosters, audits, etc.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Bench Testing"
    ]
  },
  {
    "objectID": "data-quality/bench-testing.html#how-to-bench-test",
    "href": "data-quality/bench-testing.html#how-to-bench-test",
    "title": "Bench Testing",
    "section": "How to Bench Test",
    "text": "How to Bench Test\nEffective bench testing requires input from multiple team members and should be an ongoing process throughout survey development.\n\nIn the Office\nKey resources for office testing include:\n\nField managers\nResearch assistants\nProject managers\nAvailable enumerators\nFresh eyes from other teams\n\nUsing SurveyCTO‚Äôs ‚ÄúTest‚Äù feature allows you to:\n\nSimulate different scenarios without submitting data\nSave progress for various respondent profiles\nReview constraints and skip logic in real-time\nSwitch between testing and editing\n\n\n\nDuring Enumerator Training\nEnumerator training provides a crucial opportunity for final survey validation:\n\nQuality Feedback: Enumerators can identify:\n\nUnclear wording\nMissing answer options\nConstraint issues\nSkip logic problems\n\nRole-Playing Benefits:\n\nCreates realistic testing scenarios\nGenerates dummy data for analysis\nHelps standardize question delivery\nFamiliarizes enumerators with the survey flow\n\n\nAs enumerators inevitably find issues and bugs within the survey, be sure to write these down and track all changes. One solution is to include the ‚Äúcomments‚Äù field type in SurveyCTO. SurveyCTO forms allow for a ‚Äúcomments‚Äù field type, where enumerators can add comments to survey questions while administering a survey. In order to use this, include a ‚Äúcomments‚Äù field type, and name the variable. See the example below:\n\n\n\nComments field type\n\n\nThis field type creates an option for every question in the survey where enumerators can comment on the question. This can include survey logic issues, missing options, confusing wording, or any other issues an enumerator has with a question. It appears next to other options as a pencil while administering the survey:\n \nThe system exports comments as media files, similar to audio audits or images. The files appear in a directory under the variable name you assigned to the comments field type. For this example, they would appear under the variable q_comments. IPA‚Äôs Data Management System includes the command -ipacheckcomment-, which compiles these files into a summary sheet in the outputs. There is also a user-written Stata command named -remedia- that helps rename and move media files.\nThis feature helps catch issues while the survey is being administered to quickly change them during data collection. Once you have dummy data from enumerator training or other types of bench testing, it is important to run your entire data flow to test your checks and inspect the format of your data before the survey enters the field.\nThis feature helps catch issues while enumerators administer the survey to change them during data collection. After you have dummy data from enumerator training or other bench testing, run your entire data flow. Test your checks and inspect the format of your data before the survey enters the field.\nThis feature helps catch issues while enumerators administer the survey to change them during data collection. After you have dummy data from enumerator training or other bench testing, run your entire data flow. Test your checks and inspect the format of your data before the survey enters the field.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Bench Testing"
    ]
  },
  {
    "objectID": "data-quality/data-integrity.html",
    "href": "data-quality/data-integrity.html",
    "title": "Data Integrity",
    "section": "",
    "text": "This guide covers procedures for ensuring data integrity in research, including double data entry methods, data backup strategies, and secure file storage using Box.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Integrity"
    ]
  },
  {
    "objectID": "data-quality/data-integrity.html#what-is-data-integrity",
    "href": "data-quality/data-integrity.html#what-is-data-integrity",
    "title": "Data Integrity",
    "section": "What is Data Integrity?",
    "text": "What is Data Integrity?\nEnsuring data integrity and security is crucial for high-quality research. This involves verifying data accuracy through Double Entry, protecting against loss by Maintaining Data Backups, and ensuring secure access by Storing Files on Box. Adopting these best practices minimizes errors, preserves data, and promotes reliability.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Integrity"
    ]
  },
  {
    "objectID": "data-quality/data-integrity.html#double-entry",
    "href": "data-quality/data-integrity.html#double-entry",
    "title": "Data Integrity",
    "section": "Double Entry",
    "text": "Double Entry\nDefinition: Double entry is a method where researchers enter data twice independently and then compare the entries to detect discrepancies.\nSteps:\n\nInitial Data Entry: Enumerators or data clerks enter data from source documents into the database.\nSecond Independent Entry: A different person re-enters the same data into a separate database.\nComparison and Validation: Researchers compare the two datasets to identify inconsistencies.\nError Resolution: Staff review discrepancies and correct them using the original source.\n\nWhy it Matters:\n\nReduces human error in manual data entry.\nEnsures higher accuracy and reliability of data.\nEssential for sensitive data where precision is critical.\n\nBest Practices:\n\nUse automated comparison tools to speed up validation.\nTrain staff on accurate data entry techniques.\nImplement a tracking system to monitor common errors.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Integrity"
    ]
  },
  {
    "objectID": "data-quality/data-integrity.html#maintaining-data-backups",
    "href": "data-quality/data-integrity.html#maintaining-data-backups",
    "title": "Data Integrity",
    "section": "Maintaining Data Backups",
    "text": "Maintaining Data Backups\nDefinition: A backup is a copy of data stored to protect against accidental loss, corruption, or hardware failure.\nSteps:\n\nAutomated Backups: Schedule automatic backups at regular intervals.\nMultiple Storage Locations: Store backups in at least two locations, such as cloud storage or an external drive.\nVersion Control: Maintain different versions of files to recover previous states.\nRegular Testing: Periodically restore files to ensure backups are functional.\n\nWhy it Matters:\n\nPrevents loss of critical research data.\nEnsures continuity in case of system failure.\nProtects against accidental deletions and cyber threats.\n\nBest Practices:\n\nUse encrypted backups to secure sensitive data.\nFollow the 3-2-1 rule: Keep three copies of data, on two different media, with one copy offsite.\nDocument backup procedures for easy recovery.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Integrity"
    ]
  },
  {
    "objectID": "data-quality/data-integrity.html#storing-files-on-box",
    "href": "data-quality/data-integrity.html#storing-files-on-box",
    "title": "Data Integrity",
    "section": "Storing Files on Box",
    "text": "Storing Files on Box\nDefinition: Box is a secure cloud storage platform that allows teams to store, access, and share files efficiently.\nSteps:\n\nUpload Files to Box: Organize files into appropriate folders.\nSet Access Permissions: Restrict access based on roles, such as view-only, edit, or admin.\nEnable File Versioning: Maintain previous versions to track changes.\nUse Encryption and Two-Factor Authentication: Enhance data security with two-factor authentication (2FA).\n\nWhy it Matters:\n\nProvides secure storage with controlled access.\nFacilitates team collaboration with real-time file sharing.\nEnsures compliance with data protection policies.\n\nBest Practices:\n\nAudit file permissions and access logs.\nUse Box Sync or Box Drive for seamless integration with local storage.\nSet up automated alerts for unauthorized access attempts.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Data Integrity"
    ]
  },
  {
    "objectID": "data-quality/data-security-protocol.html",
    "href": "data-quality/data-security-protocol.html",
    "title": "Data Security Protocol",
    "section": "",
    "text": "Comprehensive guidelines for protecting both hardcopy and softcopy data throughout research projects, including protocols for personally identifiable information (PII), encryption standards, password security, and device management."
  },
  {
    "objectID": "data-quality/data-security-protocol.html#what-is-the-data-security-protocol",
    "href": "data-quality/data-security-protocol.html#what-is-the-data-security-protocol",
    "title": "Data Security Protocol",
    "section": "What is the Data Security Protocol?",
    "text": "What is the Data Security Protocol?\nThe data security protocol is a set of guidelines and practices designed to protect data from unauthorized access, corruption, or loss.\n\nImportance of Data Security\n\nCompliance: Required by Institutional Review Boards and donors.\nPrevention of Data Loss: Avoid costly data breaches or losses.\nRespect for Respondents: Protect respondents‚Äô privacy and confidentiality.\nData Integrity: Ensure data quality for sharing and publication."
  },
  {
    "objectID": "data-quality/data-security-protocol.html#personally-identifiable-information-pii",
    "href": "data-quality/data-security-protocol.html#personally-identifiable-information-pii",
    "title": "Data Security Protocol",
    "section": "Personally Identifiable Information (PII)",
    "text": "Personally Identifiable Information (PII)\nPII refers to any datapoint or combination of datapoints that can identify an individual or household with reasonable certainty. Examples of PII can include:\n\nName\nGPS coordinates\nAddress\nCombinations of demographic data, village name, birth date, gender, occupation in small communities\n\n\n\n\n\n\n\nData Protection Requirements\n\n\n\n\n\nPII must be protected using the following protocols:\n\nEncryption: Apply encryption at every stage of the data lifecycle: collection, transmission, storage.\nSeparation: Separate PII from research data as soon as possible.\nOngoing Protection: Protect PII retained after study closure.\nExclusion: Exclude PII from microdata publications like the AEA registry.\n\n\n\n\n\nEncryption: Apply encryption at every stage of the data lifecycle: collection, transmission, storage.\nSeparation: Separate PII from research data as soon as possible.\nOngoing Protection: Protect PII retained after study closure.\nExclusion: Exclude PII from microdata publications like the AEA registry. :::\nEncryption: Apply encryption at every stage of the data lifecycle: collection, transmission, storage.\nSeparation: Separate PII from research data as soon as possible.\nOngoing Protection: Protect PII retained after study closure.\nExclusion: Exclude PII from microdata publications like the AEA registry. :::\n\nIn terms of sharing PII, this kind of information may only be shared under the following conditions:\n\nThe recipient is named or referenced in the informed consent.\nThe recipient is included in the approved research protocol.\nSecure methods are used for sharing.\n\n\nSensitive Data\nSensitive data is information where a loss of confidentiality, integrity, or availability could result in serious, severe, or catastrophic consequences.\n\n\nSensitivity Levels\n\nLevel 1: Non-confidential.\nLevel 2: Contains PII, but no material harm.\nLevel 3: Contains PII and could cause material harm.\nLevel 4: High-risk confidential data.\n\n\n\n\n\n\n\nExamples of PII (Based on 45 CFR 164.514 - HIPAA Standard)\n\n\n\n\n\nBased on 45 CFR 164.514 HIPAA Standard, some examples of PII may include:\n\nNames\nGeographic subdivisions smaller than a state/province\nDates directly related to an individual (e.g., birth date, graduation date, marriage date)\nTelephone numbers, fax numbers, email addresses\nSocial security numbers, medical record numbers, health plan beneficiary numbers\nAccount numbers, certificate or license numbers, vehicle identifiers, device identifiers\nWeb URLs, IP addresses, biometric identifiers like fingerprints\nFull-face photos or any other unique identifying number, characteristic, or code"
  },
  {
    "objectID": "data-quality/data-security-protocol.html#access-to-pii",
    "href": "data-quality/data-security-protocol.html#access-to-pii",
    "title": "Data Security Protocol",
    "section": "Access to PII",
    "text": "Access to PII\n\nOnly individuals approved in the IRB submission can access PII.\nApproved individuals must complete human subjects protection training like CITI Program certification."
  },
  {
    "objectID": "data-quality/data-security-protocol.html#password-security",
    "href": "data-quality/data-security-protocol.html#password-security",
    "title": "Data Security Protocol",
    "section": "Password Security",
    "text": "Password Security\n\nCreating a Strong Password\n\nAt least 10 characters.\nUse a mix of numbers, symbols, uppercase, and lowercase letters.\nAvoid repetition, dictionary words, usernames, pronouns, IDs, and predefined sequences.\n\n\n\nPassword Best Practices\n\nNever share passwords through email.\nUse a secure password manager.\nUse a complex password consistently across the project.\n\nExample: The New Hampshire training was awesome because of the data security presentation in 2024 ‚Üí TnhtwabotDSPi2024!\nThis creates a 17-character password with uppercase, lowercase, numbers, and symbols while being memorable through the sentence structure."
  },
  {
    "objectID": "data-quality/data-security-protocol.html#data-protection-summary",
    "href": "data-quality/data-security-protocol.html#data-protection-summary",
    "title": "Data Security Protocol",
    "section": "Data Protection Summary",
    "text": "Data Protection Summary\nEnsure your data is always:\n\nProduced through a clear step in your data flow.\nSaved to a logical and secure location.\nPassword-protected with a strong password.\nEncrypted if it contains PII or sensitive data.\nBacked up in multiple locations."
  },
  {
    "objectID": "data-quality/data-security-protocol.html#data-flow-faq",
    "href": "data-quality/data-security-protocol.html#data-flow-faq",
    "title": "Data Security Protocol",
    "section": "Data Flow FAQ",
    "text": "Data Flow FAQ\n\nWhen to Keep Data Secure\nSecure data whenever sensitive information is linked to PII. Raw data containing PII must be encrypted and stored securely.\n\n\nWhen and How to Remove PII\n\nRemove PII as soon as possible.\nCollaborate with your team to determine who will remove PII and when.\nStata Tip: Use the lookfor command to identify PII."
  },
  {
    "objectID": "data-quality/data-security-protocol.html#device-security",
    "href": "data-quality/data-security-protocol.html#device-security",
    "title": "Data Security Protocol",
    "section": "Device Security",
    "text": "Device Security\n\nGeneral Device Security\n\nConsult the Data Coordinator for recommended devices.\nEncrypt all sensitive data.\nPassword-protect devices.\nPhysically secure and label devices.\n\n\n\nField Device Management\n\nImplement check-in/out systems for devices.\nTrain staff not to broadcast locations or activities.\nEnsure signed device liability forms.\nMaintain charging schedules and fire safety precautions.\nAssign responsibility for nightly device checks.\nLabel devices with QR code stickers.\nStore netbooks in protective casings.\n\n\n\nLaptop and Personal Device Security\n\nUse IPA‚Äôs encryption software for PII.\nAvoid unapproved programs.\nRun antivirus scans frequently.\nBackup data securely using tools like Box.\nWork computers only: No personal files.\n\n\n\nLaptop/Netbook Security in the Field\n\nUser Accounts\n\nCreate two accounts:\nAdmin account: Full rights.\nEnumerator account: Limited access.\n\n\n\nFunctionality Restrictions\n\nSet accurate date and time.\nDisable internet access if unnecessary.\nDisable audio, video, and USB ports unless required for data extraction.\n\n\n\nSoftware and Protection\n\nInstall antivirus software.\nEncrypt sensitive survey data using tools like Cryptomator.\nUpdate and back up data regularly."
  },
  {
    "objectID": "data-quality/data-security-protocol.html#data-backup-and-storage",
    "href": "data-quality/data-security-protocol.html#data-backup-and-storage",
    "title": "Data Security Protocol",
    "section": "Data Backup and Storage",
    "text": "Data Backup and Storage\n\nUse central/supervisor computer backups.\nLocal backups: External hard drives with automated backup software like CrashPlan.\nCloud backups: IPA‚Äôs encrypted cloud storage through Box with additional encryption using Cryptomator."
  },
  {
    "objectID": "data-quality/data-security-protocol.html#conclusion",
    "href": "data-quality/data-security-protocol.html#conclusion",
    "title": "Data Security Protocol",
    "section": "Conclusion",
    "text": "Conclusion\nBy adhering to these data security protocols, we ensure compliance, safeguard respondent privacy, and maintain the integrity of research data.\nBy adhering to these data security protocols, organizations ensure compliance, safeguard respondent privacy, and maintain the integrity of research data."
  },
  {
    "objectID": "data-quality/how-to-dms-stata.html",
    "href": "data-quality/how-to-dms-stata.html",
    "title": "Guide to IPA‚Äôs Data Management System",
    "section": "",
    "text": "Learn to set up and use IPA‚Äôs Data Management System for comprehensive data quality monitoring. This hands-on guide walks you through configuring high-frequency checks, tracking survey progress, and conducting back check comparisons using the ipacheck package in Stata. You‚Äôll work with real survey data to practice identifying outliers, duplicates, and logical inconsistencies while building automated quality control workflows.\nThis guide provides hands-on exercises to help you master IPA‚Äôs Data Management System tools in Stata. You‚Äôll learn to set up and run three key components: High-Frequency Checks, Survey Tracking, and Back Check comparison."
  },
  {
    "objectID": "data-quality/how-to-dms-stata.html#what-youll-learn",
    "href": "data-quality/how-to-dms-stata.html#what-youll-learn",
    "title": "Guide to IPA‚Äôs Data Management System",
    "section": "What You‚Äôll Learn",
    "text": "What You‚Äôll Learn\n\nInstall and configure the ipacheck package\nSet up automated data quality checks for survey data\nTrack survey progress and identify missing submissions\nCompare survey responses with back check data\nGenerate reports for field teams and data managers"
  },
  {
    "objectID": "data-quality/how-to-dms-stata.html#prerequisites",
    "href": "data-quality/how-to-dms-stata.html#prerequisites",
    "title": "Guide to IPA‚Äôs Data Management System",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nStata 17.0 or later\nBasic familiarity with Stata commands and do-files\nUnderstanding of survey data collection workflows\nAccess to Excel for configuration files"
  },
  {
    "objectID": "data-quality/how-to-dms-stata.html#installation",
    "href": "data-quality/how-to-dms-stata.html#installation",
    "title": "Guide to IPA‚Äôs Data Management System",
    "section": "Installation",
    "text": "Installation\nThe ipacheck package bundles user-written Stata commands for running high-frequency checks at IPA. Install the package by opening Stata and running:\n* install ipacheck\nnet install ipacheck, all replace from(\"https://raw.githubusercontent.com/PovertyAction/high-frequency-checks/master\")\nipacheck update\n\n* after initial installation ipacheck can be updated at any time via\nipacheck update"
  },
  {
    "objectID": "data-quality/how-to-dms-stata.html#exercise-overview",
    "href": "data-quality/how-to-dms-stata.html#exercise-overview",
    "title": "Guide to IPA‚Äôs Data Management System",
    "section": "Exercise Overview",
    "text": "Exercise Overview\nThis exercise demonstrates how to use the ipacheck package with real survey data from a previous IPA project. The dataset includes:\n\nA simple household survey collected using SurveyCTO\nDe-identified data with all PII removed\nAnonymized GPS coordinates\nBoth survey and back check datasets for comparison"
  },
  {
    "objectID": "data-quality/how-to-dms-stata.html#instructions",
    "href": "data-quality/how-to-dms-stata.html#instructions",
    "title": "Guide to IPA‚Äôs Data Management System",
    "section": "Instructions",
    "text": "Instructions\n\nSection 1: Package Overview\nTo download the exercise, start by using the ipacheck command. This will initialize a folder structure, readme files, all the input sheets, and the data for the exercise:\n* To download the exercise folder and files to current directory\nipacheck new, exercise\n\n* To download the exercise folder and files to a different directory, use the folder option as follows\nipacheck new, exercise folder(\"folder path here\")\n\n\n\n\n\n\nNote\n\n\n\nThe program may take longer to fetch files with slow internet connections.\n\n\nThere are also a few additional utility functions included in the ipacheck package including:\n\nipacheck update - downloads the updated ado files directly from GitHub whenever IPA HQ releases an update so you don‚Äôt have to go through the above Installation process again.\nipacheck version - lists the current installed versions of the user-written commands to verify you have the latest version.\n\nAfter downloading the exercise files, navigate to the proper directory and examine the HFC files. Start with the 3_checks/1_inputs folder, which contains these files:\n\nhfc_inputs.xlsm - input Excel fileÕæ inside you‚Äôll find a convenient form for configuring some of the HFC commands.\ncorrections.xlsm - replacement Excel fileÕæ it is a running list of edits/corrections based on HFC outputsÕæ these replacements can be automatically added to your workflow using ipacheckcorrections.\nspecifyrecode.xlsm - other specify input Excel fileÕæ it is a running list instruction for recoding other specify valuesÕæ these instructions will be automatically added to your workflow using ipacheckspecifyrecode.\n\nNow go back to the main folders.\n\n0_master.do - master dofileÕæ it runs all other do-files 2_dofiles folder\n\nNow navigate to 2_dofiles.\n\n1_globals.do - globals do-file; it contains globals for setting up the Data Management System.\n3_prepsurvey.do - data preparation do-file; it contains sample code for preparing the raw Survey data for High-Frequency Checks (HFCs).\n4_checksurvey.do - HFC do-file; it contains code for running checks on the Survey dataset.\n5_prepbc.do - data preparation do-file; it contains sample code for preparing the raw Back Check data for Back Check Comparison (HFCs).\n6_checkbc.do - Back Check do-file; it contains code for running checks on the back check dataset as well as running the back check comparison.\n\n\n\nSection 2: Configure Main Inputs\nThis section walks you through setting up your DMS configuration.\n\nOpen the Globals File\nNavigate to the 2_dofiles folder and open the 1_globals.do file. This file configures the High-Frequency Checks. Each section includes descriptions for each global variable. Some globals contain default values that you may need to modify.\nConfigure the following sections:\n\n\nRun/Turn Off Specific Checks\nSet the value ‚Äú1‚Äù for each check you want to run. The value ‚Äú1‚Äù tells Stata to run and configure that check. To turn off a specific check, change the corresponding global value to 0. This exercise runs almost all available checks, so activate all checks as shown in the code below.\n\n**# Run/Turn Off Specific Checks\n*------------------------------------------------------------------------------*\n\n\n    * NB: Edit this section: Change the value to 0 to turn off specific checks\n\n    gl run_corrections      1   //  Apply corrections\n    gl run_specifyrecode    1   //  Recode other specify\n    gl run_version      1   //  Check for outdated survey form versions\n    gl run_ids      1   //  Check Survey ID for duplicates\n    gl run_dups     1   //  Check other Survey variables for duplicates\n    gl run_missing      1   //  Check variable missingness\n    gl run_outlier      1   //  Check numeric variables for outliers\n        gl run_constraints  1   //  Check numeric variables for constraint violations\n    gl run_logic        1   //  Check variables for logical violations\n    gl run_specify      1   //  Check for other specify values\n    gl run_comments     1   //  Collate and output field comments\n    gl run_textaudit    1   //  Check Survey duration using text audit data\n    gl run_timeuse      1   //  Check active survey hours using text audit data\n    gl run_surveydb     1   //  Create survey Dashboard\n    gl run_enumdb       1   //  Create enumerator Dashboard\n    gl run_tracksurvey  1   //  Report on survey progress\n    gl run_trackbc      0   //  Report on Back check progress\n    gl run_bc       1   //  Back Check comparison\n\n\nInput Files\nThis section specifies file paths for the hfc_inputs, corrections, and recode specify input files. The default values are correct for this exercise, so no changes are needed.\n\n\nDatasets\nThis section specifies file paths and filenames for input datasets required by various commands, including:\n\nRaw, prepped and de-duplicated survey datasets\nBack check datasets\nCollated field comments and text audit datasets\n\nThe default values for rawsurvey, preppedsurvey, checkedsurvey, rawbc, preppedbc and checkedbc are correct for this exercise.\nYou will be configuring the ipatracksurveys command using a master dataset which has information for each target respondent. Navigate to the 4_data/2_survey folder and examine the household_preloads.xlsx file. This file will be used as the master dataset when comparing the surveys submitted to the individuals to be interviewed. After viewing this file, go back to the 1_globals.do file and indicate the value \"${cwd}/4_data/2_survey/household_preloads.xlsx\" for the global mastersurvey. Note that the master data can also be a dta or csv file.\n\n\n\n\n\n\nNote\n\n\n\ntrackingsurvey is an alternative to mastersurvey. However, trackingsurvey expects a dataset than contains targets for specified groups. eg. community, district, treatment_status. It is more suitable for census-type surveys when we don‚Äôt have a pre-selected list of respondents. Explore the dataset respondent_targets.xlsx in 4_data/2_survey to see an example of a trackingsurvey dataset.\n\n\nIgnore the masterbc & trackingbc globals for now. When will configure those when reviewing the back check sections.\nNow configure the commentsdata and textauditdata global by yourself. Note that these files currently do not exist and will be created when you run your DMS. It is recommended to set the file path for these files to \"${cwd}/4_data/2_survey\". Also, ensure to use file names that will be easier to identify later on.\nCompare your do-file to the code below and make any adjustments if necessary.\n\n* Datasets\n*------------------------------------------------------------------------------*\n\n    * NB: Edit this section: Change filenames if necessary\n\n    gl rawsurvey            \"${cwd}/4_data/2_survey/household_survey.dta\"\n    gl preppedsurvey        \"${cwd}/4_data/2_survey/household_survey_prepped.dta\"\n    gl checkedsurvey        \"${cwd}/4_data/2_survey/household_survey_checked.dta\"\n\n    gl mastersurvey         \"${cwd}/4_data/2_survey/household_preloads.xlsx\"\n    gl trackingsurvey       \"\"\n\n    gl rawbc            \"${cwd}/4_data/3_backcheck/household_backcheck.dta\"\n    gl preppedbc            \"${cwd}/4_data/3_backcheck/household_backcheck_prepped.dta\"\n    gl checkedbc            \"${cwd}/4_data/3_backcheck/household_backcheck_checked.dta\"\n\n    gl masterbc             \"${cwd}/4_data/3_backcheck/household_backcheck_preloads.xlsx\"\n    gl trackingbc           \"\"\n\n    gl commentsdata         \"${cwd}/4_data/2_survey/comments_data.dta\"\n    gl textauditdata        \"${cwd}/4_data/2_survey/textaudit_data.dta\"\n\n\nSurveyCTO media folder\nThis section specifies the folder that contains the SurveyCTO generated comments and text audits files. You do not need to make changes here since the default values are correct for this exercise.\n\n\nOutput Date Folder\nThis section creates a subfolder in 3_checks/2_outputs for the current date that the DMS is running. This section should never be edited.\n\n\nOutput files\nThis section specifies the output files from the various checks. The output files include the duplicates, corrections, recode specify, hfc, text audit, timeuse, survey dashboard, enumerator dashboard and survey tracking outputs. You do not need to make changes here since the default values are correct for this exercise.\n\n\nAdmin variables\nThis section specifies the global administrative variables for the DMS. These are variables that are used in various sections of the DMS. These include the key, survey id, enumerator, date, team, starttime, duration, form version, field comments, text audit, consent and outcome variables as well as variables to keep in all checks. The default variables specified for some of the globals are correct for this check, however, we need to configure the remaining additional checks as follows.\n* Admin variables\n*------------------------------------------------------------------------------*\n\n    * NB: Edit this section: Change variable names if necessary.\n\n    * Required Variables:\n\n    gl key              \"key\"\n    gl id               \"hhid\"\n    gl enum             \"a_enum_id\"\n    gl enumteam             \"a_team_id\"\n    gl bcer             \"a_bcer_id\"\n    gl bcerteam         \"a_team_id\"\n    gl date             \"starttime\"\n\n    * Optional Variables:\n\n    gl team             \"a_team_name\"\n    gl starttime            \"starttime\"\n    gl duration         \"duration\"\n    gl formversion          \"formdef_version\"\n    gl fieldcomments        \"field_comments\"\n    gl textaudit            \"text_audit\"\n    gl keepvars             \"a_kg a_district\"\n    gl consent          \"c_consent\"\n    gl outcome          \"\"\n\n\n\n\n\n\nNote\n\n\n\nThe survey has no outcome variable so we will leave the outcome global blank.\n\n\n\n\nResponse values\nThis section specifies values that represent specified response values in the survey. You will be making some adjustments to this section to reflect the dataset. The values for cons_vals are correct and should not be modified. However, you remember from the previous section that the survey does not include an outcome variable, therefore set the value for the outc_vals global to empty ie. \"\".\nAfter reviewing the data, we realize that the survey programming allowed the values -999, 999 and .999 as don;t know values for numeric variables and -888, 888 and .888 as ‚Äúrefuse to answer‚Äù values for numeric variables.\nTo account for this, modify the values of the dk_num and ref_num globals as follows:\n* Missing values\n*------------------------------------------------------------------------------*\n\n    * NB: Edit this section: Change values if necessary.\n\n    gl cons_vals            \"1\"\n    gl outc_vals            \"1\"\n    gl dk_num           \"-999 999 .999\"\n    gl dk_str           \"-999\"\n    gl ref_num          \"-888 888 .888\"\n    gl ref_str          \"-888\"\n\n\n\nSection 3: Configure additional inputs\nThe second half of 1_globals.do specify settings that are peculiar to individual checks in the DMS. Review the remaining sections carefully. Note that for many of these sections, you will not need to make any additional changes. After review, make changes to the following sections:\n\nipacheckdups: Export variable duplicates\nSet the value of the global dp_vars as \"resp_contact\" to flag duplicates in phone numbers.\n\n\nipachecksurveydb: Export Survey Dashboard\nSet the value of the global sv_by as \"a_district\" to disaggregate the survey dashboard by the values of the a_district variable.\n\n\nipatracksurvey: Export tracking sheet\nSet the values of the following globals as follows:\n* ipatracksurvey: Export tracking sheet\n*------------------------------------------------------------------------------*\n\n    gl tr_by            \"a_kg\"\n    gl tr_target            \"\"\n    gl tr_keepmaster        \"a_district\"\n    gl tr_keeptracking      \"\"\n    gl tr_keepsurvey        \"submissiondate c_consent\"\n    gl tr_summaryonly       \"\"\n    gl tr_workbooks         \"workbooks\"\n        gl tr_save                      \"\"\n        gl tr_surveyok          \"surveyok\"\nSave the 1_globals.do file.\nNavigate to the 3_checks/1_inputs folder and open the hfc_inputs.xlsm file in Excel. This is where you‚Äôll do the additional configuration for the ipacheckoutliers, ipacheckconstraints, ipachecklogic, ipacheckspecify and enumstats for ipacheckenumdb.\nExplore the dataset 4_data/2_survey/household_survey.dta and identify the variables for the following sheets:\n\n\nother specify\nThe ‚Äúother specify‚Äù sheet specifies the parent and child variables for other specify responses. Explore the household dataset and identify the parent and child variables. The parent variables are the control variables which trigger the other specify question. Note that all the child variables in this dataset include _osp in their variable names.\nUpon completion, navigate to 0_archive and open the file hfc_inputs_exercise.xlsm and compare your work with the sheet \"other specify\".\n\n\noutliers\nThe ‚Äúoutliers‚Äù sheet specifies the numeric variables to check for outliers. These are usually numeric variables that are not constrained or have a wide constraint range. Review the help file for ipacheckoutliers to understand what is required for by, method, multiplier, combine and the keep columns.\nUpon completion, navigate to 0_archive and open the file hfc_inputs_exercise.xlsm and compare your work with the sheet \"outliers\".\n\n\nconstraints\nThe constraints sheet specifies the numeric variables to check for constraint violations. These are usually numeric variables which you want to check for soft constraints. Review the help file for ipacheckconstraints to understand what is required for hard_min, soft_min, soft_max, hard_max and the keep columns.\nUpon completion, navigate to 0_archive and open the file hfc_inputs_exercise.xlsm and compare your work with the sheet \"constraints\".\n\n\nlogic\nThe logic sheet specifies the variables to check for logical inconsistencies in survey variables. ipachecklogic checks for logic violations using logical statements as indicated in the assert column of the input sheet. Review the help file for ipachecklogic to understand what is required for assert, if_condition and the keep columns.\nThe scope of this exercise does not cover the ipachecklogic so you are free to exclude it from this exercise. You may also read the help file for ipachecklogic for examples of how to use this check.\n\n\nenumstats\nThe enumstats sheet specifies the variables and additional setup for the enumstats option of ipacheckenumdb. The variables specified are usually numeric variables for which we want to check for variations among enumerators. For instance, we may want to examine how enumerators are reporting household member counts or total household income. Review the help file for ipacheckenumdb to understand what is required for the columns min, mean, show_mean_as, median, sd, max and combine. Please note that for this section, there is no single correct or wrong answer, however, to help guide the exercise complete the enumstats sheet as follows:\n\n\n\nTable¬†1: enumstats sheet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nlabel\nmin\nmean\nshow_mean_as\nmedian\nsd\nmax\ncombine\n\n\n\n\ngpsaccuracy\nGPS Accuracy\nyes\nyes\n\n\n\nyes\n\n\n\nf_hhm_cnt\n(FA2) Based on the definition that I just read to you, how many members are in y\nyes\nyes\n\nyes\n\nyes\n\n\n\ni_phone_num\n(I01) How many household members own a working mobile phone?\nyes\n\n\nyes\n\nyes\n\n\n\nj_land_yn\n(J01) Does your household own any land?\n\nyes\npercentage\n\n\n\n\n\n\nk_own_yn\n(K01) Does your household own any livestock?\n\nyes\npercentage\n\n\n\n\n\n\n\n\n\n\nYou may also include other variables that you find useful.\nSave the hfc_inputs.xlsm file\nNavigate back to 2_dofiles folder and open up the 3_prepsurvey.do file. This do-file contains sections of sample codes that may be needed to prepare your data before running the checks. These include dropping variables, destringing variables, generating a short key if needed, generating date variables from default datetime variables and dropping observations that were started before specified dates.\nReview this do-file carefully and make edits where necessary.\nThe data includes temporary variables that were added for SurveyCTO programming purposes only and therefore do not contain any useful information. Luckily these variables are prefixed by tmp_. Drop these variables by adding tmp_* to the list of variables to drop in the ‚Äúdrop unwanted variables‚Äù section.\nThe data also contains ‚Äúcalculate fields‚Äù that were imported as strings instead of numeric variables. Include *_yn* to the ‚Äúdestring numeric‚Äù variables sections to destring all variables that have the `_yn‚Äô in their variable names. Explore the data and include more variables to destring.\nSave the 3_prepsurvey.do file.\n\n\n\nSection 4: Run and Review the Output\nNavigate back to the main folder and open the 0_master.do file. Before running your checks, make sure that your current working directory is the directory that contains 0_master.do file.\nComment out the section of the code that runs the import do-file, the back check prep and the back check comparison.\n**# Survey 1\n*------------------------------------------------------------------------------*\n\n    do \"2_dofiles/1_globals.do\"         // globals do-file\n    * do \"2_dofiles/2_import_wbnp_hhs_2021.do\"  // import do-file\n    do \"2_dofiles/3_prepsurvey.do\"          // prep survey do-file\n    do \"2_dofiles/4_checksurvey.do\"         // check survey do-file\n    * do \"2_dofiles/5_prepbc.do\"            // prep back check do-file\n    * do \"2_dofiles/6_checkbc.do\"           // check survey do-file\nRun the entire do file.\nAfter running the do-file, you should get the following error:\nworksheet duplicates corrections not found\nTo resolve this error, we need to set-up the correction sheet. Navigate to 3_checks\\1_inputs and open the file corrections.xlsm. Read the instructions and create 2 correction sheets on this workbook using the following information:\n\ncorrection sheet for duplicates\n\n\n\nduplicates_corrections\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe SurveyCTO generated key variable or another type of key variable should be used as ID when resolving duplicates. Although you can use the key and hence the same sheet to resolve other duplicates, it is recommended to create a separate sheet using the Survey ID since that will facilitate communication when resolving these issues.\n\n\n\n\ncorrection sheet for other issues\n\n\n\nother_corrections\n\n\nRun the 0_master.do again. Once it runs successfully, you should have a list of output files in the date subfolder the in 3_checks/2_outputs folder. Open each file and inspect its content.\nNavigate to the 04_checks/02_outputs folder and open the various output files and answer the following questions:\n\nHow many interviews have been conducted?\nAre we missing any submissions that we planned?\nIs everyone using the latest form version?\nHow many Survey ID duplicates are there in the dataset?\nHow many variables have all missing values that shouldn‚Äôt be missing?\nHow many outliers are flagged? Do any values appear to be nonsensical? What should be done?\nDo you see any specify options that could be recoded?\nWhich 3 enumerators have the lowest survey productivity scores.\nWhich 5 enumerators have the highest ‚Äúdon‚Äôt know‚Äù response rate\nAre there any useful comments? What should be done?\nWhat time period during the day are the enumerators most active on the field.\n\n\n\n\nSection 5: Make Replacements & Recode Other Specify\nTry adding some corrections yourself using the instructions in corrections.xlsm that was set-up earlier. Navigate to 0_archive and open the corrections_exercise.xlsm for guidance on how to make some of the corrections.\nRe-run the DMS to effect the corrections. Review the corrections logfile in 3_checks/2_outputs to verify that the changes were effected.\nNavigate to 3_checks/2_outputs to inspect the new output files. Review the hfc_output file and include your corrections in the corrections.xlsm file.\n\n\n\n\n\n\nNote\n\n\n\nOther specify recodes cannot be done in the corrections.xlsm file and must be done in the specifyrecode.xlsm file.\n\n\nOpen the specifyrecode.xlsm file (in 3_checks/1_inputs folder). Read the Stata help file for ipacheckspecifyrecode to understand how to use the specifyrecode.xlsm file.\nNavigate to 0_archive and open the specifyrecode_exercise.xlsm for guidance on how to recode other specify values.\nRe-run the DMS to effect the recodes. Review the other specify recode logfile in 3_checks/2_outputs to verify that the changes were effected.\n\n\nSection 6: Set-up back check comparison\nNavigate back to 2_dofiles folder and open up the 5_prepbc.do file. This do-file contains sections of sample codes that may be needed to prepare your back check dataset as well as running back check comparison. Before we edit this do-file, open the 0_globals.do file and make the following changes to the section titled ipabcstats: Compare survey and back check data\n*ipabcstats: Compare survey and back check data\n*----------------------------------------------\n    * Comparsion variables\n    * --------------------\n\n    gl bs_t1vars        \"e_hhh_relig e_hhh_native e_hhh_howlong h_rooms h_electricity h_cook_fuel h_toilet i_radio i_mosquito_net j_land_yn \"\n\n    gl bs_t2vars        \"f_hhm_cnt\"\n\n    gl bs_t3vars        \"j_land_size j_land_value m_mon_exp m_mon_inc\"\n\n    * Enumerator Checks\n    * -----------------\n\n    gl bs_showid        \"30%\"\n\n    * Stability Checks\n    * ----------------\n\n    gl bs_ttest         \"f_hhm_cnt j_land_size j_land_value m_mon_exp m_mon_inc\"\n\n    gl bs_prtest        \"\"\n\n    gl bs_signrank      \"\"\n\n    gl bs_level         \"95\"\n\n    * Reliability Checks\n        * ------------------\n\n    gl bs_reliability   \"j_land_size j_land_value m_mon_exp m_mon_inc\"\n\n    * Comparison datasets\n    * -------------------\n\n    gl bs_keepsurvey    \"\"\n\n    gl bs_keepbc        \"\"\n\n    gl bs_full      \"\"\n\n    gl bs_nolabel       \"nolabel\"\n\n    gl bs_dta       \"\"\n\n\n    * Options\n    * -------\n\n    gl bs_okrange       \"e_hhh_howlong[-1, 1], j_land_size[-5%, 5%], j_land_value [-5%, 5%], m_mon_exp [-10%, 10%], m_mon_inc [-10%, 10%]\"\n\n    gl bs_nodiffnum     \"\"\n\n    gl bs_nodiffstr     \"\"\n\n    gl bs_excludenum    \"${dk_num} ${ref_num}\"\n\n    gl bs_excludestr    \"${dk_str} ${ref_str}\"\n\n    gl bs_exclusemissing    \"excludemissing\"\n\n    gl bs_lower         \"lower\"\n\n    gl bs_upper         \"\"\n\n    gl bs_nosymbol      \"nosymbol\"\n\n    gl bs_trim      \"trim\"\nNavigate back to 2_dofiles folder and open up the 3_prepbc.do file. This do-file contains sections of sample codes that may be needed to prepare your back check data before comparison. These include dropping variables, destringing variables, generating a short key if needed, generating date variables from default datetime variables and dropping observations that were started before specified dates.\nReview this do-file and make any changes that are necessary.\nNavigate to 3_checks/1_inputs and re-open the corrections.xlsm file. Add a new sheet bc duplicates corrections using a_bcer_id as the Enumerator ‚ÄúID Variable (required)‚Äù and key as the ‚ÄúID Variable (required)‚Äù.\nRe-activate the section of the code that runs the import do-file, the back check prep and the back check comparison.\n**# Survey 1\n*------------------------------------------------------------------------------*\n\n    do \"2_dofiles/1_globals.do\"         // globals do-file\n    do \"2_dofiles/2_import_wbnp_hhs_2021.do\"    // import do-file\n    do \"2_dofiles/3_prepsurvey.do\"          // prep survey do-file\n    do \"2_dofiles/4_checksurvey.do\"         // check survey do-file\n    do \"2_dofiles/5_prepbc.do\"          // prep back check do-file\n    do \"2_dofiles/6_checkbc.do\"         // check survey do-file\nRe-run the 0_master.do file again and review the output."
  },
  {
    "objectID": "data-quality/pilot-survey.html",
    "href": "data-quality/pilot-survey.html",
    "title": "Pilot Survey",
    "section": "",
    "text": "Guidelines on why, when, and how to pilot your survey instrument. Covers the iterative process of piloting, including different phases, sample selection, timeline planning, and using pilot results to refine survey content, improve flow, and test protocols for high-quality data collection.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#why-pilot",
    "href": "data-quality/pilot-survey.html#why-pilot",
    "title": "Pilot Survey",
    "section": "Why Pilot?",
    "text": "Why Pilot?\nAs one of IPA‚Äôs Required Research Protocols ‚Äì or MMDs ‚Äì piloting is an essential step for which there are no shortcuts. Teams can use piloting for different purposes and it has numerous benefits:\n\nDesigning a New Survey or Important Module:\n\nAcquire knowledge of unknown phenomena/concepts and answer questions about the overall survey design.\n\nRefining the Content of the Survey and Improving Survey Flow:\n\nAdjust question wording and order.\nIdentify non-essential questions to drop from the final version.\nIdentify common responses for pre-encoding.\nIdentify translation mistakes for correction.\nPay attention to how experienced surveyors ask questions and incorporate their ‚Äúconversation‚Äù into the question text for a smooth flow.\nVerify that respondents maintain interest.\nVerify that interviewers and respondents feel comfortable with the interview flow.\nIdentify potential hitches for the interviewer, such as needing to repeat questions, correct misinterpretations, or record volunteered information.\n\nTesting Your Data Flow:\n\nVerify your survey programming works well using High-Frequency Checks.\nVerify your preloading from a baseline survey works well.\nCollect metadata.\nCheck the completeness of your pilot dataset.\nTest your back-check strategy and dataset using the Back Check Manual.\n\nTesting Your Field Protocols:\n\nVerify the planned timing of interviews fits respondents.\nCheck the existing infrastructure, such as access to electricity, internet, cell phone networks, and other resources.\nEnsure your sampling, replacement protocols, and tracking strategy work well in the field.\nVerify your existing sampling frame is up to date or identify the time needed for making a listing.\nIdentify the duration and accuracy of capturing GPS locations with your devices.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#pilot-sample",
    "href": "data-quality/pilot-survey.html#pilot-sample",
    "title": "Pilot Survey",
    "section": "Pilot Sample",
    "text": "Pilot Sample\nWhen selecting your piloting sample, ensure that:\n\nThe piloting area is outside your sample area. Pilot the survey with respondents who are outside your sample but in similar neighborhoods/backgrounds, such as people from the same village but in different households or people from neighboring villages.\nEvery question in every section gets piloted several times. Pilot with at least 30 households if the survey contains many new questions or you administer it in a new context, or 15 households if you used it before in a similar context.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#pilot-phases",
    "href": "data-quality/pilot-survey.html#pilot-phases",
    "title": "Pilot Survey",
    "section": "Pilot Phases",
    "text": "Pilot Phases\nPiloting is an iterative process, meaning teams should pilot surveys each time they undergo significant changes. The complete piloting process consists of three phases:\n\nPre-pilot Phase:\n\nConducted when designing a new survey or important sections.\nStart with focus groups and qualitative interviews.\nThis phase should happen early in the piloting process‚Äîa few months before data collection‚Äîand helps with survey design and demystifying important phenomena.\n\n1st Pilot Phase:\n\nConducted before translating the survey.\nPilot the survey instrument yourself with a few experienced surveyors who speak the local language or dialect.\nThis phase improves the content of the survey.\n\n2nd Pilot Phase:\n\nConducted after finalizing, programming, and translating the survey.\nPilot the final translated instrument with a larger group of surveyors.\nThis phase should happen before enumerator training and survey launch and checks the survey flow, programming, data flow, and data quality checks.\n\n\nAt a minimum, every survey must go through the second phase of piloting before its launch, but not all surveys have to go through all three phases. This table shows simple rules for piloting:\n\nPiloting Steps by Survey Type\n\n\n\n\n\n\n\n\nSurvey Type\nPre-pilot\n1st Pilot\n2nd Pilot\n\n\n\n\nBrand new survey / important module\n+\n+\n+\n\n\nExisting well-designed survey in a new location/language\n\n+\n+\n\n\nExisting well-designed survey in the same location/language, such as endline with small changes\n\n\n+",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#pilot-timeline",
    "href": "data-quality/pilot-survey.html#pilot-timeline",
    "title": "Pilot Survey",
    "section": "Pilot Timeline",
    "text": "Pilot Timeline\nThe amount of time needed to pilot the survey depends on how long and how new the survey is. The newer the survey, the more piloting it requires.\n\nIf designing your survey from scratch, start pre-piloting five to six months before the survey launch.\nIf adapting an existing survey to a new context/language, start the first pilot three to four months before the launch.\nIf making small changes to an existing survey for the same context/language, start the second phase four to five weeks before the launch.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#pilot-length",
    "href": "data-quality/pilot-survey.html#pilot-length",
    "title": "Pilot Survey",
    "section": "Pilot Length",
    "text": "Pilot Length\nPiloting is an iterative process, and there is no best practice regarding the required number of iterations. Piloting is over when:\n\nThe survey flows well.\nNo significant changes remain.\nRespondents can understand all the questions.\n\nHowever, consider that some questions, such as hypotheticals, will always require respondents to struggle, even if the question is clearly understood.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#pilot-results",
    "href": "data-quality/pilot-survey.html#pilot-results",
    "title": "Pilot Survey",
    "section": "Pilot Results",
    "text": "Pilot Results\nTo use pilot results:\n\nTake detailed notes during piloting.\nUse these notes to make changes to your questionnaire, translation, or programming.\nDiscuss important points during enumerator training and include them in the survey manual.\n\n\nOn the Content of the Survey\n\nReword or re-translate questions that are mistakenly interpreted by respondents.\nDrop or rephrase questions if responses show no variation.\nChange question order if respondents can guess the general hypothesis, as this may indicate biased or leading questions.\n\n\n\nOn the Flow of the Survey\n\nShorten the survey by dropping non-essential questions if respondents or interviewers show signs of fatigue.\nMove interesting modules, such as games, to the middle of the survey to maintain engagement.\n\n\n\nOn the Data Flow and Field Protocols\n\nCorrect programming mistakes, such as adjusting relevance fields or constraints.\nUse pilot data to refine survey constraints based on reasonable responses.\nUse pilot metadata, such as duration, to adjust survey and field plans.\nAdjust interview timing or provide additional equipment if teams identify infrastructure issues such as lack of electricity.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#pilot-vs.-false-launch",
    "href": "data-quality/pilot-survey.html#pilot-vs.-false-launch",
    "title": "Pilot Survey",
    "section": "Pilot vs.¬†False Launch",
    "text": "Pilot vs.¬†False Launch\nThere are two main differences between a survey pilot and a false launch:\n\nTiming: The pilot starts earlier ‚Äì often during survey design ‚Äì finalizes the survey‚Äôs content, translation, and programming, and checks data flow and field protocols.\nAwareness: Your team knows about the pilot. In contrast, teams implement a false launch with a final translated instrument, and the field team believes it is the first day of real data collection.\n\nWhile it is possible to organize the second phase of the pilot as a false launch, this approach is not ideal. Piloting usually requires multiple iterations with debriefing sessions and revisions.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#pilot-vs.-soft-launch",
    "href": "data-quality/pilot-survey.html#pilot-vs.-soft-launch",
    "title": "Pilot Survey",
    "section": "Pilot vs.¬†Soft Launch",
    "text": "Pilot vs.¬†Soft Launch\nThere are two main differences between a survey pilot and a soft launch:\n\nTiming: The pilot starts earlier ‚Äì often during survey design ‚Äì and finalizes the survey‚Äôs content, translation, and programming, and checks data flow and field protocols.\nLocation: Teams implement the pilot outside your intended sample area. In contrast, teams always implement a soft launch with a final translated instrument and in a small area within the intended sample area, that is, part of the real data collection.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#piloting-and-time-constraints",
    "href": "data-quality/pilot-survey.html#piloting-and-time-constraints",
    "title": "Pilot Survey",
    "section": "Piloting and Time Constraints",
    "text": "Piloting and Time Constraints\nRemember, teams do not have time to skip the pilot! Piloting requires effort, but it is an upfront investment that will save time and money during data collection, cleaning, and analysis. Mistakes cost least when caught during the pilot. Moreover, piloted surveys are much more likely to result in high-quality data than non-piloted surveys. Always pilot your survey!",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "data-quality/pilot-survey.html#pilot-irb-approval",
    "href": "data-quality/pilot-survey.html#pilot-irb-approval",
    "title": "Pilot Survey",
    "section": "Pilot IRB Approval",
    "text": "Pilot IRB Approval\nWhether your pilot requires IRB approval depends on whether the activity constitutes human subjects research. Research is a systematic evaluation designed to produce generalizable knowledge.\n\nIf the pilot refines a tool ‚Äì such as evaluating text delivery, testing question length, finding programming errors ‚Äì 4and teams use the results to improve data quality rather than as part of outcome measures, it does not require IRB approval.\nIf the pilot collects identifiable data that teams will use for the study, it requires IRB approval.\n\nIf you are unsure whether your pilot activity needs IRB approval, email humansubjects@poverty-action.org.\nIf you are unsure whether your pilot activity needs IRB approval, email humansubjects@poverty-action.org.",
    "crumbs": [
      "Data Quality",
      "IPA Research Protocols",
      "Pilot Survey"
    ]
  },
  {
    "objectID": "research-design/how-to-power-calculation.html",
    "href": "research-design/how-to-power-calculation.html",
    "title": "Guide to Power Calculations",
    "section": "",
    "text": "Learn how to perform power calculations for randomized controlled trials using Stata‚Äôs power command. This guide covers three essential scenarios: calculating required sample size, determining minimum detectable effect, and estimating statistical power for given parameters."
  },
  {
    "objectID": "research-design/how-to-power-calculation.html#practical-tools-and-applications-in-stata",
    "href": "research-design/how-to-power-calculation.html#practical-tools-and-applications-in-stata",
    "title": "Guide to Power Calculations",
    "section": "Practical Tools and Applications in Stata",
    "text": "Practical Tools and Applications in Stata\n\nUsing the power Command in Stata\nThis guide uses Stata‚Äôs power command to perform sample size and power calculations, and to create sensitivity tables and graphs.1\nüìå Type this in Stata for help:\nhelp power\nThe India education intervention example illustrates three types of power analysis:\n\n\n\n\n\n\nüßÆ Case 1: Estimate Required Sample Size\n\n\n\n\n\nGoal: Determine how many participants you need to detect an expected effect.\nAssumptions:\n\nEffect size or minimum detectable effect: 0.2 S.D.\nStatistical power: 80%\nVariance of the outcome variable: 1 S.D.\nA single treatment group and a control group\nA single post-treatment data collection at endline\nProgram randomization at the individual child level\nPerfect compliance\nNo attrition\nNo additional controls\n\nStata code:\n\n%%stata\npower twomeans 0 0.2 , power(0.8) sd(1)\n\n\nPerforming iteration ...\n\nEstimated sample sizes for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.8000\n        delta =    0.2000\n           m1 =    0.0000\n           m2 =    0.2000\n           sd =    1.0000\n\nEstimated sample sizes:\n\n            N =       788\n  N per group =       394\n\n\nThis tells Stata to calculate the required sample size for each group‚Äîtreatment and control‚Äîto detect a 0.2 S.D. effect. The required study sample size to achieve a power of 80% is 788 students: 394 in the treatment group and 394 in the control group.\nStata code:\nNow, suppose researchers expect a larger treatment effect of 0.4 standard deviations instead of 0.2 S.D., keeping all other assumptions the same. To calculate the required sample size for this new scenario:\n\n%%stata\npower twomeans 0 0.4 , power(0.8) sd(1)\n\n\nPerforming iteration ...\n\nEstimated sample sizes for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.8000\n        delta =    0.4000\n           m1 =    0.0000\n           m2 =    0.4000\n           sd =    1.0000\n\nEstimated sample sizes:\n\n            N =       200\n  N per group =       100\n\n\nStata code:\nNow, changing the assumption: What if researchers want to increase the statistical power to 90%?\nTo calculate the required sample size for an effect size of 0.4 S.D. and 90% power:\n\n%%stata\npower twomeans 0 0.2 , power(0.9) sd(1)\n\n\nPerforming iteration ...\n\nEstimated sample sizes for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.9000\n        delta =    0.2000\n           m1 =    0.0000\n           m2 =    0.2000\n           sd =    1.0000\n\nEstimated sample sizes:\n\n            N =     1,054\n  N per group =       527\n\n\nThe required sample size would increase to 1,054.\nAdditional exercises are available in the following do file: Download power.do\n\n\n\n\n\n\n\n\n\nüßÆ Case 2: Estimate Minimum Detectable Effect\n\n\n\n\n\nGoal: Find the smallest effect size you can detect given your sample size.\nAssumptions:\n\nSample size: 1,000 students with 500 per group\nPower: 80%\nVariance of the outcome variable: 1 S.D\n\nStata code:\n\n%%stata\npower twomeans 0 , n(1000) power(0.8) sd(1)\n\n\nPerforming iteration ...\n\nEstimated experimental-group mean for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1; m2 &gt; m1\n\nStudy parameters:\n\n        alpha =    0.0500\n        power =    0.8000\n            N =     1,000\n  N per group =       500\n           m1 =    0.0000\n           sd =    1.0000\n\nEstimated effect size and experimental-group mean:\n\n        delta =    0.1774\n           m2 =    0.1774\n\n\nThis calculates the minimum effect size you can detect with the given sample size.\nStata code:\nAs shown in the graph, with 500, 750, 1,000, 1,250, and 1,500 students, researchers can detect smaller treatment effects as the sample size increases.\n\n%%stata\npower twomeans 0 , n(500(250)1500) power(0.8) sd(1) graph\n\n\n\n\n\n\n\n\nAdditional exercises are available in the following do file: Download power.do\n\n\n\n\n\n\n\n\n\nüßÆ Case 3: Estimate Power for a Given Sample Size and Effect\n\n\n\n\n\nGoal: Determine the statistical power based on your sample size and expected effect.\nAssumptions:\n\nStudy sample size: 1,000\nEffect size: 0.2 S.D.\nVariance of the outcome variable: 1 S.D.\n\nStata code:\n\n%%stata\npower twomeans 0 0.2 , n(1000) sd(1)\n\n\nEstimated power for a two-sample means test\nt test assuming sd1 = sd2 = sd\nH0: m2 = m1  versus  Ha: m2 != m1\n\nStudy parameters:\n\n        alpha =    0.0500\n            N =     1,000\n  N per group =       500\n        delta =    0.2000\n           m1 =    0.0000\n           m2 =    0.2000\n           sd =    1.0000\n\nEstimated power:\n\n        power =    0.8848\n\n\nThe estimated power under these assumptions is 0.8,848.\nStata code:\nSmaller samples lead to less power. As sample size increases, studies reach a larger probability of avoiding a type II error or false negative. In the graph, as the sample size increases, the estimated power also increases. This means studies are more likely to detect a true effect with a larger sample.\n\n%%stata\npower twomeans 0 0.2 , n(100(100)1000) sd(1) graph\n\n\n\n\n\n\n\n\nAdditional exercises are available in the following do file: Download power.do"
  },
  {
    "objectID": "research-design/how-to-power-calculation.html#additional-resources",
    "href": "research-design/how-to-power-calculation.html#additional-resources",
    "title": "Guide to Power Calculations",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOptimal Design Software: Free tool for complex designs\nG*Power: User-friendly power calculator\nJ-PAL‚Äôs Power Calculations Guide"
  },
  {
    "objectID": "research-design/how-to-power-calculation.html#footnotes",
    "href": "research-design/how-to-power-calculation.html#footnotes",
    "title": "Guide to Power Calculations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more details on the power command, see the Stata documentation.‚Ü©Ô∏é"
  },
  {
    "objectID": "research-design/index.html",
    "href": "research-design/index.html",
    "title": "Research Design",
    "section": "",
    "text": "Essential considerations for designing rigorous impact evaluations, from theoretical foundations to practical implementation strategies for randomized controlled trials and quasi-experimental studies.\nResearch design is a key ingredient for reliable, policy-relevant research and data science projects. It determines whether you can credibly measure and attribute observed changes to your intervention, rather than to other factors. This page provides an overview of key research design considerations and links to detailed guidance on specific topics.",
    "crumbs": [
      "Research Design",
      "About IPA IRB"
    ]
  },
  {
    "objectID": "research-design/index.html#core-research-design-elements",
    "href": "research-design/index.html#core-research-design-elements",
    "title": "Research Design",
    "section": "Core Research Design Elements",
    "text": "Core Research Design Elements\nEvery research and data science project must address several fundamental questions that determine the strength and credibility of your findings:\n\nTheory of Change and Causal Pathways\nBefore designing your project ‚Äì whether exploratory analysis, predictive modeling, or causal inference ‚Äì you need a clear understanding of your research context. You also need to understand how change occurs within that context. A Theory of Change maps the logical sequence from program activities to intended outcomes, making explicit the assumptions that underlie each step.\nKey considerations:\n\nWhat specific changes do you expect to see?\nWhat are the key assumptions about how change happens?\nWhat risks could break the causal chain?\n\nüëâ Learn more: Theory of Change\n\n\nStatistical Power and Sample Size\nYour study must be adequately powered to detect meaningful effects. This requires careful consideration of the minimum effect size worth detecting, expected variability in outcomes, and practical constraints.\nKey considerations:\n\nWhat is the minimum detectable effect that would be policy-relevant?\nWhat sample size do you need to achieve adequate statistical power?\nHow will attrition and non-compliance affect your power?\n\nüëâ Learn more: Sample and Power Calculations | How-to Guide to Power Calculations\n\n\nRandomization Strategy\nThe method of assigning participants to treatment and control groups is crucial for causal identification. The right approach depends on your intervention, context, and implementation constraints.\nKey considerations:\n\nShould you randomize at the individual or cluster level?\nDo you need stratification to ensure balance on key characteristics?\nHow will you handle spillovers between treatment and control groups?\n\nüëâ Learn more: Randomization | How-to Guide to Randomization\n\n\nMeasurement Strategy\nValid and reliable measurement of outcomes is essential for credible impact evaluation. This involves both technical considerations about instruments and practical considerations about data collection.\nKey considerations:\n\nWhich outcomes should you measure and when?\nHow will you ensure measurement validity and reliability?\nWhat baseline data do you need for analysis and balance checks?\n\nüëâ Learn more: Measurement",
    "crumbs": [
      "Research Design",
      "About IPA IRB"
    ]
  },
  {
    "objectID": "research-design/index.html#research-design-process",
    "href": "research-design/index.html#research-design-process",
    "title": "Research Design",
    "section": "Research Design Process",
    "text": "Research Design Process\nDesigning a rigorous research and data science project involves several interconnected steps:\n\n1. Define Your Research Questions\nStart with clear, specific questions about what you want to learn. Policy needs and your Theory of Change should tell you what questions to ask.\n\n\n2. Choose Your Identification Strategy\nDecide how you will establish causal attribution. Randomized experiments offer the strongest identification, but quasi-experimental methods may be necessary in some contexts.\n\n\n3. Plan Your Implementation\nConsider practical constraints and implementation challenges that might affect your design. This includes compliance, attrition, spillovers, and logistical feasibility.\n\n\n4. Design Your Analysis Plan\nPre-specify your analysis approach, including primary outcomes, estimation methods, and strategies for handling missing data or non-compliance.\n\n\n5. Consider Ethical Implications\nEnsure your design meets ethical standards for research with human subjects, including considerations of balance, informed consent, and potential harms.",
    "crumbs": [
      "Research Design",
      "About IPA IRB"
    ]
  },
  {
    "objectID": "research-design/index.html#getting-started",
    "href": "research-design/index.html#getting-started",
    "title": "Research Design",
    "section": "Getting Started",
    "text": "Getting Started\nWhen beginning a new impact evaluation:\n\nStart with your Theory of Change to understand the causal pathways you want to test\nConduct power calculations early to understand sample size requirements\nChoose your randomization strategy based on your intervention and context\nPlan your measurement approach to ensure you can test your key hypotheses\nConsider implementation challenges that might affect your design\n\nRemember that research design is iterative. You may need to revisit and refine your approach as you learn more about the intervention and context.",
    "crumbs": [
      "Research Design",
      "About IPA IRB"
    ]
  },
  {
    "objectID": "research-design/randomization.html",
    "href": "research-design/randomization.html",
    "title": "Randomization",
    "section": "",
    "text": "Introduction to randomization in impact evaluations, covering theoretical foundations and practical implementation strategies for researchers.",
    "crumbs": [
      "Research Design",
      "Randomization"
    ]
  },
  {
    "objectID": "research-design/randomization.html#what-is-randomization",
    "href": "research-design/randomization.html#what-is-randomization",
    "title": "Randomization",
    "section": "What is Randomization?",
    "text": "What is Randomization?\nRandomization is the cornerstone of rigorous impact evaluation. This process assigns units ‚Äì individuals, households, schools, and other entities ‚Äì to treatment and control groups, ensuring that assignment relies purely on chance rather than systematic factors.\n\n\n\n\n\n\nNote\n\n\n\nFor a practical introductory guide to randomization, see Guide to Randomization.\n\n\n\nRandom Sampling vs.¬†Random Assignment\nIt is crucial to distinguish between two related but distinct concepts:\n\n\n\n\n\n\nRandom Sampling\n\n\n\n\n\nThe process of selecting a subset of units from a larger population where each unit has a known probability of being selected. This ensures the sample is representative of the population.\n\n\n\nRandom sampling from population\n\n\n\n\n\n\n\n\n\n\n\nRandom Assignment\n\n\n\n\n\nThe process of allocating sampled units to treatment and control groups using a random mechanism. This ensures groups are comparable on both observable and unobservable characteristics.\n\n\n\nRandom assignment to groups\n\n\n\n\n\n\n\n\n\n\n\nCommon Misconception\n\n\n\nGeographic or systematic assignment ‚Äì such as ‚Äúnorthern half gets treatment, southern half gets control‚Äù ‚Äì is not random assignment, even if the sample was selected through random sampling.",
    "crumbs": [
      "Research Design",
      "Randomization"
    ]
  },
  {
    "objectID": "research-design/randomization.html#basic-randomization-procedures",
    "href": "research-design/randomization.html#basic-randomization-procedures",
    "title": "Randomization",
    "section": "Basic Randomization Procedures",
    "text": "Basic Randomization Procedures\n\nComplete Randomization: Fixed Proportion\nThis method assigns a predetermined number of units to treatment and control groups. For example, if you have 1000 participants and want exactly 400 in treatment:\n\nYou would randomly order all 1000 participants\nAssign the first 400 to treatment\nAssign the remaining 600 to control\n\nThis ensures precise control over group sizes, which proves important when:\n\nLimited treatment resources exist, such as only 400 program slots\nBalanced groups are needed for statistical power\nImplementation requires exact numbers, such as classroom capacity\n\nUnlike randomization with fixed probability, this method guarantees your desired treatment/control ratio.\n\n\n\n\n\n\nSteps for Complete Randomization\n\n\n\n\n\n\nCompile a list of all subjects in your sample\nDetermine the number you want in treatment (and control)\nUse a random number generator to order observations randomly\nAssign the first N units to treatment, remainder to control\n\nAdvantages:\n\nEnsures exact group sizes\nSimple to implement and explain\nEquitable when resources are limited\n\nChallenge: Requires a complete list of participants upfront\n\n\n\n\n\nSimple Randomization: Fixed Probability\nEach unit has a fixed probability, for example 50%, of assignment to treatment, regardless of other units‚Äô assignments. This resembles flipping a coin for each participant:\n\nHeads: 50% probability, assigned to treatment\nTails: 50% probability, assigned to control\n\nFor example:\n\nA student program might assign each applicant to treatment with 40% probability\nA health intervention might use 30% probability for intensive treatment\nA pilot study might start with 20% in treatment to test implementation\n\nUnlike fixed proportion, the final group sizes may vary due to chance, but the law of large numbers means they‚Äôll approach the target ratio with larger samples.\n\n\n\n\n\n\nWhen to Use Simple Randomization\n\n\n\n\n\n\nWalk-in situations where you can‚Äôt create a list in advance\nRolling enrollment contexts\nWhen exact group sizes aren‚Äôt critical\n\nImportant Note: This method may result in unequal group sizes due to chance",
    "crumbs": [
      "Research Design",
      "Randomization"
    ]
  },
  {
    "objectID": "research-design/randomization.html#choosing-the-unit-of-randomization",
    "href": "research-design/randomization.html#choosing-the-unit-of-randomization",
    "title": "Randomization",
    "section": "Choosing the Unit of Randomization",
    "text": "Choosing the Unit of Randomization\nThe level at which you randomize is one of the most critical decisions in evaluation design.\n\nIndividual-Level Randomization\nIndividual-level treatment assignment means each participant has an independent probability of selection for treatment or control. This offers maximal statistical power since each individual is an independent observation.\nFor example, in a microfinance study, individual entrepreneurs might be randomly selected to receive business loans, while others serve as controls.\n\n\n\n\n\n\nWhen to Randomize at Individual Level\n\n\n\n\n\nAppropriate when:\n\nTreatment delivery to individuals independently\nMinimal risk of spillovers between individuals\nMaximum statistical power requirements\n\nExamples:\n\nPatient-level medical interventions\nIndividual tutoring programs\nPersonal financial incentives\n\n\n\n\n\n\nCluster-Level Randomization\nCluster randomization assigns entire groups, such as schools, villages, or health clinics, to treatment or control conditions. Instead of randomizing individual participants, all units within a cluster receive the same treatment status.\nFor example, in a teacher training program, entire schools might be randomized rather than individual teachers. This means:\n\nIf School A is assigned to treatment, all teachers in School A receive training\nIf School B is assigned to control, no teachers in School B receive training\n\n\n\n\n\n\n\nWhen to Randomize at Cluster Level\n\n\n\n\n\nNecessary when:\n\nTreatment affects entire groups, such as teacher training affecting whole classrooms\nHigh risk of spillovers within clusters\nImplementation constraints require treating entire groups\n\nCommon clusters:\n\nSchools for education interventions\nVillages for community programs\nHealth clinics for health system interventions\n\nStatistical implications:\n\nReduced statistical power due to fewer independent units\nNeed to account for intra-cluster correlation\nTypically requires larger sample sizes\n\nBenefits:\n\nMinimizes spillovers within natural groups\nOften more practical to implement\nCan capture group-level effects\n\n\n\n\n\n\nKey Considerations for Choosing Randomization Level",
    "crumbs": [
      "Research Design",
      "Randomization"
    ]
  },
  {
    "objectID": "research-design/randomization.html#decision-framework",
    "href": "research-design/randomization.html#decision-framework",
    "title": "Randomization",
    "section": "Decision Framework",
    "text": "Decision Framework\nWhen choosing your randomization level, consider these key factors:\n\nIndividual-Level Randomization\nBest when:\n\nOutcomes measured at individual level‚Äîtest scores, health metrics, income\nIntervention targets individuals‚Äîpersonal tutoring, cash transfers\nLow spillover risk from personal interventions with minimal interaction\nMaximum statistical power needed\nSimple interventions with good tracking systems\n\n\n\nCluster-Level Randomization\nBest when:\n\nOutcomes measured at group level‚Äîschool performance, village outcomes\nIntervention targets groups‚Äîteacher training, community programs\nHigh spillover potential from social programs and shared resources\nCoordination with group leaders required\nWilling to accept lower statistical power for practical benefits",
    "crumbs": [
      "Research Design",
      "Randomization"
    ]
  },
  {
    "objectID": "research-design/randomization.html#managing-common-challenges",
    "href": "research-design/randomization.html#managing-common-challenges",
    "title": "Randomization",
    "section": "Managing Common Challenges",
    "text": "Managing Common Challenges\n\nNoncompliance\nNoncompliance occurs when units don‚Äôt follow their assigned treatment status. This can happen in two ways:\n\nOne-Sided Noncompliance: Only treatment group members can deviate\n\n\nTreatment group members do not participate, know as ‚Äúno-shows‚Äù, ‚Äúdropouts‚Äù, or ‚Äúrefusers‚Äù\nControl group cannot access treatment\n\n\nTwo-Sided Noncompliance: Both groups can deviate\n\n\nTreatment group members do not participate\nControl group members receive treatment, known as ‚Äúcrossover‚Äù\n\nCommon causes:\n\nService providers struggle to distinguish groups\nLogistical challenges in differential treatment\nParticipant self-selection or refusal\nControl group finding alternative ways to access treatment\n\nSolutions:\n\nRandomize at provider level when possible\nClear marking/identification systems\nStrong monitoring protocols\nDocument all cases of noncompliance\nUse intention-to-treat (ITT) analysis\nConsider instrumental variables for treatment-on-treated effects\n\nAnalysis Approaches:\n\nIntention-to-Treat (ITT): Analyze by original assignment\nTreatment-on-Treated (TOT): Account for actual treatment received\nLocal Average Treatment Effect (LATE): For partial compliance\n\n\n\nSpillovers\nSpillovers occur when the treatment affects units beyond those directly treated, potentially contaminating the control group and biasing treatment estimates.\nTypes of spillovers:\n\nEnvironmental: Treatment changes shared conditions,such as reduced disease transmission\nBehavioral: Control group imitates treatment behaviors\nInformational: Knowledge spreads between groups\n\nStrategies:\n\nIncrease geographic/social distance between groups\nRandomize at higher level to contain spillovers\nDesign evaluation to measure spillovers explicitly\n\nImpact on Analysis:\n\nCan lead to underestimated treatment effects if control group benefits\nMay require additional buffer zones between treatment and control\nSometimes spillovers themselves are of research interest, such as vaccine studies",
    "crumbs": [
      "Research Design",
      "Randomization"
    ]
  },
  {
    "objectID": "research-design/randomization.html#alternative-randomization-methods",
    "href": "research-design/randomization.html#alternative-randomization-methods",
    "title": "Randomization",
    "section": "Alternative Randomization Methods",
    "text": "Alternative Randomization Methods\n\n\n\n\n\n\nCross-Cutting Treatments\n\n\n\n\n\nPurpose: Test multiple treatment variations simultaneously and efficiently\nDesign Types:\n\n2x2 factorial design (most common)\nMultiple treatment arms (3+ variations)\nNested designs\n\nAdvantages:\n\nTests interaction effects between treatments\nMore cost-effective than separate trials\nIdentifies optimal treatment combinations\n\nImplementation:\n\nList all treatment combinations\nAssign units randomly to each arm\nTrack and monitor each variation separately\nAnalyze both main effects and interactions\n\n\nTeacher Training and Materials Example\n\n\n\nGroup\nTraining\nMaterials\nSample Size\n\n\n\n\nControl\nNo\nNo\n25%\n\n\nT1\nYes\nNo\n25%\n\n\nT2\nNo\nYes\n25%\n\n\nT3\nYes\nYes\n25%\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhase-In Design\n\n\n\n\n\nPurpose: Ensure all units eventually receive treatment while maintaining a valid control group\nDesign Types:\n\nSequential rollout (most common)\nRandom order phase-in\nStratified phase-in\nTime-lagged treatments\n\nAdvantages:\n\nEthically sound (all units get treatment)\nMaintains experimental control\nAllows program refinement\nMultiple measurement points\n\nImplementation:\n\nDetermine number and timing of phases\nRandomize order of treatment receipt\nMonitor phase transitions carefully\nAnalyze data from each phase\n\n\nCommunity Program Rollout Example\n\n\n\nPhase\nTreatment Group\nControl Group\nTimeline\n\n\n\n\n1\n25%\n75%\nMonths 1-3\n\n\n2\n50%\n50%\nMonths 4-6\n\n\n3\n75%\n25%\nMonths 7-9\n\n\n4\n100%\n0%\nMonths 10-12\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncouragement Design\n\n\n\n\n\nPurpose: Evaluate impact when treatment cannot be denied but participation can be influenced\nDesign Types:\n\nInformation campaigns\nPersonalized reminders\nApplication assistance\nCost subsidies\nBehavioral nudges\n\nAdvantages:\n\nEthical when services are entitlements\nMaintains experimental validity\nMeasures both take-up and impact\nIdentifies barriers to participation\n\nImplementation:\n\nDesign effective encouragement strategy\nRandomize who receives encouragement\nTrack both encouragement and participation\nUse instrumental variables analysis\n\n\nHealth Insurance Program Example\n\n\n\nGroup\nEncouragement\nAccess\nExpected Take-up\n\n\n\n\nTreatment\nActive outreach\nYes\n60%\n\n\nControl\nNo outreach\nYes\n30%\n\n\nDifference\nInformation + reminders\nNone\n30%\n\n\n\n\n\n\n\n\n\n\n\n\n\nStratified Randomization\n\n\n\n\n\nPurpose: Ensure balance on key characteristics across treatment groups, especially important with small samples or critical covariates\nDesign Types:\n\nGeographic stratification\nDemographic characteristics\nBaseline performance levels\nMultiple variable combinations\nCovariate-adaptive methods\n\nAdvantages:\n\nImproves statistical precision\nGuarantees balance on key variables\nReduces chance of unlucky draws\nFacilitates subgroup analysis\nIncreases credibility of results\n\nImplementation:\n\nSelect key stratification variables\nCreate strata combinations\nRandomize within each stratum\nVerify balance across strata\n\n\nEducation Program Example\n\n\n\nStratum\nBaseline Score\nGender\nTreatment Share\n\n\n\n\nLow-Male\nBelow median\nMale\n50%\n\n\nLow-Female\nBelow median\nFemale\n50%\n\n\nHigh-Male\nAbove median\nMale\n50%\n\n\nHigh-Female\nAbove median\nFemale\n50%\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Balsakhi Tutoring Program\n\n\n\n\n\nThe Balsakhi program in India is a classic example of rigorous randomization in education research (Banerjee et al., 2007; Duflo, Glennerster, and Kremer, 2007):\nContext:\nImplemented by Pratham, the Balsakhi program provided remedial tutoring to academically weaker primary school students in Mumbai and Vadodara. The goal was to help students who were falling behind catch up with their peers.\nRandomization:\n\nUnit: 122 schools were randomized to either receive the Balsakhi intervention or serve as controls.\nStratification: Randomization was stratified by language of instruction (Marathi, Gujarati, Hindi, English) and by gender composition (boys‚Äô, girls‚Äô, and co-ed schools) to ensure balance across key characteristics.\n\nImplementation:\n\nWithin treatment schools, Balsakhis (local young women trained as tutors) worked with the lowest-performing students, identified using baseline test scores.\nStrict protocols were followed to prevent contamination between treatment and control schools, including separate training and monitoring teams.\nDetailed documentation and monitoring ensured fidelity to the randomization plan.\n\nResults:\n\nThe program led to a significant improvement of 0.14 standard deviations in test scores for all students, and even larger gains for the weakest students.\nThe study demonstrated the power of cluster-level randomization and the importance of stratification for balance.\n\nReferences:\n\nBanerjee, A., Cole, S., Duflo, E., and Linden, L. (2007). ‚ÄúRemedying Education: Evidence from Two Randomized Experiments in India.‚Äù Quarterly Journal of Economics, 122(3), 1235-1264. DOI: 10.1162/qjec.122.3.1235 1\nDuflo, E., Glennerster, R., and Kremer, M. (2007). ‚ÄúUsing Randomization in Development Economics Research: A Toolkit.‚Äù Handbook of Development Economics, 4, 3895-3962. 2",
    "crumbs": [
      "Research Design",
      "Randomization"
    ]
  },
  {
    "objectID": "research-design/randomization.html#footnotes",
    "href": "research-design/randomization.html#footnotes",
    "title": "Randomization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBanerjee, A., Cole, S., Duflo, E., and Linden, L. (2007). Remedying education: Evidence from two randomized experiments in India. The Quarterly Journal of Economics, 122(3), 1235-1264. https://doi.org/10.1162/qjec.122.3.1235‚Ü©Ô∏é\nDuflo, E., Glennerster, R., and Kremer, M. (2007). Using randomization in development economics research: A toolkit. In T. P. Schultz and J. A. Strauss (Eds.), Handbook of Development Economics (Vol. 4, pp.¬†3895-3962). Elsevier.‚Ü©Ô∏é",
    "crumbs": [
      "Research Design",
      "Randomization"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html",
    "href": "research-design/theory-of-change.html",
    "title": "Theory of Change",
    "section": "",
    "text": "How to develop a robust Theory of Change, linking program activities to outcomes through evidence-based pathways.",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#what-is-a-theory-of-change",
    "href": "research-design/theory-of-change.html#what-is-a-theory-of-change",
    "title": "Theory of Change",
    "section": "What is a Theory of Change?",
    "text": "What is a Theory of Change?\nA Theory of Change (ToC) is a comprehensive description and illustration of how and why a desired change is expected to happen in a particular context. It maps out the logical sequence from activities to outputs to outcomes to impact, making explicit the assumptions underlying each link in the causal chain.\n\nVisual vs.¬†Narrative Representation\n\n\n\n\n\n\nForms of Theory of Change\n\n\n\n\n\nA ToC can be presented as:\n\nVisual diagram: Shows the causal pathways through boxes and arrows\nNarrative description: Written explanation of the change process\nCombination: Visual diagram with accompanying narrative (most effective)\n\nThe visual representation helps stakeholders quickly grasp the overall logic, while the narrative provides essential detail about mechanisms and assumptions.\n\n\n\n\n\nPurpose and Benefits\n\n\n\n\n\n\nWhy Develop a Theory of Change?\n\n\n\n\n\nCommunication: Provides a clear, concise way to explain your program to stakeholders\nAlignment: Creates shared understanding among team members about goals and strategies\nLearning Framework: Identifies what to monitor and evaluate at each stage\nAdaptation: Highlights assumptions to test and refine through implementation",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#components-of-a-theory-of-change",
    "href": "research-design/theory-of-change.html#components-of-a-theory-of-change",
    "title": "Theory of Change",
    "section": "Components of a Theory of Change",
    "text": "Components of a Theory of Change\nA comprehensive ToC consists of five key components that form a logical chain from implementation to impact.\n\nThe Five Essential Components\n\n\n\n\n\n\n1. Activities\n\n\n\n\n\nDefinition: The specific actions your organization implements\nCharacteristics:\n\nUse active verbs (train, provide, deliver, organize)\nSpecify who does what\nExclude internal processes (e.g., planning, procurement)\n\nExample: ‚ÄúDeliver 10 business skills training sessions to caregivers‚Äù NOT ‚ÄúSupport caregivers‚Äù\n\n\n\n\n\n\n\n\n\n2. Outputs\n\n\n\n\n\nDefinition: The direct products of activities\nCharacteristics:\n\nImmediately measurable\nUnder direct control of the program\nOne output per activity\n\nExample: ‚ÄúCaregivers attend and complete 10 training sessions‚Äù\n\n\n\n\n\n\n\n\n\n3. Initial Outcomes\n\n\n\n\n\nDefinition: Short-term changes in participants\nTypes of changes:\n\nKnowledge\nAttitudes\nBeliefs\nBehaviors (sometimes)\n\nMeasurement: During or immediately after implementation Attribution: Directly attributable to program activities\n\n\n\n\n\n\n\n\n\n4. Intermediate Outcomes\n\n\n\n\n\nDefinition: Medium to long-term changes resulting from initial outcomes\nCharacteristics:\n\nMay be influenced by external factors\nRequire initial outcomes to be achieved first\nNeed experimental methods to measure causality\n\nExample: ‚ÄúCaregivers apply business skills to increase income‚Äù\n\n\n\n\n\n\n\n\n\n5. Final Outcomes/Impact\n\n\n\n\n\nDefinition: Long-term goals of the program\nCharacteristics:\n\nUltimate changes in beneficiaries‚Äô lives\nInfluenced by multiple factors beyond the program\nRequire rigorous evaluation to establish attribution\n\nExample: ‚ÄúImproved economic stability and wellbeing of families‚Äù\n\n\n\n\n\nVisual Representation\n\n\n\nTheory of Change Components Diagram",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#characteristics-of-a-strong-theory-of-change",
    "href": "research-design/theory-of-change.html#characteristics-of-a-strong-theory-of-change",
    "title": "Theory of Change",
    "section": "Characteristics of a Strong Theory of Change",
    "text": "Characteristics of a Strong Theory of Change\n\nThe Four Essential Qualities\n\n\n\n\n\n\n1. Active\n\n\n\n\n\nActivities use specific action verbs:\n\n‚úÖ Good: ‚ÄúTrain 20 facilitators in child development‚Äù\n‚ùå Weak: ‚ÄúSupport child development‚Äù\n\nFocus on what the organization does, not internal processes\n\n\n\n\n\n\n\n\n\n2. Clear\n\n\n\n\n\nEach component is distinct and specific:\n\nSame level of detail across components\nOne output per activity\nNo overlapping elements\n\nAll aspects of the program are represented\n\n\n\n\n\n\n\n\n\n3. Logical\n\n\n\n\n\nArrows represent causal relationships, not chronology:\n\nEach link shows how one element causes the next\nThe pathway makes sense conceptually\nNo logical leaps or missing steps\n\nExample:\n\nLogical: Training ‚Üí Increased knowledge ‚Üí Applied skills ‚Üí Higher income\nNot logical: Training ‚Üí Higher income (missing intermediate steps)\n\n\n\n\n\n\n\n\n\n\n4. Detailed\n\n\n\n\n\nSufficient detail for monitoring and evaluation:\n\nDistinguishes between initial and intermediate outcomes\nIncludes all necessary steps in the causal chain\nSpecific enough to guide measurement\n\nHelps identify:\n\nWhat outputs to monitor during implementation\nWhich outcomes to measure at different stages\nWhen to conduct evaluations",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#assumptions-and-risks",
    "href": "research-design/theory-of-change.html#assumptions-and-risks",
    "title": "Theory of Change",
    "section": "Assumptions and Risks",
    "text": "Assumptions and Risks\nEvery arrow in a Theory of Change represents assumptions about how change happens. Making these explicit is crucial for learning and adaptation.\n\nUnderstanding Assumptions\n\n\n\n\n\n\nWhat Are Assumptions?\n\n\n\n\n\nDefinition: Conditions that must hold true for the causal pathway to work\nExamples:\n\nParticipants will attend sessions if scheduled conveniently\nNew knowledge will be retained and applied\nMarket conditions will remain stable enough for businesses to grow\n\nWhy they matter: Unmet assumptions can break the causal chain\n\n\n\n\n\nIdentifying and Managing Risks\n\n\n\n\n\n\nFrom Assumptions to Risks\n\n\n\n\n\nRisk: A specific threat to an assumption being met\nExamples of improving risk specification:\n\n\n\n\n\n\n\nWeak Risk Statement\nStrong Risk Statement\n\n\n\n\n‚ÄúLow attendance‚Äù\n‚ÄúWork schedules prevent 40% of participants from attending morning sessions‚Äù\n\n\n‚ÄúContent not relevant‚Äù\n‚Äú60% of business curriculum doesn‚Äôt match local market conditions‚Äù\n\n\n‚ÄúNo behavior change‚Äù\n‚ÄúSocial norms discourage women from starting businesses independently‚Äù\n\n\n\n\n\n\n\n\nPrioritizing Risks\n\n\n\n\n\n\nRisk Assessment Matrix\n\n\n\n\n\nFocus learning efforts on risks that are both:\n\nImportant: Would significantly impact success if they occur\nUncertain: Limited evidence about likelihood or mitigation strategies\n\nPriority quadrants:\n\nHigh importance + High uncertainty = Priority for testing\nHigh importance + Low uncertainty = Standard mitigation\nLow importance + High uncertainty = Monitor only\nLow importance + Low uncertainty = Acknowledge but don‚Äôt focus",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#learning-approaches",
    "href": "research-design/theory-of-change.html#learning-approaches",
    "title": "Theory of Change",
    "section": "Learning Approaches",
    "text": "Learning Approaches\nDifferent methods help test assumptions and understand risks at various stages of implementation.\n\n\n\n\n\n\nPiloting\n\n\n\n\n\n\nWhen to use: Before full implementation\nPurpose: Test feasibility and refine approach\nMethods: Small-scale implementation with intensive monitoring\nExample: Test training curriculum with 2-3 groups before scaling\n\n\n\n\n\n\n\n\n\n\nFocus Groups\n\n\n\n\n\n\nWhen to use: Any stage, especially for understanding ‚Äúwhy‚Äù\nPurpose: Explore perceptions, motivations, and barriers\nMethods: Facilitated group discussions\nExample: Understand why attendance is lower than expected\n\n\n\n\n\n\n\n\n\n\nAdministrative Data Analysis\n\n\n\n\n\n\nWhen to use: During implementation\nPurpose: Track outputs and early outcomes\nMethods: Analyze program records and monitoring data\nExample: Review attendance records to identify patterns\n\n\n\n\n\n\n\n\n\n\nExpert Consultation\n\n\n\n\n\n\nWhen to use: Design phase or when facing specific challenges\nPurpose: Leverage specialized knowledge\nMethods: Interviews or advisory meetings\nExample: Consult labor economists about local market conditions",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#using-theory-of-change-for-monitoring-and-evaluation",
    "href": "research-design/theory-of-change.html#using-theory-of-change-for-monitoring-and-evaluation",
    "title": "Theory of Change",
    "section": "Using Theory of Change for Monitoring and Evaluation",
    "text": "Using Theory of Change for Monitoring and Evaluation\n\nMonitoring Focus\n\n\n\n\n\n\nWhat to Monitor\n\n\n\n\n\nOutputs and Initial Outcomes are the focus of monitoring because:\n\nThey‚Äôre under program control\nDirectly attributable to activities\nProvide real-time feedback for improvement\n\nKey indicators:\n\nParticipation rates\nCompletion rates\nQuality measures\nImmediate knowledge/attitude changes\n\n\n\n\n\n\nEvaluation Focus\n\n\n\n\n\n\nWhat to Evaluate\n\n\n\n\n\nIntermediate and Final Outcomes require evaluation because:\n\nExternal factors may influence them\nAttribution needs to be established\nLonger time horizons are involved\n\nMethods:\n\nExperimental designs (RCTs)\nQuasi-experimental approaches\nMixed methods to understand mechanisms",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#ipas-right-fit-evidence-approach",
    "href": "research-design/theory-of-change.html#ipas-right-fit-evidence-approach",
    "title": "Theory of Change",
    "section": "IPA‚Äôs Right-Fit Evidence Approach",
    "text": "IPA‚Äôs Right-Fit Evidence Approach\nIPA‚Äôs Right-Fit Evidence (RFE) team has developed practical tools and frameworks for creating and using Theories of Change effectively.\n\nKey Principles\n\n\n\n\n\n\nRight-Fit Evidence Philosophy\n\n\n\n\n\nMatch methods to learning needs:\n\nNot every question needs an RCT\nStart with the decision you need to make\nUse the most efficient method that provides sufficient confidence\n\nIterative learning:\n\nTest assumptions progressively\nAdapt based on evidence\nDocument changes and rationale\n\n\n\n\n\n\nResources and Support\n\n\n\n\n\n\nAvailable Resources\n\n\n\n\n\nTemplates and Tools:\n\nToC development worksheets\nAssumption mapping guides\nRisk prioritization matrices\n\nCapacity Building:\n\nWorkshops on ToC development\nPeer review processes\nCommunity of practice\n\nTechnical Assistance:\n\nDirect support for complex programs\nReview and feedback services\nConnection to subject matter experts",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#common-pitfalls-and-how-to-avoid-them",
    "href": "research-design/theory-of-change.html#common-pitfalls-and-how-to-avoid-them",
    "title": "Theory of Change",
    "section": "Common Pitfalls and How to Avoid Them",
    "text": "Common Pitfalls and How to Avoid Them\n\n\n\n\n\n\nPitfall 1: Too Vague\n\n\n\n\n\nProblem: Using general terms like ‚Äúimprove wellbeing‚Äù or ‚Äústrengthen capacity‚Äù Solution: Be specific about what will change and how you‚Äôll measure it\n\n\n\n\n\n\n\n\n\nPitfall 2: Missing Steps\n\n\n\n\n\nProblem: Jumping from training to long-term impact without intermediate steps Solution: Think through each necessary change in the causal chain\n\n\n\n\n\n\n\n\n\nPitfall 3: Overly Complex\n\n\n\n\n\nProblem: Including every possible pathway and outcome Solution: Focus on the main causal pathways; document others separately\n\n\n\n\n\n\n\n\n\nPitfall 4: Static Document\n\n\n\n\n\nProblem: Creating a ToC once and never updating it Solution: Review and revise based on learning; document changes",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-design/theory-of-change.html#additional-resources",
    "href": "research-design/theory-of-change.html#additional-resources",
    "title": "Theory of Change",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nIPA‚Äôs Right-Fit Evidence Resources\nTheory of Change Guidance - UNICEF\nDIY Toolkit: Theory of Change",
    "crumbs": [
      "Research Design",
      "Theory of Change"
    ]
  },
  {
    "objectID": "research-ethics/irb-docs.html",
    "href": "research-ethics/irb-docs.html",
    "title": "Research and Data Science Hub",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Ethics and IPA IRB",
      "IRB Documentation üöß"
    ]
  },
  {
    "objectID": "research-ethics/irb-key-topics.html",
    "href": "research-ethics/irb-key-topics.html",
    "title": "Essential IRB Components",
    "section": "",
    "text": "This reference guide provides essential definitions, requirements, and procedures for risk level assessment, human subjects training, and informed consent that apply to all research studies requiring IRB review.",
    "crumbs": [
      "Ethics and IPA IRB",
      "Key Topics"
    ]
  },
  {
    "objectID": "research-ethics/irb-key-topics.html#risk-level",
    "href": "research-ethics/irb-key-topics.html#risk-level",
    "title": "Essential IRB Components",
    "section": "Risk Level",
    "text": "Risk Level\n\nMinimal Risk\nDefinition: The probability and magnitude of harm or discomfort anticipated in the research are not greater than those ordinarily encountered in daily life or routine procedures.\nReview Process: - Studies deemed minimal risk that fit into one of OHRP‚Äôs (Office for Human Research Protections) seven designated expedited review categories undergo expedited review - Expedited review is conducted by an IPA IRB representative designated by the IPA IRB Chair - Reviews are conducted on an ongoing basis without waiting for full board meetings - Expedited review cannot be requested - it is determined by the IRB based on risk level\n\n\nNon-Minimal Risk\nDefinition: Research that does not meet minimal risk criteria - poses more risk than encountered in daily life, or does not fit expedited review categories.\nAdditional Criteria: - Studies collecting both PII and sensitive information are likely considered non-minimal risk - Sensitive information: Information that (1) poses criminal or civil liability to respondent OR (2) could jeopardize employability, financial standing, or reputation\nReview Process: - Referred to full IRB Board for monthly meeting review - Any submission type can be considered non-minimal risk\n\n\nVulnerable Populations\nDefinition: Groups whose ability to provide voluntary, informed consent is compromised relative to a free, functioning adult.\nFederally Protected Categories: - Children/Minors: Defined according to local laws and regulations - Generally requires assent from minors and permission from parents/guardians - Prisoners: Any individual involuntarily confined or detained in a penal institution, including those awaiting arraignment, trial, or sentencing - Pregnant women/fetuses/neonates: Risk assessment depends on whether intervention targets improvement of woman and/or child welfare\nAdditional Protections: - Additional populations may be considered vulnerable based on local context - Inclusion of vulnerable populations may elevate the risk level of the study but not necessarily make it non-minimal risk",
    "crumbs": [
      "Ethics and IPA IRB",
      "Key Topics"
    ]
  },
  {
    "objectID": "research-ethics/irb-key-topics.html#human-subjects-research-training",
    "href": "research-ethics/irb-key-topics.html#human-subjects-research-training",
    "title": "Essential IRB Components",
    "section": "Human Subjects Research Training",
    "text": "Human Subjects Research Training\n\nDefinition of Human Subjects Research\nSystematic investigation including research development, testing, and evaluation, designed to develop or contribute to generalizable knowledge and that involves living individuals about whom an investigator obtains information either: - (a) through intervention or interaction - (b) identifiable private information\n\n\nTraining Requirements\nRequired Training: US federal regulations require human subjects training for all study personnel. IPA recommends CITI Social, Behavioral, and Educational Research (SBER) Basic course.\nWho Must Complete Training: 1. ALL principal investigators 2. Any IPA research staff on the project 3. Any other research staff (such as university-based research analysts or field staff) who handle or view data with more than 10% Personally Identifiable Information (PII)\nTimeline and Validity: - Must be completed before conducting human subjects research - Certificate issued immediately upon completion - Valid for three years - Refresher course required for continued research after expiration",
    "crumbs": [
      "Ethics and IPA IRB",
      "Key Topics"
    ]
  },
  {
    "objectID": "research-ethics/irb-key-topics.html#informed-consent",
    "href": "research-ethics/irb-key-topics.html#informed-consent",
    "title": "Essential IRB Components",
    "section": "Informed Consent",
    "text": "Informed Consent\n\nWhat is informed consent?\nInformed consent in research is the process of providing potential participants with all necessary information about a study. This allows them to make an informed decision about whether to participate. Researchers must obtain participants‚Äô informed consent before involving them in research.\n\n\nWhy is informed consent important?\nThis requirement is founded on the principle of ‚Äúrespect for persons‚Äù‚Äîone of three core ethical principles governing human subjects research described in the Belmont Report. Respect for persons requires that individuals receive treatment as autonomous agents and ‚Äúbe given the opportunity to choose what shall or shall not happen to them.‚Äù It also requires that persons with diminished autonomy ‚Äì such as children, prisoners, and cognitively impaired adults ‚Äì receive special protections.\n\n\nWhat does informed consent look like in practice?\n‚ÄúInformed consent‚Äù is simultaneously (1) a process, (2) a decision, and (3) a form or script.\n\nProcess\nThe informed consent process is a dynamic interaction wherein: - Researchers provide information, such as risks and benefits, about the study which may affect a potential subject‚Äôs willingness to participate - Researchers facilitate the potential subject‚Äôs understanding of that information by inviting them to ask questions, re-explaining anything they found confusing, etc. - Researchers emphasize and protect the voluntariness of a subject‚Äôs decision about whether to participate\nThis process should not feel like a dry, esoteric, one-sided lecture. Instead, it creates an opportunity for an engaging, honest discussion about the risks and benefits of the research with a potential participant. Enumerators need training to verify whether respondents have questions and to check for understanding of key elements. IPA encourages researchers to think innovatively about the best way to deliver the information required in an informed consent.\n\n\nDecision\nAt the end of this process‚Äîonce the potential subject understands all relevant information‚Äîthe subject will decide whether to participate. The decision to participate in research is not a binding agreement‚Äîa subject is always free to withdraw their consent.\n\n\n\n\n\n\nImportant\n\n\n\nThe decision to participate in research is not a binding agreement‚Äîa subject is always free to withdraw their consent.\n\n\n\n\nForm / Script\nThe exchange of information‚Äîbetween researchers and the potential subject‚Äîthat occurs during the informed consent process can take place in either a written or verbal format (or both). The following approaches are most common at IPA: - An enumerator reads aloud a written form or script to the subject (face-to-face or by phone) - The research team provides a written form for the subject to read (on paper or electronically) and keep for future reference\nThe format used often depends on specific research logistics. For example, phone surveys make it impossible to provide subjects with written paper forms, so enumerators read the consent aloud by phone.\nRegardless of how researchers conduct the informed consent process, they must also decide how to document a subject‚Äôs decision. Consent may be documented in one or more of the following ways: - The subject may provide a signature (or thumbprint) - The subject may click a button or checkbox indicating their consent (e.g., if completing an online survey) - The subject may express their consent verbally; in this case, researchers must have some system for documenting the subject‚Äôs response\nIf the IRB determines the research is greater than minimal risk, subjects typically must provide a signature documenting their consent.\n\n\n\nWhat should be included in a consent form/script?\nFederal regulations require that certain key elements be included in an informed consent. The IPA IRB has developed two resources to help research teams prepare good consent forms: - A checklist of required elements (this is what IPA IRB looks for when evaluating a submitted consent form) - A template consent form (this provides a sample structure and language to cover all required elements in the checklist)\nYou do not need to use IPA IRB‚Äôs template to develop your consent, but every relevant element on the checklist must be present. Using the template is a shortcut to ensuring that all requirements in the checklist are met. If you do not wish to use the template, it is still to your benefit to check your consent against the checklist before you submit, as IPA IRB uses this checklist each time it evaluates whether a submitted consent meets the proper standards.\n\n\nAlteration or waiver of consent\nIn some circumstances, researchers may request that required elements of the consent process be waived (‚Äúalteration of consent‚Äù). In even rarer circumstances, researchers may request that the requirement to obtain subject consent be waived entirely (‚Äúwaiver of consent‚Äù).\nIn either of these circumstances, all the following criteria must be met in order for the alteration or waiver to be granted by the IRB: - The research involves no more than minimal risk to the subjects - Researchers could not practicably carry out the research without the requested waiver or alteration - If the research involves using PII or identifiable biospecimens, researchers could not practicably carry out the research without using such PII or biospecimens in an identifiable format - The waiver or alteration will not adversely affect the rights and welfare of the subjects - Whenever appropriate, researchers will provide subjects or legally authorized representatives with additional pertinent information after participation\nThe following are examples of circumstances where researchers might request an alteration or waiver of consent: - Waiver of parental consent (most often requested for school-based research) - Studies involving deception (i.e., leaving out information or misleading subjects about some element of the study)\nResearchers must request the IRB‚Äôs permission for any alteration or waiver of consent. To do so, describe your rationale for the requested alteration or waiver in the consent section of the protocol.\n\n\nStudies involving children\nFor studies involving minors, researchers typically must seek both: - Child assent: permission from the child themselves - Parental consent: permission, given by the parent, for the child‚Äôs participation\n‚ÄúAssent‚Äù is not, and cannot substitute for, consent‚Äîlegitimate consent can only be given by adults.\nA child assent form/script should be written in language understandable to the child‚Äôs age group. For children age 14+, it is typically fine to use IPA IRB‚Äôs standard consent template to develop the assent. For children age &lt;14, researchers should consider how best to communicate key information to those children such that they can make an informed choice about whether to participate in the study.\n\nWaiver of child assent\nThe IRB can waive the requirement to obtain child assent if the child cannot reasonably be consulted about their participation in the research, such as young children.\n\n\nWaiver of parental consent\nThe IRB can waive the requirement to obtain parental consent only in the following circumstances: - All criteria for a waiver of consent are met (see section above); OR - Parental permission is not a reasonable requirement to protect child subjects, such as neglected or abused children, or children living on the street without family\nEven if the IRB determines that it may waive the parental consent requirement, the IRB may require additional measures to protect child subjects, such as: - Obtaining consent from school officials, such as the child‚Äôs teacher for school-based research - Sending parents an ‚Äúinformation sheet‚Äù which provides information about the study, as well as instructions for how to withdraw their child from participation using an ‚Äúopt out‚Äù procedure\n\n\n\nOther considerations\nTiming: In most cases, researchers should obtain subjects‚Äô informed consent before each round of data collection. In some rarer cases, researchers may need to obtain consent before the intervention, even if no data collection occurs at that point. This applies when there is no baseline survey and the study intervention is the first research procedure subjects will participate in.\nLanguage: Researchers must conduct the informed consent process in a language understandable to the potential subject.\nLiteracy: Researchers should consider the literacy level of the potential subject population when designing the consent process for their study. For example, if working with a largely illiterate subject population, researchers should conduct the consent process orally‚Äîproviding only a written form for subjects to read would be inappropriate.\nNo exculpatory language: No consent may include language through which the subject waives any of their legal rights, or language that releases the researchers, sponsor, or institution from liability for negligence.",
    "crumbs": [
      "Ethics and IPA IRB",
      "Key Topics"
    ]
  },
  {
    "objectID": "research-ethics/irb-reliance-agreements.html#what-is-a-reliance-agreement",
    "href": "research-ethics/irb-reliance-agreements.html#what-is-a-reliance-agreement",
    "title": "Reliance Agreements",
    "section": "What is a Reliance Agreement?",
    "text": "What is a Reliance Agreement?\nA reliance agreement is a formal, written agreement between two (or more) IRBs which enables one institution engaged in research to rely on another institution‚Äôs IRB review.\nMany research studies involve collaboration between investigators from multiple institutions which each maintain their own IRB. The advantage of a reliance agreement is that, rather than having multiple IRBs conduct their own full review of a research project, a reliance allows for only one institution to conduct a full review. This streamlines the IRB process and reduces burdens on investigators. A reliance agreement documents, in writing, the responsibilities of both the relying and reviewing institutions regarding IRB review, reporting, and oversight.",
    "crumbs": [
      "Ethics and IPA IRB",
      "Reliance Agreements"
    ]
  },
  {
    "objectID": "research-ethics/irb-reliance-agreements.html#when-is-a-reliance-agreement-needed",
    "href": "research-ethics/irb-reliance-agreements.html#when-is-a-reliance-agreement-needed",
    "title": "Reliance Agreements",
    "section": "When is a Reliance Agreement Needed?",
    "text": "When is a Reliance Agreement Needed?\nFor every study IPA is engaged in, IPA IRB must do one of the following:\n\nIPA IRB conducts its own review of the project\nIPA IRB sets up a ‚Äúreliance agreement‚Äù to formally cede oversight of the project to another institution‚Äôs IRB.\n\nThese are the only two options. If you do not wish to submit your project to IPA IRB, it will be necessary to set up a reliance agreement with another IRB.\n\n\n\n\n\n\nWhat kinds of institutions can IPA enter reliance agreements with?\n\n\n\n\n\nIPA IRB can only enter reliance agreements with institutions that have a Federalwide Assurance (FWA). An FWA is a document held by an institution which signals that the institution has made a formal commitment to follow US federal regulations for the protection of human subjects. Most US-based university IRBs have FWAs. It is rarer for a local IRB to have an FWA, but some do. You can check whether an institution has an FWA here\n\n\n\n\n\n\n\n\n\nCan I have another IRB rely on IPA IRB instead?\n\n\n\n\n\nYes! You may wish for IPA IRB to review the project and to have another institution (e.g., the PI‚Äôs university) rely on IPA IRB‚Äôs review. In some cases, a donor, partner organization, or PI‚Äôs university may even require that IPA IRB conducts its own review of the study. (As of 2024, J-PAL requires that IPA projects receiving J-PAL funding be reviewed by IPA IRB - or HML IRB.)\n\n\n\n\n\n\n\n\n\nWhat about local IRB?\n\n\n\n\n\nIPA IRB also requires that all projects be reviewed by a local IRB in the country where research is taking place (if such an IRB exists) before human subjects activities begin.",
    "crumbs": [
      "Ethics and IPA IRB",
      "Reliance Agreements"
    ]
  },
  {
    "objectID": "research-ethics/irb-reliance-agreements.html#setting-up-a-reliance-agreement",
    "href": "research-ethics/irb-reliance-agreements.html#setting-up-a-reliance-agreement",
    "title": "Reliance Agreements",
    "section": "Setting up a Reliance Agreement",
    "text": "Setting up a Reliance Agreement\nAs early as possible, get in touch with both IPA IRB (humansubjects@poverty-action.org) and the other IRB you would like us to set up a reliance with. Different IRBs may have different policies about whether they are willing to enter reliances and different processes for evaluating reliance requests.\n\n\n\n\n\n\nIf you would like IPA IRB to cede oversight to another IRB‚Ä¶\n\n\n\n\n\nAsk the other IRB if they are willing to allow IPA IRB to rely on their review. Once the other IRB has approved the project, send an email to humansubjects@poverty-action.org with:\n\nA completed Reliance Agreement Assessment Worksheet ([link to download from IPA IRB webpage])\nThe approval letter and all approved materials (protocol, consents, surveys, etc.) from the reviewing IRB.\n\n\n\n\n\n\n\n\n\n\nWhy does IPA IRB have to review project materials if it is ceding oversight of the project?\n\n\n\n\n\nIn order to assess whether we are willing to cede review of any given project, an initial limited review of the project must be done. This is necessary as there are certain kinds of projects (e.g., those involving significant risks or very vulnerable populations) that IPA IRB may be less comfortable ceding to another IRB. We also must make sure that the other IRB to whom we are ceding will do the kind of thorough oversight IPA IRB expects and would do itself. Once a reliance is established, IPA IRB then cedes review/trusts that the other IRB will provide adequate oversight of the project from that point forward (potentially for years, depending on the length of the project) - so it is only initially that a team must go through ‚Äúreview‚Äù with IPA IRB.\n\n\n\n\n\n\n\n\n\nIf you would like another IRB to rely on IPA‚Äôs review‚Ä¶\n\n\n\n\n\nAsk the other IRB if they are willing to rely on IPA IRB‚Äôs review. Then, send an email to humansubjects@poverty-action.org with:\n\nA completed Reliance Agreement Assessment Worksheet ([link to download from IPA IRB webpage])\nIf not done already, prepare and submit an IRB application for your project for IPA IRB review.\n\n\n\n\nIf IPA IRB will cede oversight another IRB, there is a $500 fee. This can be either charged to a grant or invoiced. If another IRB will rely on IPA IRB‚Äôs review, there is no fee for the reliance agreement. However, regular IPA IRB review fees apply (for your initial application and any subsequent amendments, renewals, etc.).",
    "crumbs": [
      "Ethics and IPA IRB",
      "Reliance Agreements"
    ]
  },
  {
    "objectID": "software/git/index.html",
    "href": "software/git/index.html",
    "title": "Git",
    "section": "",
    "text": "Click on the image below to watch a video with an introduction to Git for beginners:",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "Git"
    ]
  },
  {
    "objectID": "software/git/index.html#how-to-install-git",
    "href": "software/git/index.html#how-to-install-git",
    "title": "Git",
    "section": "How to install Git?",
    "text": "How to install Git?\nInstall Git for Windows via winget. Git comes pre-installed with MacOS and Linux, if you can‚Äôt find it, try installing with Homebrew.\n\nWindowsMacOSLinux\n\n\nwinget install Git.Git\n\n\n# Git comes pre-installed with most MacOS distributions.\ngit --version\n\n\n# Git comes pre-installed with most Linux distributions.\ngit --version",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "Git"
    ]
  },
  {
    "objectID": "software/git/index.html#using-git",
    "href": "software/git/index.html#using-git",
    "title": "Git",
    "section": "Using Git",
    "text": "Using Git\nGit can be confusing and overwhelming. We recommend starting with a graphical user interface (GUI) to help you understand the basics of Git. GitHub Desktop is a good option or, if you use VS Code, you can use the built-in Git functionality (See VS Code documentation).\nClick on the image below to watch a video on basic Git commands:\n\n\n\nBasic Git Commands\n\n\nHere are the basic commands you need to know to get started with Git:\n\ngit init\nCreate a new Git repository\ngit init\n\n\ngit clone\nClone a repository into a new directory For example, to clone the IPA handbook repository:\n# If using HTTPS\ngit clone https://github.com/PovertyAction/ipa-data-tech-handbook.git\n\n# If using SSH\ngit clone git@github.com:PovertyAction/ipa-data-tech-handbook.git\n\n\ngit checkout\nBranches are used to develop new code or modify existing code such that the ‚Äúmain‚Äù code is not affected until the new code is ready. To create a new branch, use:\ngit checkout -b new-branch-name\nThis checks out a new branch called new-branch-name. To switch back to the main branch, use:\ngit checkout main\nTo checkout a branch from the remote repository, use:\ngit fetch origin\ngit checkout --track origin/remote-branch-name\nTo push a branch to the remote repository, use:\ngit push origin new-branch-name\nTo delete a branch, use:\ngit branch -d new-branch-name\nList all local branches:\ngit branch --list\n\n\ngit add\nAdd files that have changed and are ready to be committed to the staging area. To add a file, file_name.md, use:\ngit add file_name.md\n\n\ngit commit\nCommit changes to the checked out branch.\nGood commit messages follow the following format:\ngit commit -m \"&lt;type&gt;: &lt;description&gt;\"\nTo commit changes with a message, use:\ngit commit -m \"feat: adding new feature to the codebase\"\n\n\ngit push\nSend changes to the remote repository. To push any commits to remote use the following:\ngit push origin new-branch-name\nIf you are pushing a new branch to the remote repository, use:\ngit push --set-upstream origin new-branch-name\nConsider adding the Conventional Commits extension to your VS Code to help you write good commit messages.",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "Git"
    ]
  },
  {
    "objectID": "software/git/index.html#best-practices",
    "href": "software/git/index.html#best-practices",
    "title": "Git",
    "section": "Best Practices",
    "text": "Best Practices\n\nAlways include a README file in your repository and keep it up to date with key information that anyone who visits your repository should know for using, replicating, or contributing to code in the repository.\nUse branches to develop new features or fix bugs. This helps to keep the main branch clean and stable.\nWrite clear and concise commit messages. A good commit message should describe what changes were made and why they were made. Refer to Conventional Commits for good practice in writing commit messages.\nPush changes to the remote repository frequently. This helps to keep your codebase up to date and allows others to collaborate with you.\nUse pull requests to propose changes to the main branch. Try to keep the pull request small such that there is a manageable amount of code to review.",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "Git"
    ]
  },
  {
    "objectID": "software/git/index.html#learning-resources",
    "href": "software/git/index.html#learning-resources",
    "title": "Git",
    "section": "Learning Resources",
    "text": "Learning Resources\n\nGitHub‚Äôs YouTube Git for Beginners\nGitHub‚Äôs Git Cheat Sheet\ngit - the simple guide\nGit Best Practices\nHappy Git with R\nVisual Git Guide\nVisualizing Git with D3",
    "crumbs": [
      "Software Guides",
      "Version Control",
      "Git"
    ]
  },
  {
    "objectID": "software/guides/venv.html#python-virtual-environments",
    "href": "software/guides/venv.html#python-virtual-environments",
    "title": "Virtual Environments",
    "section": "Python Virtual Environments",
    "text": "Python Virtual Environments\nWe recommend using uv to manage Python virtual environments as it provides an overarching framework for managing Python installations and virtual environments. See information on uv in the uv documentation.",
    "crumbs": [
      "Software Guides",
      "Coding Guides",
      "Virtual Environments"
    ]
  },
  {
    "objectID": "software/guides/venv.html#r-virtual-environments",
    "href": "software/guides/venv.html#r-virtual-environments",
    "title": "Virtual Environments",
    "section": "R Virtual Environments",
    "text": "R Virtual Environments\nR virtual environments can be created using the renv package. See documentation for more information about how to use renv.",
    "crumbs": [
      "Software Guides",
      "Coding Guides",
      "Virtual Environments"
    ]
  },
  {
    "objectID": "software/guides/venv.html#docker",
    "href": "software/guides/venv.html#docker",
    "title": "Virtual Environments",
    "section": "Docker",
    "text": "Docker\nDocker is a tool that allows you to create, deploy, and run applications using containers. In some instances, it may be more appropriate to use Docker containers to manage your project environment, especially when there is a project environment that needs to be deployed to cloud computing services.",
    "crumbs": [
      "Software Guides",
      "Coding Guides",
      "Virtual Environments"
    ]
  },
  {
    "objectID": "software/guides/venv.html#learning-resources",
    "href": "software/guides/venv.html#learning-resources",
    "title": "Virtual Environments",
    "section": "Learning Resources",
    "text": "Learning Resources\n\nA Complete Guide to Python Virtual Environments\nReal Python, Python Virtual Environments\nPython Virtual Environments\nR Virtual Environments",
    "crumbs": [
      "Software Guides",
      "Coding Guides",
      "Virtual Environments"
    ]
  },
  {
    "objectID": "software/python/coding-python.html",
    "href": "software/python/coding-python.html",
    "title": "Coding in Python",
    "section": "",
    "text": "In this tutorial, you will learn the fundamentals of Python programming. We‚Äôll cover variables, data types, and how to use built-in functions to perform basic operations.",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#learning-objectives",
    "href": "software/python/coding-python.html#learning-objectives",
    "title": "Coding in Python",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this tutorial, you will be able to:\n\nAssign values to variables\nUnderstand basic data types in Python\nUse built-in Python functions\nGet help while programming",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#variables",
    "href": "software/python/coding-python.html#variables",
    "title": "Coding in Python",
    "section": "Variables",
    "text": "Variables\nA variable is a name for a value. In Python, variables are created when you assign a value to them using the = sign.\n\nweight_kg = 60.3\n\nIn this example, we‚Äôve created a variable called weight_kg and assigned it the value 60.3.\nOnce a variable has been created, we can use it in calculations:\n\nprint(weight_kg)\n\n60.3\n\n\nWe can also do arithmetic with variables:\n\nprint('weight in pounds:', 2.2 * weight_kg)\n\nweight in pounds: 132.66\n\n\n\nVariable Names\nVariable names can contain letters, digits, and underscores. However, they:\n\nCannot start with a digit\nAre case-sensitive\n\n\n# Valid variable names\npatient_id = 'inflam_001'\npatient_weight = 75.0\npatient1_age = 45\n\n# Display the variables\nprint('Patient ID:', patient_id)\nprint('Patient weight:', patient_weight, 'kg')\nprint('Patient age:', patient1_age, 'years')\n\nPatient ID: inflam_001\nPatient weight: 75.0 kg\nPatient age: 45 years\n\n\n\n\nChanging Variables\nWe can change the value of a variable by assigning it a new value:\n\nweight_kg = 60.3\nprint('weight in kilograms:', weight_kg)\n\nweight_kg = 65.0\nprint('weight in kilograms is now:', weight_kg)\n\nweight in kilograms: 60.3\nweight in kilograms is now: 65.0\n\n\nNotice that when we assign a new value to a variable, it does not change other variables that might have used the old value:\n\nweight_kg = 60.3\nweight_lb = 2.2 * weight_kg\nprint('weight in kilograms:', weight_kg, 'and in pounds:', weight_lb)\n\nweight_kg = 65.0\nprint('weight in kilograms is now:', weight_kg, 'and weight in pounds is still:', weight_lb)\n\nweight in kilograms: 60.3 and in pounds: 132.66\nweight in kilograms is now: 65.0 and weight in pounds is still: 132.66",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#data-types",
    "href": "software/python/coding-python.html#data-types",
    "title": "Coding in Python",
    "section": "Data Types",
    "text": "Data Types\nEvery value in Python has a specific type. Three common types are:\n\nInteger numbers (whole numbers)\nFloating point numbers (numbers with decimals)\nStrings (text)\n\n\n\n\n\n\n\nNote\n\n\n\nThere are many other data types, eg. bool, list, set, which you will discover as you progress.\n\n\n\n# Examples of different data types\npatient_age = 25          # integer\npatient_weight = 68.5     # float\npatient_name = 'Alice'    # string\n\nprint('Patient name:', patient_name)\nprint('Patient age:', patient_age)\nprint('Patient weight:', patient_weight)\n\nPatient name: Alice\nPatient age: 25\nPatient weight: 68.5\n\n\n\nFinding the Type of a Variable\nWe can use the built-in function type() to find out what type a value has:\n\nprint(type(60.3))\nprint(type('hello world'))\nprint(type(25))\n\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n\n\n\n\nWorking with Strings\nStrings must be enclosed in quotes (either single or double):\n\npatient_id = 'inflam_001'\npatient_name = \"John Doe\"\n\nprint('Patient ID:', patient_id)\nprint('Patient name:', patient_name)\n\nPatient ID: inflam_001\nPatient name: John Doe\n\n\nWe can add strings together (concatenation):\n\nprefix = 'inflam_'\npatient_number = '001'\npatient_id = prefix + patient_number\nprint('Full patient ID:', patient_id)\n\nFull patient ID: inflam_001",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#built-in-functions",
    "href": "software/python/coding-python.html#built-in-functions",
    "title": "Coding in Python",
    "section": "Built-in Functions",
    "text": "Built-in Functions\nPython provides many built-in functions that perform common operations. We‚Äôve already seen print() and type(). (Parentheses distinguish functions from variables.)\n\nThe print() Function\nThe print() function displays values:\n\nprint('Hello, world!')\nprint('Patient weight:', 68.5, 'kg')\n\nHello, world!\nPatient weight: 68.5 kg\n\n\nYou can print multiple values by separating them with commas:\n\nweight_kg = 68.5\nweight_lb = 2.2 * weight_kg\nprint('Weight:', weight_kg, 'kg =', weight_lb, 'lbs')\n\nWeight: 68.5 kg = 150.70000000000002 lbs\n\n\n\n\nMathematical Operations\nPython supports common mathematical operations:\n\n# Basic arithmetic\na = 10\nb = 3\n\nprint('Addition:', a + b)\nprint('Subtraction:', a - b)\nprint('Multiplication:', a * b)\nprint('Division:', a / b)\nprint('Integer division:', a // b)\nprint('Remainder:', a % b)\nprint('Power:', a ** b)\n\nAddition: 13\nSubtraction: 7\nMultiplication: 30\nDivision: 3.3333333333333335\nInteger division: 3\nRemainder: 1\nPower: 1000",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#getting-help",
    "href": "software/python/coding-python.html#getting-help",
    "title": "Coding in Python",
    "section": "Getting Help",
    "text": "Getting Help\nWhen you need help with Python, there are several ways to get information:\n\nUsing the help() Function\nThe help() function provides documentation about Python functions:\n\nhelp(print)\n\nThis will display detailed information about the print() function, including its parameters and usage examples.\n\n\nOther Ways to Get Help\n\nOnline documentation: The official Python documentation at python.org\nStack Overflow: A community-driven Q&A site where you can search for answers or ask questions\nColleagues and mentors: Don‚Äôt hesitate to ask experienced programmers for help\nLLMs: Modern generative AI tools like ChatGPT, Claude, and GitHub Copilot can be useful to explain code. (Note that AI tools are not perfect and may make mistakes, but they will usually get fundamental questions right.)",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#practice-exercises",
    "href": "software/python/coding-python.html#practice-exercises",
    "title": "Coding in Python",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nLet‚Äôs practice what we‚Äôve learned with some exercises:\n\n\n\n\n\n\nPro Tip\n\n\n\nTry to write the code by yourself before revealing the solution\n\n\n\nExercise 1: Variable Assignment\nCreate variables for a patient‚Äôs information and display them: - name: Sarah Johnson - age: 34 - height (cm): 165\n\n\nShow the code\npatient_name = 'Sarah Johnson'\npatient_age = 34\npatient_height = 165.0  # in cm\n\nprint('Patient:', patient_name)\nprint('Age:', patient_age, 'years')\nprint('Height:', patient_height, 'cm')\n\n\n\n\nExercise 2: Data Type Investigation\nWhat are the types of the following values?\n- 3.25 - 3 - ‚Äú3.25‚Äù\nUse the type() function to check:\n\n\nShow the code\nprint('Type of 3.25:', type(3.25)) # Expect float\nprint('Type of 3:', type(3)) # Expect int\nprint('Type of \"3.25\":', type('3.25')) # Expect str\n\n\n\n\nExercise 3: Calculations\nA patient‚Äôs body mass index (BMI) is calculated as weight (kg) divided by height (m) squared. Calculate a patient‚Äôs BMI for the following values:\n\nweight_kg = 70.0\nheight_cm = 175.0\n\n\n\nShow the code\n# Patient data\nweight_kg = 70.0\nheight_cm = 175.0\n\n# Convert height to meters\nheight_m = height_cm / 100\n\n# Calculate BMI\nbmi = weight_kg / (height_m ** 2)\n\nprint('Patient weight:', weight_kg, 'kg')\nprint('Patient height:', height_m, 'm')\nprint('Patient BMI:', round(bmi, 1))\n\n\nThe expected outcome is 22.9",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#key-points",
    "href": "software/python/coding-python.html#key-points",
    "title": "Coding in Python",
    "section": "Key Points",
    "text": "Key Points\n\nUse variables to store values and make calculations\nUse print(...) to display values\nVariables persist between cells\nVariables must be created before they are used\nVariables can be used in calculations\nUse type(...) to determine the type of a value\nPython is case-sensitive\nUse help(...) to get help about functions",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#whats-next",
    "href": "software/python/coding-python.html#whats-next",
    "title": "Coding in Python",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nNow that you understand Python fundamentals, you can explore:\n\nWorking with lists and data structures\nControl flow with loops and conditionals\n\nFunctions and modules\nWorking with data using libraries like pandas\n\nContinue practising these basics as they form the foundation for all Python programming!",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/python/coding-python.html#learning-resources",
    "href": "software/python/coding-python.html#learning-resources",
    "title": "Coding in Python",
    "section": "Learning Resources",
    "text": "Learning Resources\n\nThe Python Tutorial\nPython Data Science Handbook\nEfficient Python for Data Scientists\nThe Hitchhiker‚Äôs Guide to Python",
    "crumbs": [
      "Software Guides",
      "Python",
      "Coding in Python"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#what-makes-quarto-special",
    "href": "software/quarto/basics.html#what-makes-quarto-special",
    "title": "Writing in Quarto",
    "section": "What Makes Quarto Special?",
    "text": "What Makes Quarto Special?\nQuarto lets you combine text, code, and visualizations in a single document. Write regular text and seamlessly include code that runs and displays results.",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#text-formatting-essentials",
    "href": "software/quarto/basics.html#text-formatting-essentials",
    "title": "Writing in Quarto",
    "section": "Text Formatting Essentials",
    "text": "Text Formatting Essentials\n\nBasic Text Styling\n\nCodeResult\n\n\n*This text is italic*\n**This text is bold** \n***This text is bold and italic***\n~~This text has strikethrough~~\n`This is inline code`\n\n\nThis text is italic\nThis text is bold\nThis text is bold and italic\nThis text has strikethrough\nThis is inline code\n\n\n\n\n\nHeaders\n\nCodeResult\n\n\n# Main Title (H1)\n## Section Header (H2)  \n### Subsection Header (H3)\n#### Smaller Header (H4)\n\n\nHeaders create structure and automatic navigation links.",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#lists-for-clear-organization",
    "href": "software/quarto/basics.html#lists-for-clear-organization",
    "title": "Writing in Quarto",
    "section": "Lists for Clear Organization",
    "text": "Lists for Clear Organization\n\nCodeResult\n\n\n# Unordered Lists\n* Research Design\n* Data Collection\n  + Phone Surveys\n  + In-Person Interviews\n* Data Analysis\n\n# Ordered Lists\n1. Design your survey\n2. Collect pilot data\n3. Refine your instrument\n4. Launch full data collection\n\n\nUnordered Lists * Research Design * Data Collection + Phone Surveys + In-Person Interviews * Data Analysis\nOrdered Lists 1. Design your survey 2. Collect pilot data 3. Refine your instrument 4. Launch full data collection",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#code-blocks",
    "href": "software/quarto/basics.html#code-blocks",
    "title": "Writing in Quarto",
    "section": "Code Blocks",
    "text": "Code Blocks\n\nCodeResult\n\n\n# Simple code display\n```\nsummary(data)\nmean(variable)\n```\n\n# With syntax highlighting\n```python\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\nprint(df.head())\n```\n\n# Executable code cells\n\n::: {#6a701b50 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nx = [1, 2, 3, 4]\ny = [1, 4, 9, 16]\nplt.plot(x, y)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](basics_files/figure-html/cell-2-output-1.png){width=566 height=411}\n:::\n:::\n\n\n\nSimple Code Display:\nsummary(data)\nmean(variable)\nWith Syntax Highlighting:\nimport pandas as pd\ndf = pd.read_csv(\"data.csv\")\nprint(df.head())\nExecutable cells run code and show output in your document.",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#callouts",
    "href": "software/quarto/basics.html#callouts",
    "title": "Writing in Quarto",
    "section": "Callouts",
    "text": "Callouts\n\nCodeResult\n\n\n::: {.callout-note}\nThis is a standard note callout.\n:::\n\n::: {.callout-tip title=\"Pro Tip\"}  \nAlways pilot your survey before deployment!\n:::\n\n::: {.callout-warning}\nRemember to back up your data.\n:::\n\n::: {.callout-important appearance=\"simple\"}\nIRB approval required before data collection.\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Potential Issues\nClick to expand details...\n:::\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a standard note callout.\n\n\n\n\n\n\n\n\nPro Tip\n\n\n\nAlways pilot your survey before deployment!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember to back up your data.\n\n\n\n\n\n\n\n\nIRB approval required before data collection.\n\n\n\n\n\n\n\n\n\nPotential Issues\n\n\n\n\n\nClick to expand details‚Ä¶",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#tabsets",
    "href": "software/quarto/basics.html#tabsets",
    "title": "Writing in Quarto",
    "section": "Tabsets",
    "text": "Tabsets\n\nCodeResult\n\n\n::: {.panel-tabset}\n\n## Survey Methods\n- Phone surveys\n- In-person interviews  \n- Online questionnaires\n\n## Analysis Tools  \n- Stata for statistical analysis\n- R for data visualization\n- Python for machine learning\n\n## Best Practices\n- Always clean your data\n- Document your methodology\n- Version control your code\n:::\n\n\n\nSurvey MethodsAnalysis ToolsBest Practices\n\n\n\nPhone surveys\nIn-person interviews\n\nOnline questionnaires\nWhatsApp surveys\n\n\n\n\nStata for statistical analysis\nR for data visualization\nPython for machine learning\nExcel for basic calculations\n\n\n\n\nAlways clean your data\nDocument your methodology\nVersion control your code\nShare reproducible results",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#links-and-images",
    "href": "software/quarto/basics.html#links-and-images",
    "title": "Writing in Quarto",
    "section": "Links and Images",
    "text": "Links and Images\n\nCodeResult\n\n\n# Links\n[Visit IPA's website](https://poverty-action.org)\n[Quarto Documentation](https://quarto.org/docs/)\n\n# Images\n![Survey team in Colombia](/assets/images/Colombia_Survey_2020.jpg){width=75%}\n\n\nLinks: Visit IPA‚Äôs website and Quarto Documentation\nImages:",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#custom-styling",
    "href": "software/quarto/basics.html#custom-styling",
    "title": "Writing in Quarto",
    "section": "Custom Styling",
    "text": "Custom Styling\n\nCodeResult\n\n\n::: {.border}\nThis content has a border around it.\n:::\n\n::: {.custom-summary-block}\nThis matches the site's summary block styling.\n:::\n\n\n\nThis content has a border around it.\n\n\nThis matches the site‚Äôs summary block styling.",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#mathematical-expressions",
    "href": "software/quarto/basics.html#mathematical-expressions",
    "title": "Writing in Quarto",
    "section": "Mathematical Expressions",
    "text": "Mathematical Expressions\n\nCodeResult\n\n\n# Inline math\nThe formula $E = mc^2$ is famous.\n\n# Block equations\n$$\n\\text{Treatment Effect} = \\bar{Y}_{\\text{treatment}} - \\bar{Y}_{\\text{control}}\n$$\n\n\nInline math: The formula \\(E = mc^2\\) is famous.\nBlock equations: \\[\n\\text{Treatment Effect} = \\bar{Y}_{\\text{treatment}} - \\bar{Y}_{\\text{control}}\n\\]",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#tables",
    "href": "software/quarto/basics.html#tables",
    "title": "Writing in Quarto",
    "section": "Tables",
    "text": "Tables\n\nCodeResult\n\n\n| Survey Type | Response Rate | Cost per Response |\n|-------------|---------------|-------------------|\n| Phone       | 65%           | $12               |\n| In-person   | 85%           | $45               |\n| WhatsApp    | 78%           | $3                |\n\n\n\n\n\nSurvey Type\nResponse Rate\nCost per Response\n\n\n\n\nPhone\n65%\n$12\n\n\nIn-person\n85%\n$45\n\n\nWhatsApp\n78%\n$3",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#next-steps",
    "href": "software/quarto/basics.html#next-steps",
    "title": "Writing in Quarto",
    "section": "Next Steps",
    "text": "Next Steps\n\nCreate Professional Documents: Combine these elements for research reports and analysis guides\nCustomize Your Style: Add a brand.yml file for consistent styling Learn more\nShare Your Work: Publish to GitHub Pages, Netlify, or other platforms\nCollaborate: Use version control with Git to track changes\n\n\n\n\n\n\n\nReady to Practice?\n\n\n\nTry creating your own Quarto document using these elements. Start simple with headers, text formatting, and a callout, then add more complex features.",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/quarto/basics.html#learning-resources",
    "href": "software/quarto/basics.html#learning-resources",
    "title": "Writing in Quarto",
    "section": "Learning Resources",
    "text": "Learning Resources\n\nOfficial Documentation\n\nQuarto Guide - Comprehensive documentation covering all features\nAuthoring Guide - Deep dive into markdown and content creation\nPublishing Guide - How to share your documents online\n\n\n\nInteractive Tutorials\n\nHello, Quarto - Quick start tutorial for VS Code\nComputations - Learn to embed code and outputs\nAuthoring - Advanced formatting and features\n\n\n\nExamples and Inspiration\n\nQuarto Gallery - Showcase of real-world Quarto projects\nAwesome Quarto - Community-curated list of resources\n\n\n\nVideo Resources\n\nIntroduction to Quarto - Overview and key features\nAcademic Workflows with Quarto - Research-focused examples\n\n\n\nExtensions and Tools\n\nQuarto Extensions - Add functionality with community extensions\nVS Code Extension - Enhanced editing experience\n\n\n\nCommunity\n\nQuarto Discussions - Ask questions and share tips\nRStudio Community - Active forum for Quarto users",
    "crumbs": [
      "Software Guides",
      "Quarto",
      "Writing in Quarto"
    ]
  },
  {
    "objectID": "software/stata/coding-stata.html#using-stata-files",
    "href": "software/stata/coding-stata.html#using-stata-files",
    "title": "Coding in Stata",
    "section": "Using Stata Files",
    "text": "Using Stata Files\nStata data files are saved with the extension ‚Äú.dta‚Äù. This means the file is ready to use in Stata and unlike data saved in, for example, an excel file, you do not need to import this into Stata.\nTo start using this data in Stata you simply need to type\nuse \"filepath\\filename.dta\", clear\nAdditionally, you will be able to use this dataset in other commands when combining two datasets such as merge or append.\nmerge 1:1 uid using \"statadata.dta\", options\nappend using \"statadata.dta\"`\nIf your file was not already in the stata format you would not be able to call it directly. You would have to import it before you can use it.\n\nSystem Datasets\nStata comes with pre-loaded datasets that you can use to play around to learn new things on. You can see all of the available datasets by going to ‚ÄúFile &gt; Example datasets‚Ä¶‚Äù or typing help dta_examples. From here you can click the ‚Äúuse‚Äù or ‚Äúdescribe‚Äù buttons to load the dataset and describe them.\nIf you already know the name of the dataset you want to use, you simply need to type sysuse auto.dta for the auto dataset for example to use the data.\n\n\nCreating a dataset in Stata from scratch\nIn Stata, you can create a dataset from scratch. This can be helpful if you want to test some code, learn or check how something works. When working in a clear Stata session, you can use set obs n where n is an integer, to create that many observations in the dataset. From there, you can gen variables set to whatever you are interested in.\nExample to test how to generate a dummy variable as well as using _n to indicate row numbers\n*Clear your stata console\n  clear all\n*Create how many observations you want your dataset to have\n  set obs 10\n*Create variables to test the code you are interested in\n  gen test_dummy = (_n &lt; 5)",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Coding in Stata"
    ]
  },
  {
    "objectID": "software/stata/coding-stata.html#numerical-formats",
    "href": "software/stata/coding-stata.html#numerical-formats",
    "title": "Coding in Stata",
    "section": "Numerical formats",
    "text": "Numerical formats\nIt‚Äôs easy to forget that Stata code is operating a computer with very different rules for counting and numbers than we have in the real world. Ado (the language of .do files) allows for a high-level abstraction, where the programmer does not have to explicitly command the computer to do low-level tasks like allocating memory for data, ordering tasks, or defining how the computer should round values that can‚Äôt be precisely displayed in binary. This is rarely important, but there are a few cases where these processes, like precision of stored data, is highly relevant for statistical tasks and may need to be specified. The most relevant cases are: - IDs should be stored as string variables or have less than 8 digits if the storage type of the variable is a float - Asserts should only compare similar storage types. - All values in stata (e.g.¬†1 or `r(N)') are treated as doubles - Merges that do not match IDs across datasets and display bugs (e.g.¬†1.0000000784732907 in Excel) can be due to the storage type of the variables or values\n\nStata‚Äôs process\nVariables in Stata have storage formats and display formats. Storage formats describe how Stata is storing the variable in the computer memory ‚Äì what the data are ‚Äì and display formats describes the default way that the information is presented to a user. Type help format in Stata to get more information on how variables or values are displayed.\nStata has five storage formats for numerical variables that take up different amount of memory. These formats store information to a certain degree of accuracy before rounding. The first three types (byte, int, and long) in the table below can only be used for integers. float and double are the standard type. There is a trade-off for increased precision. More precise storage formats take up more memory. This will make the file larger and slow down Stata‚Äôs processes when the data are being used.\n\n\n\n\n\n\n\n\nType\nMaximum digits of accuracy\nBytes of memory for a single value\n\n\n\n\nbyte\n2\n1\n\n\nint\n4\n2\n\n\nlong\n9\n4\n\n\nfloat\n7\n4\n\n\ndouble\n16\n8\n\n\n\nThis is extremely relevant when exact equivalence matters. Stata will always conduct functions in double precision (at about 16 digits of precision). Imagine that you are comparing a variable x and a number .1. Stata defaults to generating variables as floats to conserve memory. To process a calculation, Stata will transform the float x into a double and compare that value to the .1 rounded to a double. This causes results that we may not expect for numbers, like .1, that aren‚Äôt stored exactly in binary:\n. set obs 1\n. gen x = .1\n. assert x == .1\n  assertion is false\n  r(9);\n. di \"`=float(.1)'\"\n  .1000000014901161\n. di \"`=.1'\"\n  .1\nStata is not making a mistake here. This is the result of .1 not having an exact value in binary (base 2 v. base 10). Since Stata does all calculations in double precision, the rounded value of .1 is different at float precision than double precision. The code that follows shows a few ways to compare these values exactly.\n. ds x, d\n\n              storage   display    value\nvariable name   type    format     label      variable label\n--------------------------------------------------------------------------------------------------------------\nx               float   %9.0g\n\n. assert x == float(.1) // Force Stata to round double(.1) to float(.1)\n\n// force Stata to display the first and only value of x in a double format\n. assert `: di x[1]' == .1\n\n. gen double y = .1 // generate a new variable at double precision\n. assert y == .1\nThis would not be a problem for a value that can be stored exactly such as 1.\nAlthough this seems very abstract and of limited relevance, this will cause problems in the following cases that are often encountered by IPA projects: - IDs have different storage types (one is a float and one is a double for example) in a merge. Stata will not prevent you from making that merge, but will not be able to match the IDs that the programmer intended to be the same. - Numeric IDs will no longer be unique when they have more digits than their storage type can hold (16 for doubles). No numeric IDs will be unique if they have more than 17 digits, especially if the last digits are changing for individuals that should be unique. It‚Äôs best to store IDs as strings or keep ID values at the minimum length needed. - asserts that compare a scalar (r(mean)) to a variable stored as a float may not be accurate. This can be corrected by using functions like inrange() or float() as part of the assert.\n\n\nDataset Size & Memory Usage\nThere are a number of concrete ways to avoid this, as well as a lot written on how this affects computation. Memory conservation is generally not relevant for statistical programming with small N survey data that we normally work with at IPA. But this can be the relevant in large datasets where using memory on extraneous digits will slow basic computations substantively. Administrative data with observations in the hundreds of thousands to millions is an example of this that is relevant for many IPA projects.\nIt‚Äôs generally good practice to reduce the size of files using compress or by generating values in the smallest format such as gen byte dummy = (q1 == \"Yes\"), especially when the data are larger or if you are performing commands that are computationally intensive for Stata (various regressions, reshaping, etc.).\nIf you are interested in more details, Stata Corp‚Äôs blog has a few good articles on numerical precision and why this happens in computing, as well as the specific digits that float precision loses values.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Coding in Stata"
    ]
  },
  {
    "objectID": "software/stata/coding-stata.html#quality-control",
    "href": "software/stata/coding-stata.html#quality-control",
    "title": "Coding in Stata",
    "section": "Quality Control",
    "text": "Quality Control\nOnce you‚Äôve finished cleaning a dataset, take some time to inspect the final product by using a command like codebookout. This command outputs an excel file that is a codebook of your final data including variable names, labels, types, values and value labels. Check that variables are labeled and take on a range of values that make sense.\nAnother helpful command in quality control is assert. You should include many asserts throughout your do files in order to check that your assumptions while cleaning hold. This is especially important if you are going to be collecting more data later.\nFinally, you can use the checklist in this folder to perform checks on your dataset. (To open, click on the link for the checklist and then click ‚Äúview raw‚Äù to download.)\n\nChecking for Consistency Across Datasets\nOnce you have a grasp on the overall organization of a dataset ‚Äî including the variable names, labels, and formats, as well as the number of observations ‚Äî it‚Äôs time to dive into the relationships among variables and the distribution of values within each variable. Here, you want to check that things make sense. Can someone who said they don‚Äôt have a business really be bringing in $1 million in profits each week? Unlikely. Well-programmed surveys should minimize the need for these tests, although they are still good to implement as a check on the quality of the programming.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Coding in Stata"
    ]
  },
  {
    "objectID": "software/stata/coding-stata.html#relative-references",
    "href": "software/stata/coding-stata.html#relative-references",
    "title": "Coding in Stata",
    "section": "Relative references",
    "text": "Relative references\nIt is important to ensure that code can be run on multiple computers, and that code will not require hours of repair if a folder is renamed. To do this, we use relative references for file paths. The principle behind this is simple: any time a file (data, log file, another piece of code, etc.) is references in the script, the code only specifies the name of the file and some shorthand for the filepath. This shorthand will vary depending on the software used and the preferences of the analyst. In Stata, this is generally a global macro. Scripts that modify data should avoid calling any absolute file paths to ensure that any script is as flexible as possible to changes in the file path.\nFor example, if I am loading data from ‚ÄúC:/My Documents/My Project Folder/data.dta‚Äù, I could type use \"${relative_reference}/data.dta\" if I had defined a global called relative_reference that contained C:/My Documents/My Project Folder. In this example, I would have defined that global to save the file path somewhere earlier in the script. One thing to note is that this global uses the entire file path. A relative reference such as ${directory}/My Project Folder/data.dta introduces more possibility for error by including an absolute name of a folder. Full file paths used in the project should preferable be saved as globals or called using macro extended functions.\nThere are multiple ways to make relative references including local and global macros, setting working directories, or user written commands in Stata such as fastcd. We suggest defining global macros in the master do file or a specific global.do do file so that file paths are set in one location and are able to be called at any point during the dataflow.\n\nSetting Relative References in Stata\nTo define relative references, do files can define a global for a particular path and then use the macro name throughout to refer to the set path.\n*Set directory path\nglobal path \"C:/My Documents/My Project Folder\"\n\n*Set folder directories\nglobal data \"${path}/01_data\"\nglobal dos  \"${path}/02_do\"\nThen, whenever we call a particular file, we include the correct file path:\n*Run cleaning do file\ndo \"${dos}/clean.do\"\n\n*Use clean data:\nuse \"${data}/clean.dta\"\nNote that this names the directory where all project files are stored as the first global. After that it names relative references for each folder that contains files used by the do file using the global that contains the location of the project folder. This two-step process makes it easier to modify the do file for multiple users. Generally, the only folder that will be unique between users is the file path before the project folder (e.g.¬†C:\\Users\\[username] is a common prefix that will change based on the username on Windows machines).\n\n\nWorking Default References\nWhat if someone new uses the project that you don‚Äôt know their username? Often, it may not be possible to match a system variables stored by Stata to a unique identify a user. There are multiple solutions to this problem. Some master do files will start with a local user command that defines who the user is and sets relative references using a conditional:\n*Define user\nloc user Me\n\n*Set project folder directory\nif \"`user`\" == \"Me\" {\n    global directory \"C:/My Documents/My Project Folder/\"\"\n}\nHowever, this requires manual management in the do file any time a new user is added to a project. To create a general case, the do file can take advantage of how Stata defines a project. When Stata is opened, State defaults to assigning a working directory by first, the location of the file being opened by Stata and second, the homepath in the profile.do file.\nWe suggest using `c(pwd)‚Äô as a stand-in for file location and assume the user had opened the master do file: If you add this as an else at the end of the if \"`user'\"== conditionals, then there is a good chance the do file will run even if you haven‚Äôt made a specific local for the user.\n*Set default directory\nelse {\n    global directory = subinstr(\"`c(pwd)'\", \"\\\", \"/\" ,.)\n    global directory = subinstr(\"`cdloc'\", \"/02_do\", \"\", .)\n}\nThis will not work if the instance of Stata is opened from a file in a different directory than the do file that contains the code shown above. Text editors such as Atom and Sublime preserve the same instance of Stata. This code will be more likely to cause an error if the user does not use the do file editor or opens do files from the command line with doedit.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Coding in Stata"
    ]
  },
  {
    "objectID": "software/stata/coding-stata.html#useful-stata-commands",
    "href": "software/stata/coding-stata.html#useful-stata-commands",
    "title": "Coding in Stata",
    "section": "Useful Stata commands",
    "text": "Useful Stata commands\nBelow we have listed some frequently overlooked commands that we enjoy using. They will make your life a lot easier if you know about them. We provide a quick summary of what the commands do and where they can be used.\nWhere we appropriate, there is a page dedicated to specific commands that provides more examples of our common uses tips and tricks, and potential concerns to be aware of when using these particular command. For full documentation simply type help [commandname] in Stata to read all about available options and examples for usage.\n\nfillin\nThis is a simple to use, but yet powerful command. Frequently in cleaning data sets, you will have an unbalanced panel, i.e.¬†you are missing an observation for one person for some time periods. For example, imagine you have a dataset of your sample that is supposed to have one observation for each survey round such as the baseline and two endlines. However, as is common, some people were not found in the endline surveys and thus there is no observation for them at that endline. You can use this command to create all of the pairwise combinations of values of two variables i.e.¬†you would have every survey round observation for every person.\nFrequently in cleaning data sets, you will have an unbalanced panel, i.e.¬†you are missing an observation for one person for some time periods. For example, imagine you have a dataset of your sample that is supposed to have one observation for each survey round such as the baseline and two endlines. However, as is common, some people were not found in the endline surveys and thus there is no observation for them at that endline. See the example data below where you can see person 1 is missing an observation at endline 2.\n\n\n\nID\nSurvey_Round\nGender\n\n\n\n\n1\nbaseline\nFemale\n\n\n1\nendline1\nFemale\n\n\n2\nbaseline\nMale\n\n\n2\nendline1\nMale\n\n\n2\nendline2\nMale\n\n\n\nWe can use fillin ID Survey_Round to get the following\n\n\n\nID\nSurvey_Round\nGender\n_fillin\n\n\n\n\n1\nbaseline\nFemale\n0\n\n\n1\nendline1\nFemale\n0\n\n\n1\nendline2\n.\n1\n\n\n2\nbaseline\nMale\n0\n\n\n2\nendline1\nMale\n0\n\n\n2\nendline2\nMale\n0\n\n\n\nA couple things to note: 1. This command automatically creates an indicator variable called \\_fillin that keeps track of observations that were created from the command 2. Variables not specified in the fillin are filled in with a missing value. So after you run fillin you will need to go back and replace observations as you see appropriate. For example, for typically constant variables such as gender you could replace this new missing value to match the one above it.\nsort ID Survey_Round\nreplace Gender = Gender[_n-1] if ID == ID[_n-1] & _fillin == 1\n\n\nlabeldup\nlabeldup is a user-written command that compares value labels which have duplicate contents. For example, if in your dataset the variable q1 has a value label named q1_label with 0 \"No\" 1 \"Yes\" and the variable q2 has the same value label (the SurveyCTO standard). By using labeldup, select, the q1 and q2 will be combined to a single value label that describes both variables. This is an easy way to cut down on duplicate information in value labels.\n\n\nlabelrename\nThis user-written command allows you to rename value labels using similar syntax to the rename command. Stata does not allow you to rename value labels using the label values command. This command adds that functionality with the similar syntax as the rename command. One difference is that no wildcards are allowed.\n\n\nlevelsof\nThis command will provide you with a list of all of the unique values of a variable. This comes in handy all of the time when cleaning. A very helpful option that comes with this command is local() in which you can store the list of values as a local. This makes looping through all of the values of a variable possible and easy.\n\n\nlookfor\nThis command searches across all variable names and variable labels, and allows for searching for more than one string or a phrase through the use of \"[string]\". Since lookfor searches across both variable names and variable labels at the same time, it will return a different set of results than ds can.\n\n\nmissing\nThis is not a built-in Stata command and thus you will need to type ssc install missing to get this command before you can use it or read the help file.\nThis group of commands allows for several ways to investigate the missing values in variables. Missing values are typically forgotten about or ignored, but that is a big mistake. They can complicate cleaning and analysis greatly. You should be aware of the missing values and variables in your dataset. This command includes ways to report, list, tag, and drop missing values and variables.\nA very helpful command within this group is missing dropvars which allows you to eliminate variables that are missing on all observations. This is much easier than looping through vars and obs to assert they are missing to drop them.\n\n\nmmerge\nThis is not a built-in Stata command and can be installed by typing ssc install mmerge.\nmmerge provides additional options for merge that make merging datasets more user friendly by removing a signfiicant amount of preprocessing work for complex datasets: - It allows for missing values in the match variable and let‚Äôs the user determine how they are treated with the missing() option. - It allows for differing merge variable names with the umatch() option - It allows for all merged variables from the using file to be prefixed as part of the merge with the uname() option - It stores shared variable names that are not merged in a local r(common)\nThere are a number of other options and syntax changes. You can see these by typing help mmerge once mmerge is installed.\n\n\nreturn list\nThis command allows you to see all of the stored results in working memory and their value. For example, if you ran summarize [variarble], the return list command would show all of the scalars stored by summarize. In this case that would be r(N), r(sum_w), and r(sum). Similar commands also allow you to see estimation and system commands using ereturn list and creturn list. The help file (help return list) shows more options.\n\n\nsencode\nThis is not a built-in Stata command and can be installed by typing ssc install sencode.\nsencode makes a number of improvements on the Stata command encode by including a options that allow you to replace the string variable with the numeric variable and set the order of encded values in a user-specified ways, among other user-friendly additions.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Coding in Stata"
    ]
  },
  {
    "objectID": "software/stata/data-exploration-stata.html",
    "href": "software/stata/data-exploration-stata.html",
    "title": "Research and Data Science Hub",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "software/stata/index.html",
    "href": "software/stata/index.html",
    "title": "Getting Started with Stata",
    "section": "",
    "text": "Stata is a general-purpose statistical software package developed by StataCorp and used for data manipulation, visualization, and statistical analysis. It is used by lots of researchers in different fields of study. Although there are other statistical software ‚Äì including R, SAS, SPSS and Python ‚Äì Stata is the primary statistical tool used at IPA due to its flexibility and ease of use and also because it is most widely used by social science researchers. Over the years, IPA and J-PAL have developed a lot of training materials and user-written programs for Stata‚Äîthis course is based on the work of many colleagues across many countries and projects.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "software/stata/index.html#why-use-stata",
    "href": "software/stata/index.html#why-use-stata",
    "title": "Getting Started with Stata",
    "section": "Why use Stata?",
    "text": "Why use Stata?\nWhile spreadsheet software ‚Äì like Microsoft Excel ‚Äì is widely available and easy to use, these often lack the flexibility and sophistication required for complex statistical analyses. Stata, on the other hand, is a powerful statistical software package that provides a wide range of tools and capabilities for data analysis and management. Some of Stata‚Äôs capabilities as a statistical software include:\n\nAbility to handle large and complex datasets\nPowerful data management and cleaning tools\nExtensive library of statistical procedures\nAn active user community with a wide range of resources\nHigh-quality graphics and visualization tools\nCompatibility and integration with a wide range of file formats, including Excel, CSV, and SAS\nExtensive documentation and user guides\nRegular updates and improvements to keep up with new methods.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "software/stata/index.html#installing-stata",
    "href": "software/stata/index.html#installing-stata",
    "title": "Getting Started with Stata",
    "section": "Installing Stata",
    "text": "Installing Stata\nIf Stata requires purchasing or securing a license from StataCorp. At IPA, staff have access to an instutional license. Once you have a licenses, you should install the relevant version for your computer.\n\n\n\n\n\n\nTip\n\n\n\nRemember that Stata can be installed on any computer owned by IPA. IPA staff can manage this installation directly‚Äîyou do not need to request support from IPA‚Äôs Technology team unless you encounter a problem. Please note that IPA‚Äôs Stata license should only be used on IPA devices and should not be installed on your personal computers.\nIPA staff can download and install the relevant version (.exe for Windows, .dmg for MacOS, or .tar.gz for Linux) from IPA Box installation documentation.\nAlso note that Stata is not currently available for mobile devices. If you encounter problems after carefully reviewing all instructions, send an email to the IPA Technology team at support@poverty-action.org.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "software/stata/index.html#the-stata-interface",
    "href": "software/stata/index.html#the-stata-interface",
    "title": "Getting Started with Stata",
    "section": "The Stata Interface",
    "text": "The Stata Interface\nStata‚Äôs interface is designed to provide users with a user-friendly environment that allows them to interact with the software and perform statistical analyses efficiently. That being said, there are several windows and tools (explained below) that you can use to interact with the software and conduct your work.\n\n\n\nStata‚Äôs User Interface\n\n\n\n\nCommand window: Displays the output of commands executed in the command window. This includes tables, graphs, and other results generated by Stata. In other words, all the results from your analyses appear here.\nResults window: Displays the output of commands executed in the command window. This includes tables, graphs, and other results generated by Stata. In other words, all the results from your analyses appear here.\nHistory window: This is your command history, and it shows all commands you have typed during your Stata session and all commands Stata created for you when you work with the GUI.\nVariables window: The variables window displays information about the variables in the dataset currently loaded in Stata. Users can view the variable name, label, type, and properties, among other information.\nProperties window: You can use this window to manage your variables, including their names, labels, value labels, notes, formats, and storage types.\n\n\nBesides the five main windows, Stata offers other specialized tools and features to help with your data-related work. Some of these include the project manager, the data and graph editors, and the do-file. More on these later.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "software/stata/index.html#interacting-with-stata",
    "href": "software/stata/index.html#interacting-with-stata",
    "title": "Getting Started with Stata",
    "section": "Interacting with Stata",
    "text": "Interacting with Stata\nYou can use the menus and toolbars contained in Stata‚Äôs graphical user interface rather than writing code to perform common tasks, such as opening datasets, running statistical analyses, and creating graphs. We refer to this method as point-and-click. To execute a specific task, you can simply click on the ‚ÄúRun‚Äù button in the toolbar.\n\n\n\nExample of point-and-click interaction\n\n\nHowever, a more typical (and recommended) approach involves typing ‚Äúcommands‚Äù into the program to manipulate and analyze large quantities of data; we refer to this as the commandline. This may feel unfamiliar if you don‚Äôt have a background in statistical analysis or coding, but most data-oriented professionals use the command-line approach as it eliminates the need to navigate menus and toolbars. It also allows for easier automation and scripting of analyses, which can be helpful for more advanced users.\n\n\n\nExample of Stata commands",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "software/stata/index.html#statas-command-syntax",
    "href": "software/stata/index.html#statas-command-syntax",
    "title": "Getting Started with Stata",
    "section": "Stata‚Äôs command syntax",
    "text": "Stata‚Äôs command syntax\nStata syntax is the language we use to communicate with Stata. This means that, as with any other language, we need to learn it, understand it, and use it for effective communication with the software. The standard syntax of Stata commands is as follows:\n[by varlist]: command [varlist] [=exp] [if exp] [in range] [weight] [using filename][, options]\nWhere varlist denotes a list of variable names, command denotes a Stata command, exp denotes an algebraic expression, range denotes an observation range, weight denotes a weighting expression, and options denotes a list of command-specific options. In other words, this means that any Stata command we run is accompanied by multiple options in brackets, [], that alter the output we receive from said command. Other than the command itself, everything in brackets is optional.\n\n\n\n\n\n\nStata is case sensitive!\n\n\n\nAnother important thing about syntax: Stata commands and syntax never come in CAPITAL LETTERS. Therefore, always use small letters when typing Stata commands.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "software/stata/index.html#setting-up-a-working-directory",
    "href": "software/stata/index.html#setting-up-a-working-directory",
    "title": "Getting Started with Stata",
    "section": "Setting up a working directory",
    "text": "Setting up a working directory\nWhile working in Stata, you will often need to import or export files into directories ‚Äì or folders ‚Äì on your computer. When importing or exporting a file, you need to indicate the location in which the file can be found or saved so that Stata is able to pinpoint the exact file you are looking for or output to the exact folder that you want. In this section, we will explore how Stata handles file directories.\nWhenever you open up Stata, Stata sets a default directory. You can see this directory at the bottom left corner of the Stata window.\n\n\n\nStata Working Directory\n\n\nStata also provides a convenient command for displaying your current directory. Type the following into your Command window:\npwd\nYou will notice that Stata has displayed the same directory that is displayed at the bottom left of your Stata window. This directory is your current working directory‚Äîwhenever you try to import or export in Stata without specifying a directory, Stata will assume that this is the one you mean. You can also list the names of the files and folders of your current working directory by using the following commands:\ndir\nls\nFor most projects, you will want to change your working directory to a folder that contains the files you want to work with. You can change your current working directory using the cd command. Pull up and read the help file for the following command:\ncd \"target folder\"",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "software/stata/index.html#absolute-vs.-relative-paths",
    "href": "software/stata/index.html#absolute-vs.-relative-paths",
    "title": "Getting Started with Stata",
    "section": "Absolute vs.¬†Relative Paths",
    "text": "Absolute vs.¬†Relative Paths\nWhen working with Stata, it is important to keep a good file system that allows you to easily find project files and subfolders that you may be looking for. Sometimes you may be working alone and can set all your paths with only your own workspace in mind. However, in most cases you will want to write your Stata code in a way that makes it replicable on other computers without additional setup.\nA relative path denotes a file path in relation to the current working directory, while an absolute path denotes a file‚Äôs path at a fixed location on a disk. In most cases, you want to use relative paths. Examine the folder shown below. Take notice of the path at the top as well as the subfolders in this folder.\n\n\n\nAbsolute vs.¬†Relative Paths in Stata\n\n\nLet‚Äôs assume that your current working directory in Stata is D:\\Files\\Data Cleaning\\Main Data, which is the folder in the picture above. You want to navigate within Stata to the 1 dofiles subfolder. You could achieve this using either of the following codes:\n* Option 1\ncd \"D:\\Files\\Data Cleaning\\Main Data/1 dofiles\"\n\n* Option 2\ncd \"1 dofiles\"\nCode option 1 is an example of using an absolute path; it directs Stata to an exact location every single time you run this code. This means that regardless of your current working directory in Stata, you can always switch to this target directory using Code option 1. However, this code may not work on another computer unless the other user has the Main Data folder located in the path D:\\Files\\Data Cleaning.\nCode option 2 directs Stata to the same folder using a relative path. Based on this code, Stata will assume that you want to move to a folder named 1 dofiles which is located in your current working directory. This code will only work if your current working directory contains the folder 1 dofiles.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "software/stata/index.html#loading-a-dataset",
    "href": "software/stata/index.html#loading-a-dataset",
    "title": "Getting Started with Stata",
    "section": "Loading a dataset",
    "text": "Loading a dataset\nWhen working in Stata, you will work with datasets from different sources and in different formats. Stata provides a list of commands for importing and exporting various data in various formats including Excel spreadsheets (.xls and .xlsx files), text files, and datasets from other software such as SPSS and SAS. Stata can also create and store data in a Stata-format dataset with the filename extension .dta. The .dta format is the primary format you will use when storing and analyzing data in Stata.\nThere are several ways to import a dataset into Stata, including:\n\nThe use command\nUsing File &gt; Open in the menu bar to select a file\nClicking on the ‚ÄúOpen‚Äù icon ‚Äì which looks like a folder ‚Äì in the Menu bar to select a file.\nDouble clicking on the dataset within your computer‚Äôs file directory to open a new Stata session with the dataset already loaded\n\nIn most applications, we should be using the use command. For example, let‚Äôs download a sample dataset into our current working directory using this command:\ncopy \"https://raw.githubusercontent.com/PovertyAction/IPA-Stata-Trainings/master/Stata%20101/Data/intro.dta\" \"intro.dta\", replace\nNote that you will not see a change in your Stata display after running this command. If you open the folder that you have designated as the current working directory, you should now see a .dta file (marked with the Stata icon) titled intro.dta.\nNext, import this dataset from the current working directory into Stata. To do this, use the use command to import the dataset intro.dta as follows:\nuse \"intro.dta\", clear\n\n\n\n\n\n\nTip\n\n\n\n\nBecause the file is located in your current working directory, you do not need to specify the file path.\nIf you already have a different dataset loaded in Stata‚Äôs memory, you will need to include the , clear option as part of your command.\n\n\n\nThe use command also works directly with URLs. Let‚Äôs re-import the intro dataset straight from the GitHub website by running the following code in Stata:\nuse \"https://raw.githubusercontent.com/PovertyAction/IPA-Stata-Trainings/master/Stata%20101/Data/intro.dta\", clear\nYou will see that Stata has automatically detected and imported the dataset directly from the website without you needing to download it first.",
    "crumbs": [
      "Software Guides",
      "Stata",
      "Getting Started with Stata"
    ]
  },
  {
    "objectID": "under-construction.html",
    "href": "under-construction.html",
    "title": "Research and Data Science Hub",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Research Design",
      "Measurement üöß"
    ]
  }
]